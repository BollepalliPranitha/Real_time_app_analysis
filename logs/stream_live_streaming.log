2025-05-08 04:58:13,447 - INFO - Callback Server Starting
2025-05-08 04:58:13,449 - INFO - Socket listening on ('127.0.0.1', 41229)
2025-05-08 04:58:16,460 - INFO - Started streaming for live_streaming to Snowflake
2025-05-08 04:58:20,473 - INFO - Closing down clientserver connection
2025-05-08 04:59:29,409 - INFO - Callback Server Starting
2025-05-08 04:59:29,411 - INFO - Socket listening on ('127.0.0.1', 37651)
2025-05-08 04:59:32,105 - INFO - Started streaming for live_streaming to Snowflake
2025-05-08 04:59:36,835 - INFO - Closing down clientserver connection
2025-05-08 05:00:42,875 - INFO - Callback Server Starting
2025-05-08 05:00:42,876 - INFO - Socket listening on ('127.0.0.1', 34715)
2025-05-08 05:00:45,161 - INFO - Started streaming for live_streaming to Snowflake
2025-05-08 05:00:49,567 - INFO - Closing down clientserver connection
2025-05-08 05:01:55,379 - INFO - Callback Server Starting
2025-05-08 05:01:55,381 - INFO - Socket listening on ('127.0.0.1', 46773)
2025-05-08 05:01:57,279 - INFO - Started streaming for live_streaming to Snowflake
2025-05-08 05:02:01,594 - INFO - Closing down clientserver connection
2025-05-08 05:03:07,364 - INFO - Callback Server Starting
2025-05-08 05:03:07,366 - INFO - Socket listening on ('127.0.0.1', 41885)
2025-05-08 05:03:09,478 - INFO - Started streaming for live_streaming to Snowflake
2025-05-08 05:03:13,266 - INFO - Closing down clientserver connection
2025-05-08 05:04:14,698 - INFO - Callback Server Starting
2025-05-08 05:04:14,699 - INFO - Socket listening on ('127.0.0.1', 38529)
2025-05-08 05:04:16,777 - INFO - Started streaming for live_streaming to Snowflake
2025-05-08 05:04:20,400 - INFO - Closing down clientserver connection
2025-05-08 05:05:23,060 - INFO - Callback Server Starting
2025-05-08 05:05:23,064 - INFO - Socket listening on ('127.0.0.1', 40231)
2025-05-08 05:05:25,144 - INFO - Started streaming for live_streaming to Snowflake
2025-05-08 05:05:29,030 - INFO - Closing down clientserver connection
2025-05-08 05:05:55,065 - INFO - Error while receiving.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "/opt/bitnami/python/lib/python3.9/socket.py", line 704, in readinto
    return self._sock.recv_into(b)
ConnectionResetError: [Errno 104] Connection reset by peer
2025-05-08 05:05:55,066 - INFO - Error while receiving.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty
2025-05-08 05:05:55,091 - INFO - Closing down clientserver connection
2025-05-08 05:05:55,094 - INFO - Closing down clientserver connection
2025-05-08 05:05:55,095 - INFO - Closing down clientserver connection
2025-05-08 05:05:55,096 - ERROR - Exception while sending command.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
2025-05-08 05:05:55,134 - INFO - Closing down clientserver connection
2025-05-08 05:05:55,136 - INFO - Closing down clientserver connection
2025-05-08 05:05:55,138 - INFO - Closing down clientserver connection
2025-05-08 05:07:54,911 - INFO - Callback Server Starting
2025-05-08 05:07:54,912 - INFO - Socket listening on ('127.0.0.1', 45493)
2025-05-08 05:07:57,727 - INFO - Started streaming for live_streaming to Snowflake
2025-05-08 05:08:10,072 - INFO - Closing down clientserver connection
2025-05-08 05:09:13,103 - INFO - Callback Server Starting
2025-05-08 05:09:13,104 - INFO - Socket listening on ('127.0.0.1', 46415)
2025-05-08 05:09:14,977 - INFO - Started streaming for live_streaming to Snowflake
2025-05-08 05:09:27,289 - INFO - Closing down clientserver connection
2025-05-08 05:10:27,997 - INFO - Callback Server Starting
2025-05-08 05:10:27,998 - INFO - Socket listening on ('127.0.0.1', 37497)
2025-05-08 05:10:30,619 - INFO - Started streaming for live_streaming to Snowflake
2025-05-08 05:10:42,844 - INFO - Closing down clientserver connection
2025-05-08 05:11:36,322 - INFO - Error while receiving.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty
2025-05-08 05:11:36,325 - INFO - Closing down clientserver connection
2025-05-08 05:11:36,326 - ERROR - Exception while sending command.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
2025-05-08 05:11:36,330 - INFO - Closing down clientserver connection
2025-05-08 05:11:36,338 - INFO - Closing down clientserver connection
2025-05-08 05:13:08,936 - INFO - Callback Server Starting
2025-05-08 05:13:08,938 - INFO - Socket listening on ('127.0.0.1', 46461)
2025-05-08 05:13:11,230 - INFO - Started streaming for live_streaming to Snowflake
2025-05-08 05:13:23,674 - INFO - Closing down clientserver connection
2025-05-08 05:15:35,612 - INFO - Callback Server Starting
2025-05-08 05:15:35,704 - INFO - Socket listening on ('127.0.0.1', 45641)
2025-05-08 05:15:41,325 - INFO - Started streaming for live_streaming to Snowflake
2025-05-08 05:15:54,509 - INFO - Callback Server Starting
2025-05-08 05:15:54,604 - INFO - Socket listening on ('127.0.0.1', 44795)
2025-05-08 05:16:00,230 - INFO - Started streaming for live_streaming to Snowflake
2025-05-08 05:16:09,751 - INFO - Closing down clientserver connection
2025-05-08 05:16:12,957 - INFO - Error while receiving.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty
2025-05-08 05:16:13,024 - INFO - Closing down clientserver connection
2025-05-08 05:16:13,025 - ERROR - Exception while sending command.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
2025-05-08 05:16:13,033 - INFO - Closing down clientserver connection
2025-05-08 05:16:13,037 - INFO - Closing down clientserver connection
2025-05-08 05:17:09,359 - INFO - Error while receiving.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty
2025-05-08 05:17:09,360 - INFO - Error while receiving.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty
2025-05-08 05:17:09,362 - INFO - Closing down clientserver connection
2025-05-08 05:17:09,363 - INFO - Closing down clientserver connection
2025-05-08 05:17:09,364 - ERROR - Exception while sending command.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
2025-05-08 05:17:09,365 - ERROR - Exception while sending command.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
2025-05-08 05:17:09,369 - INFO - Closing down clientserver connection
2025-05-08 05:17:09,370 - INFO - Closing down clientserver connection
2025-05-08 05:17:09,371 - INFO - Closing down clientserver connection
2025-05-08 05:19:12,638 - INFO - Callback Server Starting
2025-05-08 05:19:12,640 - INFO - Socket listening on ('127.0.0.1', 36657)
2025-05-08 05:19:15,150 - INFO - Started streaming for live_streaming to Snowflake
2025-05-08 05:19:27,828 - INFO - Closing down clientserver connection
2025-05-08 05:20:30,892 - INFO - Callback Server Starting
2025-05-08 05:20:30,893 - INFO - Socket listening on ('127.0.0.1', 45747)
2025-05-08 05:20:32,385 - INFO - Started streaming for live_streaming to Snowflake
2025-05-08 05:20:45,587 - INFO - Closing down clientserver connection
2025-05-08 05:21:43,436 - INFO - Callback Server Starting
2025-05-08 05:21:43,438 - INFO - Socket listening on ('127.0.0.1', 35277)
2025-05-08 05:21:45,829 - INFO - Started streaming for live_streaming to Snowflake
2025-05-08 05:21:56,960 - INFO - Closing down clientserver connection
2025-05-08 05:22:54,766 - INFO - Callback Server Starting
2025-05-08 05:22:54,769 - INFO - Socket listening on ('127.0.0.1', 39763)
2025-05-08 05:22:56,563 - INFO - Started streaming for live_streaming to Snowflake
2025-05-08 05:23:08,067 - INFO - Closing down clientserver connection
2025-05-08 05:24:05,692 - INFO - Callback Server Starting
2025-05-08 05:24:05,695 - INFO - Socket listening on ('127.0.0.1', 37079)
2025-05-08 05:24:07,714 - INFO - Started streaming for live_streaming to Snowflake
2025-05-08 05:24:20,219 - INFO - Closing down clientserver connection
2025-05-08 05:25:21,934 - INFO - Callback Server Starting
2025-05-08 05:25:21,935 - INFO - Socket listening on ('127.0.0.1', 37339)
2025-05-08 05:25:23,833 - INFO - Started streaming for live_streaming to Snowflake
2025-05-08 05:25:45,045 - INFO - Python Server ready to receive messages
2025-05-08 05:25:45,046 - INFO - Received command c on object id p0
2025-05-08 05:25:50,160 - INFO - Processing batch 0 with 0 rows
2025-05-08 05:25:59,862 - ERROR - There was an exception while executing the Python Proxy on the Python Side.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 617, in _call_proxy
    return_value = getattr(self.pool[obj_id], method)(*params)
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 114, in call
    raise e
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 111, in call
    self.func(DataFrame(jdf, wrapped_session_jdf), batch_id)
  File "/app/scripts/stream_live_streaming.py", line 128, in process_batch
    .withColumn("Hour", hour(to_timestamp(col("WatchTime"), "HH:mm:ss"))) \
NameError: name 'to_timestamp' is not defined
2025-05-08 05:26:00,977 - INFO - Closing down clientserver connection
2025-05-08 05:27:06,114 - INFO - Callback Server Starting
2025-05-08 05:27:06,117 - INFO - Socket listening on ('127.0.0.1', 44847)
2025-05-08 05:27:08,892 - INFO - Started streaming for live_streaming to Snowflake
2025-05-08 05:27:17,314 - INFO - Error while receiving.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty
2025-05-08 05:27:17,317 - INFO - Closing down clientserver connection
2025-05-08 05:27:17,319 - ERROR - Exception while sending command.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
2025-05-08 05:27:17,331 - INFO - Closing down clientserver connection
2025-05-08 05:27:17,333 - INFO - Closing down clientserver connection
2025-05-08 05:28:22,743 - INFO - Callback Server Starting
2025-05-08 05:28:22,744 - INFO - Socket listening on ('127.0.0.1', 36051)
2025-05-08 05:28:24,941 - INFO - Started streaming for live_streaming to Snowflake
2025-05-08 05:28:39,568 - INFO - Python Server ready to receive messages
2025-05-08 05:28:39,569 - INFO - Received command c on object id p0
2025-05-08 05:28:46,158 - INFO - Processing batch 0 with 0 rows
2025-05-08 05:28:58,675 - ERROR - There was an exception while executing the Python Proxy on the Python Side.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 617, in _call_proxy
    return_value = getattr(self.pool[obj_id], method)(*params)
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 114, in call
    raise e
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 111, in call
    self.func(DataFrame(jdf, wrapped_session_jdf), batch_id)
  File "/app/scripts/stream_live_streaming.py", line 128, in process_batch
    .withColumn("Hour", hour(to_timestamp(col("WatchTime"), "HH:mm:ss"))) \
NameError: name 'to_timestamp' is not defined
2025-05-08 05:29:00,473 - INFO - Closing down clientserver connection
2025-05-08 05:29:44,418 - INFO - Callback Server Starting
2025-05-08 05:29:44,419 - INFO - Socket listening on ('127.0.0.1', 33319)
2025-05-08 05:29:45,776 - INFO - Started streaming for live_streaming to Snowflake
2025-05-08 05:29:56,408 - INFO - Python Server ready to receive messages
2025-05-08 05:29:56,408 - INFO - Received command c on object id p0
2025-05-08 05:29:59,784 - INFO - Processing batch 0 with 0 rows
2025-05-08 05:30:07,700 - ERROR - There was an exception while executing the Python Proxy on the Python Side.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 617, in _call_proxy
    return_value = getattr(self.pool[obj_id], method)(*params)
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 114, in call
    raise e
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 111, in call
    self.func(DataFrame(jdf, wrapped_session_jdf), batch_id)
  File "/app/scripts/stream_live_streaming.py", line 128, in process_batch
    .withColumn("Hour", hour(to_timestamp(col("WatchTime"), "HH:mm:ss"))) \
NameError: name 'to_timestamp' is not defined
2025-05-08 05:30:09,016 - INFO - Closing down clientserver connection
2025-05-08 05:31:18,433 - INFO - Callback Server Starting
2025-05-08 05:31:18,434 - INFO - Socket listening on ('127.0.0.1', 46657)
2025-05-08 05:31:21,020 - INFO - Started streaming for live_streaming to Snowflake
2025-05-08 05:31:33,462 - INFO - Python Server ready to receive messages
2025-05-08 05:31:33,463 - INFO - Received command c on object id p0
2025-05-08 05:31:37,543 - INFO - Processing batch 0 with 0 rows
2025-05-08 05:31:46,066 - ERROR - There was an exception while executing the Python Proxy on the Python Side.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 617, in _call_proxy
    return_value = getattr(self.pool[obj_id], method)(*params)
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 114, in call
    raise e
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 111, in call
    self.func(DataFrame(jdf, wrapped_session_jdf), batch_id)
  File "/app/scripts/stream_live_streaming.py", line 128, in process_batch
    .withColumn("Hour", hour(to_timestamp(col("WatchTime"), "HH:mm:ss"))) \
NameError: name 'to_timestamp' is not defined
2025-05-08 05:31:47,146 - INFO - Closing down clientserver connection
2025-05-08 05:32:21,161 - INFO - Callback Server Starting
2025-05-08 05:32:21,162 - INFO - Socket listening on ('127.0.0.1', 32943)
2025-05-08 05:32:22,390 - INFO - Started streaming for live_streaming to Snowflake
2025-05-08 05:32:31,677 - INFO - Python Server ready to receive messages
2025-05-08 05:32:31,678 - INFO - Received command c on object id p0
2025-05-08 05:32:34,770 - INFO - Processing batch 0 with 0 rows
2025-05-08 05:32:42,370 - ERROR - There was an exception while executing the Python Proxy on the Python Side.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 617, in _call_proxy
    return_value = getattr(self.pool[obj_id], method)(*params)
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 114, in call
    raise e
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 111, in call
    self.func(DataFrame(jdf, wrapped_session_jdf), batch_id)
  File "/app/scripts/stream_live_streaming.py", line 128, in process_batch
    .withColumn("Hour", hour(to_timestamp(col("WatchTime"), "HH:mm:ss"))) \
NameError: name 'to_timestamp' is not defined
2025-05-08 05:32:43,711 - INFO - Closing down clientserver connection
2025-05-08 05:33:19,623 - INFO - Callback Server Starting
2025-05-08 05:33:19,625 - INFO - Socket listening on ('127.0.0.1', 36975)
2025-05-08 05:33:20,730 - INFO - Started streaming for live_streaming to Snowflake
2025-05-08 05:33:27,731 - INFO - Python Server ready to receive messages
2025-05-08 05:33:27,732 - INFO - Received command c on object id p0
2025-05-08 05:33:31,723 - INFO - Processing batch 0 with 0 rows
2025-05-08 05:33:39,210 - ERROR - There was an exception while executing the Python Proxy on the Python Side.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 617, in _call_proxy
    return_value = getattr(self.pool[obj_id], method)(*params)
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 114, in call
    raise e
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 111, in call
    self.func(DataFrame(jdf, wrapped_session_jdf), batch_id)
  File "/app/scripts/stream_live_streaming.py", line 128, in process_batch
    .withColumn("Hour", hour(to_timestamp(col("WatchTime"), "HH:mm:ss"))) \
NameError: name 'to_timestamp' is not defined
2025-05-08 05:33:40,649 - INFO - Closing down clientserver connection
2025-05-08 05:34:21,113 - INFO - Callback Server Starting
2025-05-08 05:34:21,114 - INFO - Socket listening on ('127.0.0.1', 33001)
2025-05-08 05:34:21,579 - INFO - Error while sending or receiving.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 169, in deco
    return f(*a, **kw)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 326, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: <unprintable Py4JJavaError object>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe
2025-05-08 05:34:21,582 - INFO - Closing down clientserver connection
2025-05-08 05:34:21,583 - INFO - Exception while sending command.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 169, in deco
    return f(*a, **kw)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 326, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: <unprintable Py4JJavaError object>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 506, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending
2025-05-08 05:34:21,590 - INFO - Closing down clientserver connection
2025-05-08 05:34:21,592 - INFO - Closing down clientserver connection
2025-05-08 05:34:21,578 - INFO - Error while sending or receiving.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 169, in deco
    return f(*a, **kw)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 326, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: <unprintable Py4JJavaError object>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe
2025-05-08 05:34:21,594 - INFO - Closing down clientserver connection
2025-05-08 05:34:21,595 - INFO - Closing down clientserver connection
2025-05-08 05:34:21,596 - INFO - Closing down clientserver connection
2025-05-08 05:34:21,595 - INFO - Exception while sending command.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 169, in deco
    return f(*a, **kw)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 326, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: <unprintable Py4JJavaError object>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 506, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending
2025-05-08 05:34:21,599 - INFO - Closing down clientserver connection
2025-05-08 05:34:21,600 - INFO - Closing down clientserver connection
2025-05-08 05:34:21,578 - INFO - Error while sending or receiving.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 169, in deco
    return f(*a, **kw)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 326, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: <unprintable Py4JJavaError object>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe
2025-05-08 05:34:21,602 - INFO - Closing down clientserver connection
2025-05-08 05:34:21,607 - INFO - Closing down clientserver connection
2025-05-08 05:34:21,608 - INFO - Closing down clientserver connection
2025-05-08 05:34:21,607 - INFO - Exception while sending command.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 169, in deco
    return f(*a, **kw)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 326, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: <unprintable Py4JJavaError object>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 506, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending
2025-05-08 05:34:21,611 - INFO - Closing down clientserver connection
2025-05-08 05:34:21,612 - INFO - Closing down clientserver connection
2025-05-08 05:34:21,577 - INFO - Error while sending or receiving.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 169, in deco
    return f(*a, **kw)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 326, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: <unprintable Py4JJavaError object>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe
2025-05-08 05:34:21,614 - INFO - Closing down clientserver connection
2025-05-08 05:34:21,615 - INFO - Closing down clientserver connection
2025-05-08 05:34:21,616 - INFO - Closing down clientserver connection
2025-05-08 05:34:21,615 - INFO - Exception while sending command.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 169, in deco
    return f(*a, **kw)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 326, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: <unprintable Py4JJavaError object>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 506, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending
2025-05-08 05:34:21,620 - INFO - Closing down clientserver connection
2025-05-08 05:34:21,621 - INFO - Closing down clientserver connection
2025-05-08 05:34:21,577 - INFO - Error while sending or receiving.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 169, in deco
    return f(*a, **kw)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 326, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: <unprintable Py4JJavaError object>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe
2025-05-08 05:34:21,625 - INFO - Closing down clientserver connection
2025-05-08 05:34:21,626 - INFO - Closing down clientserver connection
2025-05-08 05:34:21,627 - INFO - Closing down clientserver connection
2025-05-08 05:34:21,626 - INFO - Exception while sending command.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 169, in deco
    return f(*a, **kw)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 326, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: <unprintable Py4JJavaError object>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 506, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending
2025-05-08 05:34:21,629 - INFO - Closing down clientserver connection
2025-05-08 05:34:21,631 - INFO - Closing down clientserver connection
2025-05-08 05:34:21,576 - INFO - Error while sending or receiving.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 169, in deco
    return f(*a, **kw)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 326, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: <unprintable Py4JJavaError object>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe
2025-05-08 05:34:21,633 - INFO - Closing down clientserver connection
2025-05-08 05:34:21,634 - INFO - Closing down clientserver connection
2025-05-08 05:34:21,635 - INFO - Closing down clientserver connection
2025-05-08 05:34:21,633 - INFO - Exception while sending command.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 169, in deco
    return f(*a, **kw)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 326, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: <unprintable Py4JJavaError object>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 506, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending
2025-05-08 05:34:21,638 - INFO - Closing down clientserver connection
2025-05-08 05:34:21,639 - INFO - Closing down clientserver connection
2025-05-08 05:34:21,576 - INFO - Error while sending or receiving.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 169, in deco
    return f(*a, **kw)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 326, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: <unprintable Py4JJavaError object>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe
2025-05-08 05:34:21,641 - INFO - Closing down clientserver connection
2025-05-08 05:34:21,642 - INFO - Closing down clientserver connection
2025-05-08 05:34:21,644 - INFO - Closing down clientserver connection
2025-05-08 05:34:21,642 - INFO - Exception while sending command.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 169, in deco
    return f(*a, **kw)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 326, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: <unprintable Py4JJavaError object>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 506, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending
2025-05-08 05:34:21,646 - INFO - Closing down clientserver connection
2025-05-08 05:34:21,648 - INFO - Closing down clientserver connection
2025-05-08 05:34:21,576 - INFO - Error while sending or receiving.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 169, in deco
    return f(*a, **kw)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 326, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: <unprintable Py4JJavaError object>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe
2025-05-08 05:34:21,652 - INFO - Closing down clientserver connection
2025-05-08 05:34:21,653 - INFO - Closing down clientserver connection
2025-05-08 05:34:21,656 - INFO - Closing down clientserver connection
2025-05-08 05:34:21,652 - INFO - Exception while sending command.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 169, in deco
    return f(*a, **kw)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 326, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: <unprintable Py4JJavaError object>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 506, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending
2025-05-08 05:34:21,659 - INFO - Closing down clientserver connection
2025-05-08 05:34:21,660 - INFO - Closing down clientserver connection
2025-05-08 05:34:21,576 - INFO - Error while sending or receiving.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 169, in deco
    return f(*a, **kw)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 326, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: <unprintable Py4JJavaError object>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe
2025-05-08 05:34:21,662 - INFO - Closing down clientserver connection
2025-05-08 05:34:21,662 - INFO - Closing down clientserver connection
2025-05-08 05:34:21,663 - INFO - Closing down clientserver connection
2025-05-08 05:34:21,662 - INFO - Exception while sending command.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 169, in deco
    return f(*a, **kw)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 326, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: <unprintable Py4JJavaError object>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 506, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending
2025-05-08 05:34:21,665 - INFO - Closing down clientserver connection
2025-05-08 05:34:21,666 - INFO - Closing down clientserver connection
2025-05-08 05:34:21,576 - INFO - Error while sending or receiving.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 169, in deco
    return f(*a, **kw)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 326, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: <unprintable Py4JJavaError object>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe
2025-05-08 05:34:21,667 - INFO - Closing down clientserver connection
2025-05-08 05:34:21,668 - INFO - Closing down clientserver connection
2025-05-08 05:34:21,669 - INFO - Closing down clientserver connection
2025-05-08 05:34:21,668 - INFO - Exception while sending command.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 169, in deco
    return f(*a, **kw)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 326, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: <unprintable Py4JJavaError object>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 506, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending
2025-05-08 05:34:21,676 - INFO - Closing down clientserver connection
2025-05-08 05:34:21,677 - INFO - Closing down clientserver connection
2025-05-08 05:34:21,575 - INFO - Error while sending or receiving.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 169, in deco
    return f(*a, **kw)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 326, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: <unprintable Py4JJavaError object>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe
2025-05-08 05:34:21,679 - INFO - Closing down clientserver connection
2025-05-08 05:34:21,680 - INFO - Closing down clientserver connection
2025-05-08 05:34:21,681 - INFO - Closing down clientserver connection
2025-05-08 05:34:21,680 - INFO - Exception while sending command.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 169, in deco
    return f(*a, **kw)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 326, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: <unprintable Py4JJavaError object>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 506, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending
2025-05-08 05:34:21,683 - INFO - Closing down clientserver connection
2025-05-08 05:34:21,710 - INFO - Closing down clientserver connection
2025-05-08 05:34:21,575 - INFO - Error while sending or receiving.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 169, in deco
    return f(*a, **kw)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 326, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: <unprintable Py4JJavaError object>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe
2025-05-08 05:34:21,713 - INFO - Closing down clientserver connection
2025-05-08 05:34:21,715 - INFO - Closing down clientserver connection
2025-05-08 05:34:21,715 - INFO - Closing down clientserver connection
2025-05-08 05:34:21,714 - INFO - Exception while sending command.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 169, in deco
    return f(*a, **kw)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 326, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: <unprintable Py4JJavaError object>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 506, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending
2025-05-08 05:34:21,718 - INFO - Closing down clientserver connection
2025-05-08 05:34:21,718 - INFO - Closing down clientserver connection
2025-05-08 05:34:21,575 - INFO - Error while sending or receiving.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 169, in deco
    return f(*a, **kw)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 326, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: <unprintable Py4JJavaError object>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe
2025-05-08 05:34:21,720 - INFO - Closing down clientserver connection
2025-05-08 05:34:21,723 - INFO - Closing down clientserver connection
2025-05-08 05:34:21,724 - INFO - Closing down clientserver connection
2025-05-08 05:34:21,722 - INFO - Exception while sending command.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 169, in deco
    return f(*a, **kw)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 326, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: <unprintable Py4JJavaError object>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 506, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending
2025-05-08 05:34:21,729 - INFO - Closing down clientserver connection
2025-05-08 05:34:21,736 - INFO - Closing down clientserver connection
2025-05-08 05:34:21,575 - INFO - Error while sending or receiving.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 169, in deco
    return f(*a, **kw)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 326, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: <unprintable Py4JJavaError object>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe
2025-05-08 05:34:21,739 - INFO - Closing down clientserver connection
2025-05-08 05:34:21,741 - INFO - Closing down clientserver connection
2025-05-08 05:34:21,741 - INFO - Closing down clientserver connection
2025-05-08 05:34:21,740 - INFO - Exception while sending command.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 169, in deco
    return f(*a, **kw)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 326, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: <unprintable Py4JJavaError object>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 506, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending
2025-05-08 05:34:21,743 - INFO - Closing down clientserver connection
2025-05-08 05:34:21,744 - INFO - Closing down clientserver connection
2025-05-08 05:34:21,574 - INFO - Error while sending or receiving.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 169, in deco
    return f(*a, **kw)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 326, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: <unprintable Py4JJavaError object>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe
2025-05-08 05:34:21,745 - INFO - Closing down clientserver connection
2025-05-08 05:34:21,747 - INFO - Closing down clientserver connection
2025-05-08 05:34:21,747 - INFO - Closing down clientserver connection
2025-05-08 05:34:21,746 - INFO - Exception while sending command.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 169, in deco
    return f(*a, **kw)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 326, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: <unprintable Py4JJavaError object>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 506, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending
2025-05-08 05:34:21,749 - INFO - Closing down clientserver connection
2025-05-08 05:34:21,750 - INFO - Closing down clientserver connection
2025-05-08 05:34:21,574 - INFO - Error while sending or receiving.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 169, in deco
    return f(*a, **kw)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 326, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: <unprintable Py4JJavaError object>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe
2025-05-08 05:34:21,751 - INFO - Closing down clientserver connection
2025-05-08 05:34:21,752 - INFO - Closing down clientserver connection
2025-05-08 05:34:21,752 - INFO - Closing down clientserver connection
2025-05-08 05:34:21,751 - INFO - Exception while sending command.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 169, in deco
    return f(*a, **kw)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 326, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: <unprintable Py4JJavaError object>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 506, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending
2025-05-08 05:34:21,755 - INFO - Closing down clientserver connection
2025-05-08 05:34:21,756 - INFO - Closing down clientserver connection
2025-05-08 05:34:21,574 - INFO - Error while sending or receiving.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 169, in deco
    return f(*a, **kw)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 326, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: <unprintable Py4JJavaError object>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe
2025-05-08 05:34:21,758 - INFO - Closing down clientserver connection
2025-05-08 05:34:21,758 - INFO - Closing down clientserver connection
2025-05-08 05:34:21,759 - INFO - Closing down clientserver connection
2025-05-08 05:34:21,758 - INFO - Exception while sending command.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 169, in deco
    return f(*a, **kw)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 326, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: <unprintable Py4JJavaError object>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 506, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending
2025-05-08 05:34:21,761 - INFO - Closing down clientserver connection
2025-05-08 05:34:21,761 - INFO - Closing down clientserver connection
2025-05-08 05:34:21,574 - INFO - Error while sending or receiving.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 169, in deco
    return f(*a, **kw)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 326, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: <unprintable Py4JJavaError object>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe
2025-05-08 05:34:21,763 - INFO - Closing down clientserver connection
2025-05-08 05:34:21,764 - INFO - Closing down clientserver connection
2025-05-08 05:34:21,765 - INFO - Closing down clientserver connection
2025-05-08 05:34:21,764 - INFO - Exception while sending command.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 169, in deco
    return f(*a, **kw)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 326, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: <unprintable Py4JJavaError object>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 506, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending
2025-05-08 05:34:21,767 - INFO - Closing down clientserver connection
2025-05-08 05:34:21,768 - INFO - Closing down clientserver connection
2025-05-08 05:34:21,574 - INFO - Error while sending or receiving.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 169, in deco
    return f(*a, **kw)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 326, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: <unprintable Py4JJavaError object>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe
2025-05-08 05:34:21,769 - INFO - Closing down clientserver connection
2025-05-08 05:34:21,770 - INFO - Closing down clientserver connection
2025-05-08 05:34:21,770 - INFO - Closing down clientserver connection
2025-05-08 05:34:21,769 - INFO - Exception while sending command.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 169, in deco
    return f(*a, **kw)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 326, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: <unprintable Py4JJavaError object>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 506, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending
2025-05-08 05:34:21,772 - INFO - Closing down clientserver connection
2025-05-08 05:34:21,773 - INFO - Closing down clientserver connection
2025-05-08 05:34:21,573 - INFO - Error while sending or receiving.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 169, in deco
    return f(*a, **kw)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 326, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: <unprintable Py4JJavaError object>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe
2025-05-08 05:34:21,775 - INFO - Closing down clientserver connection
2025-05-08 05:34:21,776 - INFO - Closing down clientserver connection
2025-05-08 05:34:21,777 - INFO - Closing down clientserver connection
2025-05-08 05:34:21,776 - INFO - Exception while sending command.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 169, in deco
    return f(*a, **kw)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 326, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: <unprintable Py4JJavaError object>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 506, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending
2025-05-08 05:34:21,811 - INFO - Closing down clientserver connection
2025-05-08 05:34:21,812 - INFO - Closing down clientserver connection
2025-05-08 05:34:21,572 - INFO - Error while sending or receiving.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 169, in deco
    return f(*a, **kw)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 326, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: <unprintable Py4JJavaError object>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe
2025-05-08 05:34:21,813 - INFO - Closing down clientserver connection
2025-05-08 05:34:21,814 - INFO - Closing down clientserver connection
2025-05-08 05:34:21,815 - INFO - Closing down clientserver connection
2025-05-08 05:34:21,814 - INFO - Exception while sending command.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 169, in deco
    return f(*a, **kw)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 326, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: <unprintable Py4JJavaError object>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
BrokenPipeError: [Errno 32] Broken pipe

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 506, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending
2025-05-08 05:34:21,816 - INFO - Closing down clientserver connection
2025-05-08 05:34:21,818 - INFO - Closing down clientserver connection
2025-05-08 05:34:21,572 - INFO - Error while receiving.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 169, in deco
    return f(*a, **kw)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 326, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: <unprintable Py4JJavaError object>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty
2025-05-08 05:34:21,820 - INFO - Closing down clientserver connection
2025-05-08 05:34:21,821 - INFO - Closing down clientserver connection
2025-05-08 05:34:21,825 - INFO - Closing down clientserver connection
2025-05-08 05:34:21,820 - ERROR - Exception while sending command.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 169, in deco
    return f(*a, **kw)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 326, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: <unprintable Py4JJavaError object>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
2025-05-08 05:34:21,571 - INFO - Error while receiving.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 169, in deco
    return f(*a, **kw)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 326, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: <unprintable Py4JJavaError object>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty
2025-05-08 05:34:21,753 - INFO - Error while receiving.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty
2025-05-08 05:34:21,829 - INFO - Closing down clientserver connection
2025-05-08 05:34:21,830 - INFO - Closing down clientserver connection
2025-05-08 05:34:21,832 - INFO - Closing down clientserver connection
2025-05-08 05:34:21,833 - INFO - Closing down clientserver connection
2025-05-08 05:34:21,831 - ERROR - Exception while sending command.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 169, in deco
    return f(*a, **kw)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 326, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: <unprintable Py4JJavaError object>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
2025-05-08 05:34:21,832 - ERROR - Exception while sending command.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
2025-05-08 05:34:21,834 - INFO - Closing down clientserver connection
2025-05-08 05:34:21,835 - INFO - Closing down clientserver connection
2025-05-08 05:35:26,579 - INFO - Callback Server Starting
2025-05-08 05:35:26,580 - INFO - Socket listening on ('127.0.0.1', 36821)
2025-05-08 05:35:29,477 - INFO - Started streaming for live_streaming to Snowflake
2025-05-08 05:35:42,853 - INFO - Python Server ready to receive messages
2025-05-08 05:35:42,854 - INFO - Received command c on object id p0
2025-05-08 05:35:48,962 - INFO - Processing batch 0 with 0 rows
2025-05-08 05:36:02,776 - ERROR - There was an exception while executing the Python Proxy on the Python Side.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 617, in _call_proxy
    return_value = getattr(self.pool[obj_id], method)(*params)
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 114, in call
    raise e
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 111, in call
    self.func(DataFrame(jdf, wrapped_session_jdf), batch_id)
  File "/app/scripts/stream_live_streaming.py", line 128, in process_batch
    .withColumn("Hour", hour(to_timestamp(col("WatchTime"), "HH:mm:ss"))) \
NameError: name 'to_timestamp' is not defined
2025-05-08 05:36:04,604 - INFO - Closing down clientserver connection
2025-05-08 05:36:49,490 - INFO - Callback Server Starting
2025-05-08 05:36:49,492 - INFO - Socket listening on ('127.0.0.1', 46741)
2025-05-08 05:36:51,405 - INFO - Started streaming for live_streaming to Snowflake
2025-05-08 05:37:01,404 - INFO - Python Server ready to receive messages
2025-05-08 05:37:01,407 - INFO - Received command c on object id p0
2025-05-08 05:37:06,619 - INFO - Processing batch 0 with 0 rows
2025-05-08 05:37:16,894 - ERROR - There was an exception while executing the Python Proxy on the Python Side.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 617, in _call_proxy
    return_value = getattr(self.pool[obj_id], method)(*params)
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 114, in call
    raise e
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 111, in call
    self.func(DataFrame(jdf, wrapped_session_jdf), batch_id)
  File "/app/scripts/stream_live_streaming.py", line 128, in process_batch
    .withColumn("Hour", hour(to_timestamp(col("WatchTime"), "HH:mm:ss"))) \
NameError: name 'to_timestamp' is not defined
2025-05-08 05:37:18,326 - INFO - Closing down clientserver connection
2025-05-08 05:37:57,559 - INFO - Callback Server Starting
2025-05-08 05:37:57,560 - INFO - Socket listening on ('127.0.0.1', 43415)
2025-05-08 05:37:58,550 - INFO - Started streaming for live_streaming to Snowflake
2025-05-08 05:38:09,265 - INFO - Python Server ready to receive messages
2025-05-08 05:38:09,266 - INFO - Received command c on object id p0
2025-05-08 05:38:12,657 - INFO - Processing batch 0 with 0 rows
2025-05-08 05:38:17,960 - INFO - Error while receiving.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty
2025-05-08 05:38:17,961 - INFO - Error while receiving.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty
2025-05-08 05:38:17,963 - INFO - Closing down clientserver connection
2025-05-08 05:38:17,964 - INFO - Closing down clientserver connection
2025-05-08 05:38:17,966 - ERROR - Exception while sending command.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
2025-05-08 05:38:17,967 - ERROR - Exception while sending command.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
2025-05-08 05:38:17,972 - ERROR - There was an exception while executing the Python Proxy on the Python Side.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 617, in _call_proxy
    return_value = getattr(self.pool[obj_id], method)(*params)
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 114, in call
    raise e
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 111, in call
    self.func(DataFrame(jdf, wrapped_session_jdf), batch_id)
  File "/app/scripts/stream_live_streaming.py", line 125, in process_batch
    existing_time_df = spark.read.format("snowflake").options(**snowflake_options).option("dbtable", "PUBLIC.DIM_TIME").load().select("WatchTime")
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 307, in load
    return self._df(self._jreader.load())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 169, in deco
    return f(*a, **kw)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 334, in get_return_value
    raise Py4JError(
py4j.protocol.Py4JError: An error occurred while calling o182.load
2025-05-08 05:38:17,973 - INFO - Closing down clientserver connection
2025-05-08 05:39:58,916 - INFO - Callback Server Starting
2025-05-08 05:39:58,987 - INFO - Socket listening on ('127.0.0.1', 35219)
2025-05-08 05:40:00,881 - INFO - Started streaming for live_streaming to Snowflake
2025-05-08 05:40:11,882 - INFO - Python Server ready to receive messages
2025-05-08 05:40:11,883 - INFO - Received command c on object id p0
2025-05-08 05:40:17,298 - INFO - Processing batch 0 with 0 rows
2025-05-08 05:40:30,308 - ERROR - There was an exception while executing the Python Proxy on the Python Side.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 617, in _call_proxy
    return_value = getattr(self.pool[obj_id], method)(*params)
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 114, in call
    raise e
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 111, in call
    self.func(DataFrame(jdf, wrapped_session_jdf), batch_id)
  File "/app/scripts/stream_live_streaming.py", line 128, in process_batch
    .withColumn("Hour", hour(to_timestamp(col("WatchTime"), "HH:mm:ss"))) \
NameError: name 'to_timestamp' is not defined
2025-05-08 05:40:31,926 - INFO - Closing down clientserver connection
2025-05-08 05:41:18,734 - INFO - Callback Server Starting
2025-05-08 05:41:18,735 - INFO - Socket listening on ('127.0.0.1', 45355)
2025-05-08 05:41:20,009 - INFO - Started streaming for live_streaming to Snowflake
2025-05-08 05:41:33,442 - INFO - Python Server ready to receive messages
2025-05-08 05:41:33,454 - INFO - Received command c on object id p0
2025-05-08 05:41:37,630 - INFO - Processing batch 0 with 0 rows
2025-05-08 05:41:46,564 - ERROR - There was an exception while executing the Python Proxy on the Python Side.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 617, in _call_proxy
    return_value = getattr(self.pool[obj_id], method)(*params)
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 114, in call
    raise e
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 111, in call
    self.func(DataFrame(jdf, wrapped_session_jdf), batch_id)
  File "/app/scripts/stream_live_streaming.py", line 128, in process_batch
    .withColumn("Hour", hour(to_timestamp(col("WatchTime"), "HH:mm:ss"))) \
NameError: name 'to_timestamp' is not defined
2025-05-08 05:41:48,046 - INFO - Closing down clientserver connection
2025-05-08 05:42:35,265 - INFO - Callback Server Starting
2025-05-08 05:42:35,266 - INFO - Socket listening on ('127.0.0.1', 38863)
2025-05-08 05:42:36,165 - INFO - Started streaming for live_streaming to Snowflake
2025-05-08 05:42:44,982 - INFO - Python Server ready to receive messages
2025-05-08 05:42:44,983 - INFO - Received command c on object id p0
2025-05-08 05:42:49,278 - INFO - Processing batch 0 with 0 rows
2025-05-08 05:42:52,884 - INFO - Error while receiving.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty
2025-05-08 05:42:52,892 - INFO - Closing down clientserver connection
2025-05-08 05:42:52,893 - ERROR - Exception while sending command.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
2025-05-08 05:42:52,886 - INFO - Error while receiving.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty
2025-05-08 05:42:52,906 - INFO - Closing down clientserver connection
2025-05-08 05:42:52,907 - INFO - Closing down clientserver connection
2025-05-08 05:42:52,909 - ERROR - Exception while sending command.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
2025-05-08 05:42:52,910 - ERROR - There was an exception while executing the Python Proxy on the Python Side.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 617, in _call_proxy
    return_value = getattr(self.pool[obj_id], method)(*params)
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 114, in call
    raise e
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 111, in call
    self.func(DataFrame(jdf, wrapped_session_jdf), batch_id)
  File "/app/scripts/stream_live_streaming.py", line 125, in process_batch
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 307, in load
    return self._df(self._jreader.load())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 169, in deco
    return f(*a, **kw)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 334, in get_return_value
    raise Py4JError(
py4j.protocol.Py4JError: An error occurred while calling o182.load
2025-05-08 05:42:52,959 - INFO - Closing down clientserver connection
2025-05-08 05:43:41,934 - DEBUG - Starting stream_live_streaming.py
2025-05-08 05:43:41,935 - DEBUG - Loading configurations
2025-05-08 05:43:41,940 - DEBUG - Configurations loaded
2025-05-08 05:43:41,940 - DEBUG - Initializing Spark session
2025-05-08 05:43:41,982 - DEBUG - GatewayClient.address is deprecated and will be removed in version 1.0. Use GatewayParameters instead.
2025-05-08 05:43:41,983 - DEBUG - Command to send: A
7396bbad2bdda4460a8d88753b8fea0a7d9c86d97b891e746b1b3dcea48f9ffa

2025-05-08 05:43:42,021 - DEBUG - Answer received: !yv
2025-05-08 05:43:42,022 - DEBUG - Command to send: j
i
rj
org.apache.spark.SparkConf
e

2025-05-08 05:43:42,026 - DEBUG - Answer received: !yv
2025-05-08 05:43:42,027 - DEBUG - Command to send: j
i
rj
org.apache.spark.api.java.*
e

2025-05-08 05:43:42,028 - DEBUG - Answer received: !yv
2025-05-08 05:43:42,028 - DEBUG - Command to send: j
i
rj
org.apache.spark.api.python.*
e

2025-05-08 05:43:42,029 - DEBUG - Answer received: !yv
2025-05-08 05:43:42,030 - DEBUG - Command to send: j
i
rj
org.apache.spark.ml.python.*
e

2025-05-08 05:43:42,031 - DEBUG - Answer received: !yv
2025-05-08 05:43:42,031 - DEBUG - Command to send: j
i
rj
org.apache.spark.mllib.api.python.*
e

2025-05-08 05:43:42,033 - DEBUG - Answer received: !yv
2025-05-08 05:43:42,033 - DEBUG - Command to send: j
i
rj
org.apache.spark.resource.*
e

2025-05-08 05:43:42,077 - DEBUG - Answer received: !yv
2025-05-08 05:43:42,078 - DEBUG - Command to send: j
i
rj
org.apache.spark.sql.*
e

2025-05-08 05:43:42,080 - DEBUG - Answer received: !yv
2025-05-08 05:43:42,081 - DEBUG - Command to send: j
i
rj
org.apache.spark.sql.api.python.*
e

2025-05-08 05:43:42,082 - DEBUG - Answer received: !yv
2025-05-08 05:43:42,085 - DEBUG - Command to send: j
i
rj
org.apache.spark.sql.hive.*
e

2025-05-08 05:43:42,086 - DEBUG - Answer received: !yv
2025-05-08 05:43:42,086 - DEBUG - Command to send: j
i
rj
scala.Tuple2
e

2025-05-08 05:43:42,087 - DEBUG - Answer received: !yv
2025-05-08 05:43:42,088 - DEBUG - Command to send: r
u
SparkConf
rj
e

2025-05-08 05:43:42,093 - DEBUG - Answer received: !ycorg.apache.spark.SparkConf
2025-05-08 05:43:42,095 - DEBUG - Command to send: i
org.apache.spark.SparkConf
bTrue
e

2025-05-08 05:43:42,176 - DEBUG - Answer received: !yro0
2025-05-08 05:43:42,178 - DEBUG - Command to send: c
o0
set
sspark.app.name
sStreamLiveStreaming
e

2025-05-08 05:43:42,191 - DEBUG - Answer received: !yro1
2025-05-08 05:43:42,193 - DEBUG - Command to send: c
o0
set
sspark.sql.shuffle.partitions
s1
e

2025-05-08 05:43:42,194 - DEBUG - Answer received: !yro2
2025-05-08 05:43:42,199 - DEBUG - Command to send: c
o0
set
sspark.streaming.stopGracefullyOnShutdown
strue
e

2025-05-08 05:43:42,200 - DEBUG - Answer received: !yro3
2025-05-08 05:43:42,202 - DEBUG - Command to send: c
o0
contains
sspark.serializer.objectStreamReset
e

2025-05-08 05:43:42,205 - DEBUG - Answer received: !ybfalse
2025-05-08 05:43:42,206 - DEBUG - Command to send: c
o0
set
sspark.serializer.objectStreamReset
s100
e

2025-05-08 05:43:42,207 - DEBUG - Answer received: !yro4
2025-05-08 05:43:42,209 - DEBUG - Command to send: c
o0
contains
sspark.rdd.compress
e

2025-05-08 05:43:42,211 - DEBUG - Answer received: !ybfalse
2025-05-08 05:43:42,212 - DEBUG - Command to send: c
o0
set
sspark.rdd.compress
sTrue
e

2025-05-08 05:43:42,213 - DEBUG - Answer received: !yro5
2025-05-08 05:43:42,215 - DEBUG - Command to send: c
o0
contains
sspark.master
e

2025-05-08 05:43:42,216 - DEBUG - Answer received: !ybtrue
2025-05-08 05:43:42,276 - DEBUG - Command to send: c
o0
contains
sspark.app.name
e

2025-05-08 05:43:42,282 - DEBUG - Answer received: !ybtrue
2025-05-08 05:43:42,285 - DEBUG - Command to send: c
o0
contains
sspark.master
e

2025-05-08 05:43:42,287 - DEBUG - Answer received: !ybtrue
2025-05-08 05:43:42,288 - DEBUG - Command to send: c
o0
get
sspark.master
e

2025-05-08 05:43:42,291 - DEBUG - Answer received: !yslocal[*]
2025-05-08 05:43:42,291 - DEBUG - Command to send: c
o0
contains
sspark.app.name
e

2025-05-08 05:43:42,293 - DEBUG - Answer received: !ybtrue
2025-05-08 05:43:42,294 - DEBUG - Command to send: c
o0
get
sspark.app.name
e

2025-05-08 05:43:42,297 - DEBUG - Answer received: !ysStreamLiveStreaming
2025-05-08 05:43:42,298 - DEBUG - Command to send: c
o0
contains
sspark.home
e

2025-05-08 05:43:42,299 - DEBUG - Answer received: !ybfalse
2025-05-08 05:43:42,300 - DEBUG - Command to send: c
o0
getAll
e

2025-05-08 05:43:42,302 - DEBUG - Answer received: !yto6
2025-05-08 05:43:42,304 - DEBUG - Command to send: a
e
o6
e

2025-05-08 05:43:42,305 - DEBUG - Answer received: !yi10
2025-05-08 05:43:42,306 - DEBUG - Command to send: a
g
o6
i0
e

2025-05-08 05:43:42,308 - DEBUG - Answer received: !yro7
2025-05-08 05:43:42,309 - DEBUG - Command to send: c
o7
_1
e

2025-05-08 05:43:42,311 - DEBUG - Answer received: !ysspark.sql.shuffle.partitions
2025-05-08 05:43:42,312 - DEBUG - Command to send: c
o7
_2
e

2025-05-08 05:43:42,313 - DEBUG - Answer received: !ys1
2025-05-08 05:43:42,314 - DEBUG - Command to send: a
e
o6
e

2025-05-08 05:43:42,315 - DEBUG - Answer received: !yi10
2025-05-08 05:43:42,316 - DEBUG - Command to send: a
g
o6
i1
e

2025-05-08 05:43:42,317 - DEBUG - Answer received: !yro8
2025-05-08 05:43:42,318 - DEBUG - Command to send: c
o8
_1
e

2025-05-08 05:43:42,319 - DEBUG - Answer received: !ysspark.streaming.stopGracefullyOnShutdown
2025-05-08 05:43:42,320 - DEBUG - Command to send: c
o8
_2
e

2025-05-08 05:43:42,321 - DEBUG - Answer received: !ystrue
2025-05-08 05:43:42,322 - DEBUG - Command to send: a
e
o6
e

2025-05-08 05:43:42,323 - DEBUG - Answer received: !yi10
2025-05-08 05:43:42,323 - DEBUG - Command to send: a
g
o6
i2
e

2025-05-08 05:43:42,324 - DEBUG - Answer received: !yro9
2025-05-08 05:43:42,325 - DEBUG - Command to send: c
o9
_1
e

2025-05-08 05:43:42,326 - DEBUG - Answer received: !ysspark.rdd.compress
2025-05-08 05:43:42,327 - DEBUG - Command to send: c
o9
_2
e

2025-05-08 05:43:42,328 - DEBUG - Answer received: !ysTrue
2025-05-08 05:43:42,328 - DEBUG - Command to send: a
e
o6
e

2025-05-08 05:43:42,329 - DEBUG - Answer received: !yi10
2025-05-08 05:43:42,330 - DEBUG - Command to send: a
g
o6
i3
e

2025-05-08 05:43:42,331 - DEBUG - Answer received: !yro10
2025-05-08 05:43:42,332 - DEBUG - Command to send: c
o10
_1
e

2025-05-08 05:43:42,333 - DEBUG - Answer received: !ysspark.app.name
2025-05-08 05:43:42,333 - DEBUG - Command to send: c
o10
_2
e

2025-05-08 05:43:42,334 - DEBUG - Answer received: !ysStreamLiveStreaming
2025-05-08 05:43:42,335 - DEBUG - Command to send: a
e
o6
e

2025-05-08 05:43:42,376 - DEBUG - Answer received: !yi10
2025-05-08 05:43:42,377 - DEBUG - Command to send: a
g
o6
i4
e

2025-05-08 05:43:42,378 - DEBUG - Answer received: !yro11
2025-05-08 05:43:42,379 - DEBUG - Command to send: c
o11
_1
e

2025-05-08 05:43:42,380 - DEBUG - Answer received: !ysspark.serializer.objectStreamReset
2025-05-08 05:43:42,381 - DEBUG - Command to send: c
o11
_2
e

2025-05-08 05:43:42,382 - DEBUG - Answer received: !ys100
2025-05-08 05:43:42,383 - DEBUG - Command to send: a
e
o6
e

2025-05-08 05:43:42,383 - DEBUG - Answer received: !yi10
2025-05-08 05:43:42,384 - DEBUG - Command to send: a
g
o6
i5
e

2025-05-08 05:43:42,385 - DEBUG - Answer received: !yro12
2025-05-08 05:43:42,386 - DEBUG - Command to send: c
o12
_1
e

2025-05-08 05:43:42,388 - DEBUG - Answer received: !ysspark.master
2025-05-08 05:43:42,389 - DEBUG - Command to send: c
o12
_2
e

2025-05-08 05:43:42,390 - DEBUG - Answer received: !yslocal[*]
2025-05-08 05:43:42,391 - DEBUG - Command to send: a
e
o6
e

2025-05-08 05:43:42,392 - DEBUG - Answer received: !yi10
2025-05-08 05:43:42,393 - DEBUG - Command to send: a
g
o6
i6
e

2025-05-08 05:43:42,394 - DEBUG - Answer received: !yro13
2025-05-08 05:43:42,395 - DEBUG - Command to send: c
o13
_1
e

2025-05-08 05:43:42,396 - DEBUG - Answer received: !ysspark.submit.pyFiles
2025-05-08 05:43:42,398 - DEBUG - Command to send: c
o13
_2
e

2025-05-08 05:43:42,399 - DEBUG - Answer received: !ys
2025-05-08 05:43:42,400 - DEBUG - Command to send: a
e
o6
e

2025-05-08 05:43:42,401 - DEBUG - Answer received: !yi10
2025-05-08 05:43:42,402 - DEBUG - Command to send: a
g
o6
i7
e

2025-05-08 05:43:42,403 - DEBUG - Answer received: !yro14
2025-05-08 05:43:42,403 - DEBUG - Command to send: c
o14
_1
e

2025-05-08 05:43:42,405 - DEBUG - Answer received: !ysspark.submit.deployMode
2025-05-08 05:43:42,406 - DEBUG - Command to send: c
o14
_2
e

2025-05-08 05:43:42,408 - DEBUG - Answer received: !ysclient
2025-05-08 05:43:42,409 - DEBUG - Command to send: a
e
o6
e

2025-05-08 05:43:42,410 - DEBUG - Answer received: !yi10
2025-05-08 05:43:42,411 - DEBUG - Command to send: a
g
o6
i8
e

2025-05-08 05:43:42,413 - DEBUG - Answer received: !yro15
2025-05-08 05:43:42,414 - DEBUG - Command to send: c
o15
_1
e

2025-05-08 05:43:42,415 - DEBUG - Answer received: !ysspark.driver.extraJavaOptions
2025-05-08 05:43:42,416 - DEBUG - Command to send: c
o15
_2
e

2025-05-08 05:43:42,420 - DEBUG - Answer received: !ys--add-exports java.base/sun.nio.ch=ALL-UNNAMED
2025-05-08 05:43:42,421 - DEBUG - Command to send: a
e
o6
e

2025-05-08 05:43:42,422 - DEBUG - Answer received: !yi10
2025-05-08 05:43:42,423 - DEBUG - Command to send: a
g
o6
i9
e

2025-05-08 05:43:42,424 - DEBUG - Answer received: !yro16
2025-05-08 05:43:42,424 - DEBUG - Command to send: c
o16
_1
e

2025-05-08 05:43:42,425 - DEBUG - Answer received: !ysspark.app.submitTime
2025-05-08 05:43:42,426 - DEBUG - Command to send: c
o16
_2
e

2025-05-08 05:43:42,427 - DEBUG - Answer received: !ys1746683019114
2025-05-08 05:43:42,428 - DEBUG - Command to send: a
e
o6
e

2025-05-08 05:43:42,477 - DEBUG - Answer received: !yi10
2025-05-08 05:43:42,477 - DEBUG - Command to send: r
u
JavaSparkContext
rj
e

2025-05-08 05:43:42,508 - DEBUG - Answer received: !ycorg.apache.spark.api.java.JavaSparkContext
2025-05-08 05:43:42,509 - DEBUG - Command to send: i
org.apache.spark.api.java.JavaSparkContext
ro0
e

2025-05-08 05:43:42,983 - DEBUG - Command to send: A
7396bbad2bdda4460a8d88753b8fea0a7d9c86d97b891e746b1b3dcea48f9ffa

2025-05-08 05:43:43,097 - DEBUG - Answer received: !yv
2025-05-08 05:43:43,097 - DEBUG - Command to send: m
d
o1
e

2025-05-08 05:43:43,279 - DEBUG - Answer received: !yv
2025-05-08 05:43:43,280 - DEBUG - Command to send: m
d
o2
e

2025-05-08 05:43:43,288 - DEBUG - Answer received: !yv
2025-05-08 05:43:43,289 - DEBUG - Command to send: m
d
o3
e

2025-05-08 05:43:43,291 - DEBUG - Answer received: !yv
2025-05-08 05:43:43,291 - DEBUG - Command to send: m
d
o4
e

2025-05-08 05:43:43,294 - DEBUG - Answer received: !yv
2025-05-08 05:43:43,295 - DEBUG - Command to send: m
d
o5
e

2025-05-08 05:43:43,299 - DEBUG - Answer received: !yv
2025-05-08 05:43:43,306 - DEBUG - Command to send: m
d
o6
e

2025-05-08 05:43:43,308 - DEBUG - Answer received: !yv
2025-05-08 05:43:53,897 - DEBUG - Answer received: !yro17
2025-05-08 05:43:53,899 - DEBUG - Command to send: c
o17
sc
e

2025-05-08 05:43:53,912 - DEBUG - Answer received: !yro18
2025-05-08 05:43:53,914 - DEBUG - Command to send: c
o18
conf
e

2025-05-08 05:43:54,095 - DEBUG - Answer received: !yro19
2025-05-08 05:43:54,101 - DEBUG - Command to send: r
u
PythonAccumulatorV2
rj
e

2025-05-08 05:43:54,197 - DEBUG - Answer received: !ycorg.apache.spark.api.python.PythonAccumulatorV2
2025-05-08 05:43:54,199 - DEBUG - Command to send: i
org.apache.spark.api.python.PythonAccumulatorV2
s127.0.0.1
i42995
s7396bbad2bdda4460a8d88753b8fea0a7d9c86d97b891e746b1b3dcea48f9ffa
e

2025-05-08 05:43:54,203 - DEBUG - Answer received: !yro20
2025-05-08 05:43:54,204 - DEBUG - Command to send: c
o17
sc
e

2025-05-08 05:43:54,208 - DEBUG - Answer received: !yro21
2025-05-08 05:43:54,210 - DEBUG - Command to send: c
o21
register
ro20
e

2025-05-08 05:43:54,220 - DEBUG - Answer received: !yv
2025-05-08 05:43:54,221 - DEBUG - Command to send: r
u
PythonUtils
rj
e

2025-05-08 05:43:54,295 - DEBUG - Answer received: !ycorg.apache.spark.api.python.PythonUtils
2025-05-08 05:43:54,296 - DEBUG - Command to send: r
m
org.apache.spark.api.python.PythonUtils
isEncryptionEnabled
e

2025-05-08 05:43:54,297 - DEBUG - Answer received: !ym
2025-05-08 05:43:54,300 - DEBUG - Command to send: c
z:org.apache.spark.api.python.PythonUtils
isEncryptionEnabled
ro17
e

2025-05-08 05:43:54,305 - DEBUG - Answer received: !ybfalse
2025-05-08 05:43:54,306 - DEBUG - Command to send: r
u
PythonUtils
rj
e

2025-05-08 05:43:54,307 - DEBUG - Command to send: m
d
o0
e

2025-05-08 05:43:54,308 - DEBUG - Answer received: !yv
2025-05-08 05:43:54,308 - DEBUG - Command to send: m
d
o7
e

2025-05-08 05:43:54,309 - DEBUG - Answer received: !yv
2025-05-08 05:43:54,310 - DEBUG - Command to send: m
d
o8
e

2025-05-08 05:43:54,311 - DEBUG - Answer received: !yv
2025-05-08 05:43:54,312 - DEBUG - Command to send: m
d
o9
e

2025-05-08 05:43:54,310 - DEBUG - Answer received: !ycorg.apache.spark.api.python.PythonUtils
2025-05-08 05:43:54,314 - DEBUG - Command to send: r
m
org.apache.spark.api.python.PythonUtils
getPythonAuthSocketTimeout
e

2025-05-08 05:43:54,314 - DEBUG - Answer received: !yv
2025-05-08 05:43:54,315 - DEBUG - Command to send: m
d
o10
e

2025-05-08 05:43:54,316 - DEBUG - Answer received: !yv
2025-05-08 05:43:54,317 - DEBUG - Command to send: m
d
o11
e

2025-05-08 05:43:54,317 - DEBUG - Answer received: !ym
2025-05-08 05:43:54,320 - DEBUG - Command to send: c
z:org.apache.spark.api.python.PythonUtils
getPythonAuthSocketTimeout
ro17
e

2025-05-08 05:43:54,319 - DEBUG - Answer received: !yv
2025-05-08 05:43:54,321 - DEBUG - Command to send: m
d
o12
e

2025-05-08 05:43:54,322 - DEBUG - Answer received: !yv
2025-05-08 05:43:54,323 - DEBUG - Command to send: m
d
o13
e

2025-05-08 05:43:54,323 - DEBUG - Answer received: !yL15
2025-05-08 05:43:54,324 - DEBUG - Command to send: r
u
PythonUtils
rj
e

2025-05-08 05:43:54,323 - DEBUG - Answer received: !yv
2025-05-08 05:43:54,325 - DEBUG - Command to send: m
d
o14
e

2025-05-08 05:43:54,326 - DEBUG - Answer received: !yv
2025-05-08 05:43:54,327 - DEBUG - Command to send: m
d
o15
e

2025-05-08 05:43:54,392 - DEBUG - Answer received: !yv
2025-05-08 05:43:54,392 - DEBUG - Answer received: !ycorg.apache.spark.api.python.PythonUtils
2025-05-08 05:43:54,394 - DEBUG - Command to send: m
d
o16
e

2025-05-08 05:43:54,395 - DEBUG - Command to send: r
m
org.apache.spark.api.python.PythonUtils
getSparkBufferSize
e

2025-05-08 05:43:54,398 - DEBUG - Answer received: !yv
2025-05-08 05:43:54,399 - DEBUG - Command to send: m
d
o18
e

2025-05-08 05:43:54,399 - DEBUG - Answer received: !ym
2025-05-08 05:43:54,410 - DEBUG - Command to send: c
z:org.apache.spark.api.python.PythonUtils
getSparkBufferSize
ro17
e

2025-05-08 05:43:54,411 - DEBUG - Answer received: !yv
2025-05-08 05:43:54,412 - DEBUG - Answer received: !yi65536
2025-05-08 05:43:54,413 - DEBUG - Command to send: r
u
org
rj
e

2025-05-08 05:43:54,421 - DEBUG - Answer received: !yp
2025-05-08 05:43:54,422 - DEBUG - Command to send: r
u
org.apache
rj
e

2025-05-08 05:43:54,425 - DEBUG - Answer received: !yp
2025-05-08 05:43:54,425 - DEBUG - Command to send: r
u
org.apache.spark
rj
e

2025-05-08 05:43:54,493 - DEBUG - Answer received: !yp
2025-05-08 05:43:54,502 - DEBUG - Command to send: r
u
org.apache.spark.SparkFiles
rj
e

2025-05-08 05:43:54,504 - DEBUG - Answer received: !ycorg.apache.spark.SparkFiles
2025-05-08 05:43:54,505 - DEBUG - Command to send: r
m
org.apache.spark.SparkFiles
getRootDirectory
e

2025-05-08 05:43:54,506 - DEBUG - Answer received: !ym
2025-05-08 05:43:54,507 - DEBUG - Command to send: c
z:org.apache.spark.SparkFiles
getRootDirectory
e

2025-05-08 05:43:54,510 - DEBUG - Answer received: !ys/tmp/spark-cafbf962-924a-48ca-b149-47035d6f7852/userFiles-62c4dd9e-9120-407b-9b47-d6d4fc6dd7f1
2025-05-08 05:43:54,512 - DEBUG - Command to send: c
o19
get
sspark.submit.pyFiles
s
e

2025-05-08 05:43:54,514 - DEBUG - Answer received: !ys
2025-05-08 05:43:54,519 - DEBUG - Command to send: r
u
org
rj
e

2025-05-08 05:43:54,525 - DEBUG - Answer received: !yp
2025-05-08 05:43:54,526 - DEBUG - Command to send: r
u
org.apache
rj
e

2025-05-08 05:43:54,528 - DEBUG - Answer received: !yp
2025-05-08 05:43:54,530 - DEBUG - Command to send: r
u
org.apache.spark
rj
e

2025-05-08 05:43:54,593 - DEBUG - Answer received: !yp
2025-05-08 05:43:54,594 - DEBUG - Command to send: r
u
org.apache.spark.util
rj
e

2025-05-08 05:43:54,596 - DEBUG - Answer received: !yp
2025-05-08 05:43:54,596 - DEBUG - Command to send: r
u
org.apache.spark.util.Utils
rj
e

2025-05-08 05:43:54,602 - DEBUG - Answer received: !ycorg.apache.spark.util.Utils
2025-05-08 05:43:54,603 - DEBUG - Command to send: r
m
org.apache.spark.util.Utils
getLocalDir
e

2025-05-08 05:43:54,609 - DEBUG - Answer received: !ym
2025-05-08 05:43:54,610 - DEBUG - Command to send: c
o17
sc
e

2025-05-08 05:43:54,611 - DEBUG - Answer received: !yro22
2025-05-08 05:43:54,612 - DEBUG - Command to send: c
o22
conf
e

2025-05-08 05:43:54,616 - DEBUG - Answer received: !yro23
2025-05-08 05:43:54,621 - DEBUG - Command to send: c
z:org.apache.spark.util.Utils
getLocalDir
ro23
e

2025-05-08 05:43:54,694 - DEBUG - Answer received: !ys/tmp/spark-cafbf962-924a-48ca-b149-47035d6f7852
2025-05-08 05:43:54,695 - DEBUG - Command to send: r
u
org
rj
e

2025-05-08 05:43:54,707 - DEBUG - Answer received: !yp
2025-05-08 05:43:54,708 - DEBUG - Command to send: r
u
org.apache
rj
e

2025-05-08 05:43:54,709 - DEBUG - Answer received: !yp
2025-05-08 05:43:54,711 - DEBUG - Command to send: r
u
org.apache.spark
rj
e

2025-05-08 05:43:54,721 - DEBUG - Answer received: !yp
2025-05-08 05:43:54,722 - DEBUG - Command to send: r
u
org.apache.spark.util
rj
e

2025-05-08 05:43:54,725 - DEBUG - Answer received: !yp
2025-05-08 05:43:54,725 - DEBUG - Command to send: r
u
org.apache.spark.util.Utils
rj
e

2025-05-08 05:43:54,792 - DEBUG - Answer received: !ycorg.apache.spark.util.Utils
2025-05-08 05:43:54,796 - DEBUG - Command to send: r
m
org.apache.spark.util.Utils
createTempDir
e

2025-05-08 05:43:54,798 - DEBUG - Answer received: !ym
2025-05-08 05:43:54,804 - DEBUG - Command to send: c
z:org.apache.spark.util.Utils
createTempDir
s/tmp/spark-cafbf962-924a-48ca-b149-47035d6f7852
spyspark
e

2025-05-08 05:43:54,807 - DEBUG - Answer received: !yro24
2025-05-08 05:43:54,808 - DEBUG - Command to send: c
o24
getAbsolutePath
e

2025-05-08 05:43:54,811 - DEBUG - Answer received: !ys/tmp/spark-cafbf962-924a-48ca-b149-47035d6f7852/pyspark-79effd3b-2e59-4911-a93a-989424b5924f
2025-05-08 05:43:54,813 - DEBUG - Command to send: c
o19
get
sspark.python.profile
sfalse
e

2025-05-08 05:43:54,820 - DEBUG - Answer received: !ysfalse
2025-05-08 05:43:54,822 - DEBUG - Command to send: c
o19
get
sspark.python.profile.memory
sfalse
e

2025-05-08 05:43:54,823 - DEBUG - Answer received: !ysfalse
2025-05-08 05:43:54,824 - DEBUG - Command to send: r
u
SparkSession
rj
e

2025-05-08 05:43:55,092 - DEBUG - Answer received: !ycorg.apache.spark.sql.SparkSession
2025-05-08 05:43:55,093 - DEBUG - Command to send: r
m
org.apache.spark.sql.SparkSession
getDefaultSession
e

2025-05-08 05:43:55,225 - DEBUG - Answer received: !ym
2025-05-08 05:43:55,226 - DEBUG - Command to send: c
z:org.apache.spark.sql.SparkSession
getDefaultSession
e

2025-05-08 05:43:55,292 - DEBUG - Answer received: !yro25
2025-05-08 05:43:55,294 - DEBUG - Command to send: c
o25
isDefined
e

2025-05-08 05:43:55,300 - DEBUG - Answer received: !ybfalse
2025-05-08 05:43:55,303 - DEBUG - Command to send: r
u
SparkSession
rj
e

2025-05-08 05:43:55,314 - DEBUG - Answer received: !ycorg.apache.spark.sql.SparkSession
2025-05-08 05:43:55,317 - DEBUG - Command to send: c
o17
sc
e

2025-05-08 05:43:55,395 - DEBUG - Answer received: !yro26
2025-05-08 05:43:55,396 - DEBUG - Command to send: i
java.util.HashMap
e

2025-05-08 05:43:55,397 - DEBUG - Answer received: !yao27
2025-05-08 05:43:55,398 - DEBUG - Command to send: c
o27
put
sspark.app.name
sStreamLiveStreaming
e

2025-05-08 05:43:55,401 - DEBUG - Answer received: !yn
2025-05-08 05:43:55,402 - DEBUG - Command to send: c
o27
put
sspark.sql.shuffle.partitions
s1
e

2025-05-08 05:43:55,410 - DEBUG - Answer received: !yn
2025-05-08 05:43:55,422 - DEBUG - Command to send: c
o27
put
sspark.streaming.stopGracefullyOnShutdown
strue
e

2025-05-08 05:43:55,423 - DEBUG - Answer received: !yn
2025-05-08 05:43:55,424 - DEBUG - Command to send: i
org.apache.spark.sql.SparkSession
ro26
ro27
e

2025-05-08 05:43:55,932 - DEBUG - Answer received: !yro28
2025-05-08 05:43:55,933 - DEBUG - Command to send: r
u
SparkSession
rj
e

2025-05-08 05:43:55,937 - DEBUG - Answer received: !ycorg.apache.spark.sql.SparkSession
2025-05-08 05:43:55,939 - DEBUG - Command to send: r
m
org.apache.spark.sql.SparkSession
setDefaultSession
e

2025-05-08 05:43:55,992 - DEBUG - Answer received: !ym
2025-05-08 05:43:55,993 - DEBUG - Command to send: c
z:org.apache.spark.sql.SparkSession
setDefaultSession
ro28
e

2025-05-08 05:43:55,995 - DEBUG - Answer received: !yv
2025-05-08 05:43:56,003 - DEBUG - Command to send: r
u
SparkSession
rj
e

2025-05-08 05:43:56,006 - DEBUG - Answer received: !ycorg.apache.spark.sql.SparkSession
2025-05-08 05:43:56,008 - DEBUG - Command to send: r
m
org.apache.spark.sql.SparkSession
setActiveSession
e

2025-05-08 05:43:56,009 - DEBUG - Answer received: !ym
2025-05-08 05:43:56,010 - DEBUG - Command to send: c
z:org.apache.spark.sql.SparkSession
setActiveSession
ro28
e

2025-05-08 05:43:56,011 - DEBUG - Answer received: !yv
2025-05-08 05:43:56,012 - DEBUG - Spark session initialized
2025-05-08 05:43:56,019 - DEBUG - Schema defined
2025-05-08 05:43:56,022 - DEBUG - Reading from Kafka
2025-05-08 05:43:56,025 - DEBUG - Command to send: c
o28
readStream
e

2025-05-08 05:43:56,415 - DEBUG - Command to send: m
d
o27
e

2025-05-08 05:43:56,418 - DEBUG - Answer received: !yv
2025-05-08 05:44:03,196 - DEBUG - Answer received: !yro29
2025-05-08 05:44:03,210 - DEBUG - Command to send: c
o29
format
skafka
e

2025-05-08 05:44:03,212 - DEBUG - Answer received: !yro30
2025-05-08 05:44:03,213 - DEBUG - Command to send: c
o30
option
skafka.bootstrap.servers
skafka:9092
e

2025-05-08 05:44:03,216 - DEBUG - Answer received: !yro31
2025-05-08 05:44:03,223 - DEBUG - Command to send: c
o31
option
ssubscribe
slive_streaming
e

2025-05-08 05:44:03,224 - DEBUG - Answer received: !yro32
2025-05-08 05:44:03,228 - DEBUG - Command to send: c
o32
option
sstartingOffsets
slatest
e

2025-05-08 05:44:03,293 - DEBUG - Answer received: !yro33
2025-05-08 05:44:03,301 - DEBUG - Command to send: c
o33
option
sfailOnDataLoss
sfalse
e

2025-05-08 05:44:03,303 - DEBUG - Answer received: !yro34
2025-05-08 05:44:03,305 - DEBUG - Command to send: c
o34
load
e

2025-05-08 05:44:18,514 - DEBUG - Answer received: !yro35
2025-05-08 05:44:18,515 - DEBUG - Kafka stream initialized
2025-05-08 05:44:18,516 - DEBUG - Parsing JSON data
2025-05-08 05:44:18,517 - DEBUG - Command to send: r
u
functions
rj
e

2025-05-08 05:44:18,528 - DEBUG - Answer received: !ycorg.apache.spark.sql.functions
2025-05-08 05:44:18,529 - DEBUG - Command to send: r
m
org.apache.spark.sql.functions
col
e

2025-05-08 05:44:18,601 - DEBUG - Answer received: !ym
2025-05-08 05:44:18,602 - DEBUG - Command to send: c
z:org.apache.spark.sql.functions
col
svalue
e

2025-05-08 05:44:18,792 - DEBUG - Answer received: !yro36
2025-05-08 05:44:18,794 - DEBUG - Command to send: c
o36
cast
sstring
e

2025-05-08 05:44:18,817 - DEBUG - Answer received: !yro37
2025-05-08 05:44:18,819 - DEBUG - Command to send: r
u
functions
rj
e

2025-05-08 05:44:18,823 - DEBUG - Answer received: !ycorg.apache.spark.sql.functions
2025-05-08 05:44:18,824 - DEBUG - Command to send: r
m
org.apache.spark.sql.functions
from_json
e

2025-05-08 05:44:18,826 - DEBUG - Answer received: !ym
2025-05-08 05:44:18,827 - DEBUG - Command to send: i
java.util.HashMap
e

2025-05-08 05:44:18,829 - DEBUG - Answer received: !yao38
2025-05-08 05:44:18,830 - DEBUG - Command to send: c
z:org.apache.spark.sql.functions
from_json
ro37
s{"fields":[{"metadata":{},"name":"EventID","nullable":true,"type":"long"},{"metadata":{},"name":"EventType","nullable":true,"type":"string"},{"metadata":{},"name":"UserID","nullable":true,"type":"long"},{"metadata":{},"name":"Platform","nullable":true,"type":"string"},{"metadata":{},"name":"LiveEngagement","nullable":true,"type":"long"},{"metadata":{},"name":"ViewerCount","nullable":true,"type":"long"},{"metadata":{},"name":"StreamDuration","nullable":true,"type":"long"},{"metadata":{},"name":"DeviceType","nullable":true,"type":"string"},{"metadata":{},"name":"WatchTime","nullable":true,"type":"string"},{"metadata":{},"name":"AddictionLevel","nullable":true,"type":"long"}],"type":"struct"}
ro38
e

2025-05-08 05:44:19,111 - DEBUG - Answer received: !yro39
2025-05-08 05:44:19,113 - DEBUG - Command to send: c
o39
as
sdata
e

2025-05-08 05:44:19,116 - DEBUG - Answer received: !yro40
2025-05-08 05:44:19,118 - DEBUG - Command to send: r
u
PythonUtils
rj
e

2025-05-08 05:44:19,120 - DEBUG - Answer received: !ycorg.apache.spark.api.python.PythonUtils
2025-05-08 05:44:19,121 - DEBUG - Command to send: r
m
org.apache.spark.api.python.PythonUtils
toSeq
e

2025-05-08 05:44:19,124 - DEBUG - Answer received: !ym
2025-05-08 05:44:19,125 - DEBUG - Command to send: i
java.util.ArrayList
e

2025-05-08 05:44:19,127 - DEBUG - Answer received: !ylo41
2025-05-08 05:44:19,128 - DEBUG - Command to send: c
o41
add
ro40
e

2025-05-08 05:44:19,192 - DEBUG - Answer received: !ybtrue
2025-05-08 05:44:19,195 - DEBUG - Command to send: c
z:org.apache.spark.api.python.PythonUtils
toSeq
ro41
e

2025-05-08 05:44:19,199 - DEBUG - Answer received: !yro42
2025-05-08 05:44:19,205 - DEBUG - Command to send: c
o35
select
ro42
e

2025-05-08 05:44:19,513 - DEBUG - Command to send: m
d
o38
e

2025-05-08 05:44:19,516 - DEBUG - Answer received: !yv
2025-05-08 05:44:19,519 - DEBUG - Command to send: m
d
o41
e

2025-05-08 05:44:19,520 - DEBUG - Answer received: !yv
2025-05-08 05:44:20,540 - DEBUG - Answer received: !yro43
2025-05-08 05:44:20,541 - DEBUG - Command to send: r
u
functions
rj
e

2025-05-08 05:44:20,544 - DEBUG - Answer received: !ycorg.apache.spark.sql.functions
2025-05-08 05:44:20,545 - DEBUG - Command to send: r
m
org.apache.spark.sql.functions
col
e

2025-05-08 05:44:20,547 - DEBUG - Answer received: !ym
2025-05-08 05:44:20,548 - DEBUG - Command to send: c
z:org.apache.spark.sql.functions
col
sdata.*
e

2025-05-08 05:44:20,550 - DEBUG - Answer received: !yro44
2025-05-08 05:44:20,551 - DEBUG - Command to send: r
u
PythonUtils
rj
e

2025-05-08 05:44:20,553 - DEBUG - Answer received: !ycorg.apache.spark.api.python.PythonUtils
2025-05-08 05:44:20,554 - DEBUG - Command to send: r
m
org.apache.spark.api.python.PythonUtils
toSeq
e

2025-05-08 05:44:20,593 - DEBUG - Answer received: !ym
2025-05-08 05:44:20,594 - DEBUG - Command to send: i
java.util.ArrayList
e

2025-05-08 05:44:20,596 - DEBUG - Answer received: !ylo45
2025-05-08 05:44:20,597 - DEBUG - Command to send: c
o45
add
ro44
e

2025-05-08 05:44:20,598 - DEBUG - Answer received: !ybtrue
2025-05-08 05:44:20,599 - DEBUG - Command to send: c
z:org.apache.spark.api.python.PythonUtils
toSeq
ro45
e

2025-05-08 05:44:20,601 - DEBUG - Answer received: !yro46
2025-05-08 05:44:20,602 - DEBUG - Command to send: c
o43
select
ro46
e

2025-05-08 05:44:20,718 - DEBUG - Answer received: !yro47
2025-05-08 05:44:20,720 - DEBUG - Command to send: r
u
functions
rj
e

2025-05-08 05:44:20,723 - DEBUG - Answer received: !ycorg.apache.spark.sql.functions
2025-05-08 05:44:20,725 - DEBUG - Command to send: r
m
org.apache.spark.sql.functions
current_timestamp
e

2025-05-08 05:44:20,727 - DEBUG - Answer received: !ym
2025-05-08 05:44:20,728 - DEBUG - Command to send: c
z:org.apache.spark.sql.functions
current_timestamp
e

2025-05-08 05:44:20,729 - DEBUG - Answer received: !yro48
2025-05-08 05:44:20,730 - DEBUG - Command to send: c
o47
withColumn
sIngestionTimestamp
ro48
e

2025-05-08 05:44:20,834 - DEBUG - Answer received: !yro49
2025-05-08 05:44:20,893 - DEBUG - JSON parsed
2025-05-08 05:44:20,894 - INFO - Started streaming for live_streaming to Snowflake
2025-05-08 05:44:20,895 - DEBUG - Command to send: c
o49
writeStream
e

2025-05-08 05:44:20,928 - DEBUG - Answer received: !yro50
2025-05-08 05:44:20,929 - DEBUG - Command to send: c
o50
option
scheckpointLocation
s/app/checkpoints/stream_live_streaming
e

2025-05-08 05:44:20,935 - DEBUG - Answer received: !yro51
2025-05-08 05:44:20,993 - DEBUG - Command to send: c
o51
format
sconsole
e

2025-05-08 05:44:20,995 - DEBUG - Answer received: !yro52
2025-05-08 05:44:20,996 - DEBUG - Command to send: r
u
org
rj
e

2025-05-08 05:44:21,000 - DEBUG - Answer received: !yp
2025-05-08 05:44:21,001 - DEBUG - Command to send: r
u
org.apache
rj
e

2025-05-08 05:44:21,003 - DEBUG - Answer received: !yp
2025-05-08 05:44:21,004 - DEBUG - Command to send: r
u
org.apache.spark
rj
e

2025-05-08 05:44:21,006 - DEBUG - Answer received: !yp
2025-05-08 05:44:21,008 - DEBUG - Command to send: r
u
org.apache.spark.sql
rj
e

2025-05-08 05:44:21,010 - DEBUG - Answer received: !yp
2025-05-08 05:44:21,011 - DEBUG - Command to send: r
u
org.apache.spark.sql.streaming
rj
e

2025-05-08 05:44:21,013 - DEBUG - Answer received: !yp
2025-05-08 05:44:21,014 - DEBUG - Command to send: r
u
org.apache.spark.sql.streaming.Trigger
rj
e

2025-05-08 05:44:21,015 - DEBUG - Answer received: !ycorg.apache.spark.sql.streaming.Trigger
2025-05-08 05:44:21,016 - DEBUG - Command to send: r
m
org.apache.spark.sql.streaming.Trigger
ProcessingTime
e

2025-05-08 05:44:21,018 - DEBUG - Answer received: !ym
2025-05-08 05:44:21,019 - DEBUG - Command to send: c
z:org.apache.spark.sql.streaming.Trigger
ProcessingTime
s10 seconds
e

2025-05-08 05:44:21,093 - DEBUG - Answer received: !yro53
2025-05-08 05:44:21,094 - DEBUG - Command to send: c
o52
trigger
ro53
e

2025-05-08 05:44:21,105 - DEBUG - Answer received: !yro54
2025-05-08 05:44:21,107 - DEBUG - Command to send: c
o54
start
e

2025-05-08 05:44:21,592 - DEBUG - Command to send: m
d
o45
e

2025-05-08 05:44:21,594 - DEBUG - Answer received: !yv
2025-05-08 05:44:23,061 - DEBUG - Answer received: !yro55
2025-05-08 05:44:23,062 - DEBUG - Command to send: c
o55
awaitTermination
e

2025-05-08 05:57:52,053 - DEBUG - Answer received: 
2025-05-08 05:57:52,055 - INFO - Error while receiving.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty
2025-05-08 05:57:52,056 - INFO - Closing down clientserver connection
2025-05-08 05:57:52,057 - ERROR - Exception while sending command.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
2025-05-08 05:57:52,060 - INFO - Closing down clientserver connection
2025-05-08 05:57:52,061 - ERROR - Error in stream_live_streaming.py: An error occurred while calling o55.awaitTermination
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 89, in <module>
    cleaned_df = parsed_df.dropna(subset=critical_columns)
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/query.py", line 201, in awaitTermination
    return self._jsq.awaitTermination()
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 169, in deco
    return f(*a, **kw)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 334, in get_return_value
    raise Py4JError(
py4j.protocol.Py4JError: An error occurred while calling o55.awaitTermination
2025-05-08 05:57:52,105 - INFO - Closing down clientserver connection
2025-05-08 05:58:38,803 - INFO - Starting stream_live_streaming.py
2025-05-08 05:58:38,819 - INFO - Configurations loaded
2025-05-08 05:58:51,993 - INFO - Spark session initialized
2025-05-08 05:58:51,995 - INFO - Schema defined
2025-05-08 05:59:16,601 - INFO - Kafka stream initialized
2025-05-08 05:59:16,602 - ERROR - Error in stream_live_streaming.py: name 'from_json' is not defined
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 83, in <module>
    from_json(col("value").cast("string"), live_streaming_schema).alias("data")
NameError: name 'from_json' is not defined
2025-05-08 05:59:16,616 - INFO - Closing down clientserver connection
2025-05-08 05:59:40,914 - INFO - Starting stream_live_streaming.py
2025-05-08 05:59:40,930 - INFO - Configurations loaded
2025-05-08 06:00:04,916 - INFO - Spark session initialized
2025-05-08 06:00:04,917 - INFO - Schema defined
2025-05-08 06:00:48,948 - INFO - Starting stream_live_streaming.py
2025-05-08 06:00:49,139 - INFO - Configurations loaded
2025-05-08 06:00:52,747 - INFO - Kafka stream initialized
2025-05-08 06:00:52,750 - ERROR - Error in stream_live_streaming.py: name 'from_json' is not defined
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 83, in <module>
    from_json(col("value").cast("string"), live_streaming_schema).alias("data")
NameError: name 'from_json' is not defined
2025-05-08 06:00:52,941 - INFO - Closing down clientserver connection
2025-05-08 06:01:18,544 - INFO - Spark session initialized
2025-05-08 06:01:18,544 - INFO - Schema defined
2025-05-08 06:01:45,475 - INFO - Starting stream_live_streaming.py
2025-05-08 06:01:45,572 - INFO - Configurations loaded
2025-05-08 06:01:58,962 - INFO - Kafka stream initialized
2025-05-08 06:01:58,963 - ERROR - Error in stream_live_streaming.py: name 'from_json' is not defined
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 83, in <module>
    from_json(col("value").cast("string"), live_streaming_schema).alias("data")
NameError: name 'from_json' is not defined
2025-05-08 06:01:59,067 - INFO - Closing down clientserver connection
2025-05-08 06:02:07,173 - INFO - Spark session initialized
2025-05-08 06:02:07,177 - INFO - Schema defined
2025-05-08 06:02:29,918 - INFO - Kafka stream initialized
2025-05-08 06:02:29,920 - ERROR - Error in stream_live_streaming.py: name 'from_json' is not defined
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 83, in <module>
    from_json(col("value").cast("string"), live_streaming_schema).alias("data")
NameError: name 'from_json' is not defined
2025-05-08 06:02:29,985 - INFO - Closing down clientserver connection
2025-05-08 06:02:52,997 - INFO - Starting stream_live_streaming.py
2025-05-08 06:02:53,079 - INFO - Configurations loaded
2025-05-08 06:03:05,592 - INFO - Spark session initialized
2025-05-08 06:03:05,598 - INFO - Schema defined
2025-05-08 06:03:27,496 - INFO - Kafka stream initialized
2025-05-08 06:03:27,499 - ERROR - Error in stream_live_streaming.py: name 'from_json' is not defined
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 83, in <module>
    from_json(col("value").cast("string"), live_streaming_schema).alias("data")
NameError: name 'from_json' is not defined
2025-05-08 06:03:27,604 - INFO - Closing down clientserver connection
2025-05-08 06:03:51,529 - INFO - Starting stream_live_streaming.py
2025-05-08 06:03:51,604 - INFO - Configurations loaded
2025-05-08 06:04:02,723 - INFO - Spark session initialized
2025-05-08 06:04:02,724 - INFO - Schema defined
2025-05-08 06:04:21,415 - INFO - Kafka stream initialized
2025-05-08 06:04:21,416 - ERROR - Error in stream_live_streaming.py: name 'from_json' is not defined
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 83, in <module>
    from_json(col("value").cast("string"), live_streaming_schema).alias("data")
NameError: name 'from_json' is not defined
2025-05-08 06:04:21,426 - INFO - Closing down clientserver connection
2025-05-08 06:04:44,117 - INFO - Starting stream_live_streaming.py
2025-05-08 06:04:44,145 - INFO - Configurations loaded
2025-05-08 06:04:56,541 - INFO - Spark session initialized
2025-05-08 06:04:56,542 - INFO - Schema defined
2025-05-08 06:05:16,656 - INFO - Kafka stream initialized
2025-05-08 06:05:16,657 - ERROR - Error in stream_live_streaming.py: name 'from_json' is not defined
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 83, in <module>
    from_json(col("value").cast("string"), live_streaming_schema).alias("data")
NameError: name 'from_json' is not defined
2025-05-08 06:05:16,734 - INFO - Closing down clientserver connection
2025-05-08 06:05:38,939 - INFO - Starting stream_live_streaming.py
2025-05-08 06:05:38,949 - INFO - Configurations loaded
2025-05-08 06:05:50,273 - INFO - Spark session initialized
2025-05-08 06:05:50,274 - INFO - Schema defined
2025-05-08 06:06:08,341 - INFO - Kafka stream initialized
2025-05-08 06:06:08,343 - ERROR - Error in stream_live_streaming.py: name 'from_json' is not defined
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 83, in <module>
    from_json(col("value").cast("string"), live_streaming_schema).alias("data")
NameError: name 'from_json' is not defined
2025-05-08 06:06:08,357 - INFO - Closing down clientserver connection
2025-05-08 06:06:29,159 - INFO - Starting stream_live_streaming.py
2025-05-08 06:06:29,172 - INFO - Configurations loaded
2025-05-08 06:06:40,855 - INFO - Spark session initialized
2025-05-08 06:06:40,859 - INFO - Schema defined
2025-05-08 06:07:02,667 - INFO - Kafka stream initialized
2025-05-08 06:07:02,668 - ERROR - Error in stream_live_streaming.py: name 'from_json' is not defined
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 83, in <module>
    from_json(col("value").cast("string"), live_streaming_schema).alias("data")
NameError: name 'from_json' is not defined
2025-05-08 06:07:02,688 - INFO - Closing down clientserver connection
2025-05-08 06:07:26,009 - INFO - Starting stream_live_streaming.py
2025-05-08 06:07:26,083 - INFO - Configurations loaded
2025-05-08 06:07:39,180 - INFO - Spark session initialized
2025-05-08 06:07:39,181 - INFO - Schema defined
2025-05-08 06:07:57,507 - INFO - Kafka stream initialized
2025-05-08 06:07:57,508 - ERROR - Error in stream_live_streaming.py: name 'from_json' is not defined
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 83, in <module>
    from_json(col("value").cast("string"), live_streaming_schema).alias("data")
NameError: name 'from_json' is not defined
2025-05-08 06:07:57,516 - INFO - Closing down clientserver connection
2025-05-08 06:08:21,797 - INFO - Starting stream_live_streaming.py
2025-05-08 06:08:21,811 - INFO - Configurations loaded
2025-05-08 06:08:34,806 - INFO - Spark session initialized
2025-05-08 06:08:34,809 - INFO - Schema defined
2025-05-08 06:08:53,014 - INFO - Kafka stream initialized
2025-05-08 06:08:53,015 - ERROR - Error in stream_live_streaming.py: name 'from_json' is not defined
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 83, in <module>
    from_json(col("value").cast("string"), live_streaming_schema).alias("data")
NameError: name 'from_json' is not defined
2025-05-08 06:08:53,023 - INFO - Closing down clientserver connection
2025-05-08 06:09:15,835 - INFO - Starting stream_live_streaming.py
2025-05-08 06:09:15,917 - INFO - Configurations loaded
2025-05-08 06:09:27,629 - INFO - Spark session initialized
2025-05-08 06:09:27,630 - INFO - Schema defined
2025-05-08 06:09:47,335 - INFO - Kafka stream initialized
2025-05-08 06:09:47,335 - ERROR - Error in stream_live_streaming.py: name 'from_json' is not defined
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 83, in <module>
    from_json(col("value").cast("string"), live_streaming_schema).alias("data")
NameError: name 'from_json' is not defined
2025-05-08 06:09:47,343 - INFO - Closing down clientserver connection
2025-05-08 06:10:11,330 - INFO - Starting stream_live_streaming.py
2025-05-08 06:10:11,339 - INFO - Configurations loaded
2025-05-08 06:10:24,660 - INFO - Spark session initialized
2025-05-08 06:10:24,674 - INFO - Schema defined
2025-05-08 06:10:44,760 - INFO - Kafka stream initialized
2025-05-08 06:10:44,762 - ERROR - Error in stream_live_streaming.py: name 'from_json' is not defined
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 83, in <module>
    from_json(col("value").cast("string"), live_streaming_schema).alias("data")
NameError: name 'from_json' is not defined
2025-05-08 06:10:44,898 - INFO - Closing down clientserver connection
2025-05-08 06:16:59,077 - INFO - Starting stream_live_streaming.py
2025-05-08 06:16:59,101 - INFO - Configurations loaded
2025-05-08 06:17:11,887 - INFO - Spark session initialized
2025-05-08 06:17:11,888 - INFO - Schema defined
2025-05-08 06:17:34,871 - INFO - Kafka stream initialized
2025-05-08 06:17:37,682 - INFO - JSON parsing completed
2025-05-08 06:17:44,884 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 06:17:45,316 - ERROR - Error in stream_live_streaming.py: requirement failed: Private key must be specified in Snowflake streaming
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 274, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 265, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: requirement failed: Private key must be specified in Snowflake streaming
2025-05-08 06:17:45,482 - INFO - Closing down clientserver connection
2025-05-08 06:18:07,181 - INFO - Starting stream_live_streaming.py
2025-05-08 06:18:07,194 - INFO - Configurations loaded
2025-05-08 06:18:10,538 - INFO - Error while receiving.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty
2025-05-08 06:18:10,540 - INFO - Closing down clientserver connection
2025-05-08 06:18:10,541 - ERROR - Exception while sending command.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
2025-05-08 06:18:10,544 - INFO - Closing down clientserver connection
2025-05-08 06:18:10,544 - ERROR - Error in stream_live_streaming.py: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 45, in <module>
    spark = SparkSession.builder \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/session.py", line 477, in getOrCreate
    sc = SparkContext.getOrCreate(sparkConf)
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/context.py", line 512, in getOrCreate
    SparkContext(conf=conf or SparkConf())
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/context.py", line 200, in __init__
    self._do_init(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/context.py", line 287, in _do_init
    self._jsc = jsc or self._initialize_context(self._conf._jconf)
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/context.py", line 417, in _initialize_context
    return self._jvm.JavaSparkContext(jconf)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1587, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 334, in get_return_value
    raise Py4JError(
py4j.protocol.Py4JError: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext
2025-05-08 06:18:10,549 - INFO - Closing down clientserver connection
2025-05-08 06:18:56,605 - INFO - Starting stream_live_streaming.py
2025-05-08 06:18:56,617 - INFO - Configurations loaded
2025-05-08 06:19:09,318 - INFO - Spark session initialized
2025-05-08 06:19:09,319 - INFO - Schema defined
2025-05-08 06:19:42,912 - INFO - Kafka stream initialized
2025-05-08 06:19:48,836 - INFO - JSON parsing completed
2025-05-08 06:20:11,028 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 06:20:12,124 - ERROR - Error in stream_live_streaming.py: requirement failed: Private key must be specified in Snowflake streaming
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 274, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 265, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: requirement failed: Private key must be specified in Snowflake streaming
2025-05-08 06:20:12,529 - INFO - Closing down clientserver connection
2025-05-08 06:20:18,736 - INFO - Starting stream_live_streaming.py
2025-05-08 06:20:18,843 - INFO - Configurations loaded
2025-05-08 06:20:47,944 - INFO - Spark session initialized
2025-05-08 06:20:47,944 - INFO - Schema defined
2025-05-08 06:21:07,335 - INFO - Starting stream_live_streaming.py
2025-05-08 06:21:07,445 - INFO - Configurations loaded
2025-05-08 06:21:34,162 - INFO - Spark session initialized
2025-05-08 06:21:34,164 - INFO - Schema defined
2025-05-08 06:21:35,844 - INFO - Kafka stream initialized
2025-05-08 06:21:41,247 - INFO - JSON parsing completed
2025-05-08 06:22:05,677 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 06:22:06,956 - ERROR - Error in stream_live_streaming.py: requirement failed: Private key must be specified in Snowflake streaming
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 274, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 265, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: requirement failed: Private key must be specified in Snowflake streaming
2025-05-08 06:22:07,358 - INFO - Closing down clientserver connection
2025-05-08 06:22:22,464 - INFO - Kafka stream initialized
2025-05-08 06:22:25,166 - INFO - JSON parsing completed
2025-05-08 06:22:34,960 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 06:22:35,678 - ERROR - Error in stream_live_streaming.py: requirement failed: Private key must be specified in Snowflake streaming
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 274, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 265, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: requirement failed: Private key must be specified in Snowflake streaming
2025-05-08 06:22:35,967 - INFO - Closing down clientserver connection
2025-05-08 06:23:03,593 - INFO - Starting stream_live_streaming.py
2025-05-08 06:23:03,634 - INFO - Configurations loaded
2025-05-08 06:23:16,292 - INFO - Spark session initialized
2025-05-08 06:23:16,296 - INFO - Schema defined
2025-05-08 06:23:36,595 - INFO - Kafka stream initialized
2025-05-08 06:23:38,900 - INFO - JSON parsing completed
2025-05-08 06:23:47,696 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 06:23:48,120 - ERROR - Error in stream_live_streaming.py: requirement failed: Private key must be specified in Snowflake streaming
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 274, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 265, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: requirement failed: Private key must be specified in Snowflake streaming
2025-05-08 06:23:48,224 - INFO - Closing down clientserver connection
2025-05-08 06:24:14,888 - INFO - Starting stream_live_streaming.py
2025-05-08 06:24:14,905 - INFO - Configurations loaded
2025-05-08 06:24:27,220 - INFO - Spark session initialized
2025-05-08 06:24:27,221 - INFO - Schema defined
2025-05-08 06:24:50,018 - INFO - Kafka stream initialized
2025-05-08 06:24:52,131 - INFO - JSON parsing completed
2025-05-08 06:25:01,712 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 06:25:02,311 - ERROR - Error in stream_live_streaming.py: requirement failed: Private key must be specified in Snowflake streaming
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 274, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 265, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: requirement failed: Private key must be specified in Snowflake streaming
2025-05-08 06:25:02,511 - INFO - Closing down clientserver connection
2025-05-08 06:25:28,724 - INFO - Starting stream_live_streaming.py
2025-05-08 06:25:28,735 - INFO - Configurations loaded
2025-05-08 06:25:41,419 - INFO - Spark session initialized
2025-05-08 06:25:41,421 - INFO - Schema defined
2025-05-08 06:26:07,226 - INFO - Kafka stream initialized
2025-05-08 06:26:08,945 - INFO - JSON parsing completed
2025-05-08 06:26:19,838 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 06:26:20,827 - ERROR - Error in stream_live_streaming.py: requirement failed: Private key must be specified in Snowflake streaming
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 274, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 265, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: requirement failed: Private key must be specified in Snowflake streaming
2025-05-08 06:26:21,026 - INFO - Closing down clientserver connection
2025-05-08 06:26:45,154 - INFO - Starting stream_live_streaming.py
2025-05-08 06:26:45,228 - ERROR - Snowflake private key path is missing or invalid
2025-05-08 06:26:45,229 - ERROR - Error in stream_live_streaming.py: Snowflake private key path is missing or invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 35, in <module>
    raise ValueError("Snowflake private key path is missing or invalid")
ValueError: Snowflake private key path is missing or invalid
2025-05-08 06:27:08,641 - INFO - Starting stream_live_streaming.py
2025-05-08 06:27:08,650 - ERROR - Snowflake private key path is missing or invalid
2025-05-08 06:27:08,651 - ERROR - Error in stream_live_streaming.py: Snowflake private key path is missing or invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 35, in <module>
    raise ValueError("Snowflake private key path is missing or invalid")
ValueError: Snowflake private key path is missing or invalid
2025-05-08 06:27:29,841 - INFO - Starting stream_live_streaming.py
2025-05-08 06:27:29,856 - ERROR - Snowflake private key path is missing or invalid
2025-05-08 06:27:29,857 - ERROR - Error in stream_live_streaming.py: Snowflake private key path is missing or invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 35, in <module>
    raise ValueError("Snowflake private key path is missing or invalid")
ValueError: Snowflake private key path is missing or invalid
2025-05-08 06:27:51,450 - INFO - Starting stream_live_streaming.py
2025-05-08 06:27:51,459 - ERROR - Snowflake private key path is missing or invalid
2025-05-08 06:27:51,460 - ERROR - Error in stream_live_streaming.py: Snowflake private key path is missing or invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 35, in <module>
    raise ValueError("Snowflake private key path is missing or invalid")
ValueError: Snowflake private key path is missing or invalid
2025-05-08 06:28:42,079 - INFO - Starting stream_live_streaming.py
2025-05-08 06:28:42,089 - ERROR - Snowflake private key path is missing or invalid
2025-05-08 06:28:42,090 - ERROR - Error in stream_live_streaming.py: Snowflake private key path is missing or invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 35, in <module>
    raise ValueError("Snowflake private key path is missing or invalid")
ValueError: Snowflake private key path is missing or invalid
2025-05-08 06:29:03,772 - INFO - Starting stream_live_streaming.py
2025-05-08 06:29:03,778 - ERROR - Snowflake private key path is missing or invalid
2025-05-08 06:29:03,778 - ERROR - Error in stream_live_streaming.py: Snowflake private key path is missing or invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 35, in <module>
    raise ValueError("Snowflake private key path is missing or invalid")
ValueError: Snowflake private key path is missing or invalid
2025-05-08 06:29:25,498 - INFO - Starting stream_live_streaming.py
2025-05-08 06:29:25,506 - ERROR - Snowflake private key path is missing or invalid
2025-05-08 06:29:25,507 - ERROR - Error in stream_live_streaming.py: Snowflake private key path is missing or invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 35, in <module>
    raise ValueError("Snowflake private key path is missing or invalid")
ValueError: Snowflake private key path is missing or invalid
2025-05-08 06:29:47,191 - INFO - Starting stream_live_streaming.py
2025-05-08 06:29:47,197 - ERROR - Snowflake private key path is missing or invalid
2025-05-08 06:29:47,198 - ERROR - Error in stream_live_streaming.py: Snowflake private key path is missing or invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 35, in <module>
    raise ValueError("Snowflake private key path is missing or invalid")
ValueError: Snowflake private key path is missing or invalid
2025-05-08 06:30:07,508 - INFO - Starting stream_live_streaming.py
2025-05-08 06:30:07,514 - ERROR - Snowflake private key path is missing or invalid
2025-05-08 06:30:07,514 - ERROR - Error in stream_live_streaming.py: Snowflake private key path is missing or invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 35, in <module>
    raise ValueError("Snowflake private key path is missing or invalid")
ValueError: Snowflake private key path is missing or invalid
2025-05-08 06:30:29,413 - INFO - Starting stream_live_streaming.py
2025-05-08 06:30:29,420 - ERROR - Snowflake private key path is missing or invalid
2025-05-08 06:30:29,422 - ERROR - Error in stream_live_streaming.py: Snowflake private key path is missing or invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 35, in <module>
    raise ValueError("Snowflake private key path is missing or invalid")
ValueError: Snowflake private key path is missing or invalid
2025-05-08 06:30:50,127 - INFO - Starting stream_live_streaming.py
2025-05-08 06:30:50,133 - ERROR - Snowflake private key path is missing or invalid
2025-05-08 06:30:50,133 - ERROR - Error in stream_live_streaming.py: Snowflake private key path is missing or invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 35, in <module>
    raise ValueError("Snowflake private key path is missing or invalid")
ValueError: Snowflake private key path is missing or invalid
2025-05-08 06:31:10,903 - INFO - Starting stream_live_streaming.py
2025-05-08 06:31:10,910 - ERROR - Snowflake private key path is missing or invalid
2025-05-08 06:31:10,911 - ERROR - Error in stream_live_streaming.py: Snowflake private key path is missing or invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 35, in <module>
    raise ValueError("Snowflake private key path is missing or invalid")
ValueError: Snowflake private key path is missing or invalid
2025-05-08 06:31:30,437 - INFO - Starting stream_live_streaming.py
2025-05-08 06:31:30,443 - ERROR - Snowflake private key path is missing or invalid
2025-05-08 06:31:30,444 - ERROR - Error in stream_live_streaming.py: Snowflake private key path is missing or invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 35, in <module>
    raise ValueError("Snowflake private key path is missing or invalid")
ValueError: Snowflake private key path is missing or invalid
2025-05-08 06:31:52,141 - INFO - Starting stream_live_streaming.py
2025-05-08 06:31:52,154 - ERROR - Snowflake private key path is missing or invalid
2025-05-08 06:31:52,155 - ERROR - Error in stream_live_streaming.py: Snowflake private key path is missing or invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 35, in <module>
    raise ValueError("Snowflake private key path is missing or invalid")
ValueError: Snowflake private key path is missing or invalid
2025-05-08 06:32:15,425 - INFO - Starting stream_live_streaming.py
2025-05-08 06:32:15,433 - ERROR - Snowflake private key path is missing or invalid
2025-05-08 06:32:15,434 - ERROR - Error in stream_live_streaming.py: Snowflake private key path is missing or invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 35, in <module>
    raise ValueError("Snowflake private key path is missing or invalid")
ValueError: Snowflake private key path is missing or invalid
2025-05-08 06:32:38,218 - INFO - Starting stream_live_streaming.py
2025-05-08 06:32:38,227 - ERROR - Snowflake private key path is missing or invalid
2025-05-08 06:32:38,228 - ERROR - Error in stream_live_streaming.py: Snowflake private key path is missing or invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 35, in <module>
    raise ValueError("Snowflake private key path is missing or invalid")
ValueError: Snowflake private key path is missing or invalid
2025-05-08 06:33:00,005 - INFO - Starting stream_live_streaming.py
2025-05-08 06:33:00,011 - ERROR - Snowflake private key path is missing or invalid
2025-05-08 06:33:00,011 - ERROR - Error in stream_live_streaming.py: Snowflake private key path is missing or invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 35, in <module>
    raise ValueError("Snowflake private key path is missing or invalid")
ValueError: Snowflake private key path is missing or invalid
2025-05-08 06:33:21,747 - INFO - Starting stream_live_streaming.py
2025-05-08 06:33:21,757 - ERROR - Snowflake private key path is missing or invalid
2025-05-08 06:33:21,759 - ERROR - Error in stream_live_streaming.py: Snowflake private key path is missing or invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 35, in <module>
    raise ValueError("Snowflake private key path is missing or invalid")
ValueError: Snowflake private key path is missing or invalid
2025-05-08 06:33:43,644 - INFO - Starting stream_live_streaming.py
2025-05-08 06:33:43,653 - ERROR - Snowflake private key path is missing or invalid
2025-05-08 06:33:43,654 - ERROR - Error in stream_live_streaming.py: Snowflake private key path is missing or invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 35, in <module>
    raise ValueError("Snowflake private key path is missing or invalid")
ValueError: Snowflake private key path is missing or invalid
2025-05-08 06:34:05,539 - INFO - Starting stream_live_streaming.py
2025-05-08 06:34:05,547 - ERROR - Snowflake private key path is missing or invalid
2025-05-08 06:34:05,547 - ERROR - Error in stream_live_streaming.py: Snowflake private key path is missing or invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 35, in <module>
    raise ValueError("Snowflake private key path is missing or invalid")
ValueError: Snowflake private key path is missing or invalid
2025-05-08 06:34:26,974 - INFO - Starting stream_live_streaming.py
2025-05-08 06:34:26,981 - ERROR - Snowflake private key path is missing or invalid
2025-05-08 06:34:26,982 - ERROR - Error in stream_live_streaming.py: Snowflake private key path is missing or invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 35, in <module>
    raise ValueError("Snowflake private key path is missing or invalid")
ValueError: Snowflake private key path is missing or invalid
2025-05-08 06:34:58,531 - INFO - Starting stream_live_streaming.py
2025-05-08 06:34:58,543 - ERROR - Snowflake private key path is missing or invalid
2025-05-08 06:34:58,544 - ERROR - Error in stream_live_streaming.py: Snowflake private key path is missing or invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 35, in <module>
    raise ValueError("Snowflake private key path is missing or invalid")
ValueError: Snowflake private key path is missing or invalid
2025-05-08 06:35:14,736 - INFO - Starting stream_live_streaming.py
2025-05-08 06:35:14,749 - ERROR - Snowflake private key path is missing or invalid
2025-05-08 06:35:14,750 - ERROR - Error in stream_live_streaming.py: Snowflake private key path is missing or invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 35, in <module>
    raise ValueError("Snowflake private key path is missing or invalid")
ValueError: Snowflake private key path is missing or invalid
2025-05-08 06:35:25,393 - INFO - Starting stream_live_streaming.py
2025-05-08 06:35:25,398 - ERROR - Snowflake private key path is missing or invalid
2025-05-08 06:35:25,399 - ERROR - Error in stream_live_streaming.py: Snowflake private key path is missing or invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 35, in <module>
    raise ValueError("Snowflake private key path is missing or invalid")
ValueError: Snowflake private key path is missing or invalid
2025-05-08 06:36:18,756 - INFO - Starting stream_live_streaming.py
2025-05-08 06:36:18,765 - ERROR - Snowflake private key path is missing or invalid
2025-05-08 06:36:18,766 - ERROR - Error in stream_live_streaming.py: Snowflake private key path is missing or invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 35, in <module>
    raise ValueError("Snowflake private key path is missing or invalid")
ValueError: Snowflake private key path is missing or invalid
2025-05-08 06:36:39,092 - INFO - Starting stream_live_streaming.py
2025-05-08 06:36:39,098 - ERROR - Snowflake private key path is missing or invalid
2025-05-08 06:36:39,098 - ERROR - Error in stream_live_streaming.py: Snowflake private key path is missing or invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 35, in <module>
    raise ValueError("Snowflake private key path is missing or invalid")
ValueError: Snowflake private key path is missing or invalid
2025-05-08 06:36:58,874 - INFO - Starting stream_live_streaming.py
2025-05-08 06:36:58,884 - ERROR - Snowflake private key path is missing or invalid
2025-05-08 06:36:58,885 - ERROR - Error in stream_live_streaming.py: Snowflake private key path is missing or invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 35, in <module>
    raise ValueError("Snowflake private key path is missing or invalid")
ValueError: Snowflake private key path is missing or invalid
2025-05-08 06:37:18,824 - INFO - Starting stream_live_streaming.py
2025-05-08 06:37:18,832 - ERROR - Snowflake private key path is missing or invalid
2025-05-08 06:37:18,833 - ERROR - Error in stream_live_streaming.py: Snowflake private key path is missing or invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 35, in <module>
    raise ValueError("Snowflake private key path is missing or invalid")
ValueError: Snowflake private key path is missing or invalid
2025-05-08 06:37:39,600 - INFO - Starting stream_live_streaming.py
2025-05-08 06:37:39,621 - ERROR - Snowflake private key path is missing or invalid
2025-05-08 06:37:39,621 - ERROR - Error in stream_live_streaming.py: Snowflake private key path is missing or invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 35, in <module>
    raise ValueError("Snowflake private key path is missing or invalid")
ValueError: Snowflake private key path is missing or invalid
2025-05-08 06:38:42,608 - INFO - Starting stream_live_streaming.py
2025-05-08 06:38:42,614 - ERROR - Snowflake private key path is missing or invalid
2025-05-08 06:38:42,614 - ERROR - Error in stream_live_streaming.py: Snowflake private key path is missing or invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 35, in <module>
    raise ValueError("Snowflake private key path is missing or invalid")
ValueError: Snowflake private key path is missing or invalid
2025-05-08 06:39:03,120 - INFO - Starting stream_live_streaming.py
2025-05-08 06:39:03,130 - ERROR - Snowflake private key path is missing or invalid
2025-05-08 06:39:03,131 - ERROR - Error in stream_live_streaming.py: Snowflake private key path is missing or invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 35, in <module>
    raise ValueError("Snowflake private key path is missing or invalid")
ValueError: Snowflake private key path is missing or invalid
2025-05-08 06:39:44,710 - INFO - Starting stream_live_streaming.py
2025-05-08 06:39:44,801 - ERROR - Snowflake private key path is missing or invalid
2025-05-08 06:39:44,802 - ERROR - Error in stream_live_streaming.py: Snowflake private key path is missing or invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 35, in <module>
    raise ValueError("Snowflake private key path is missing or invalid")
ValueError: Snowflake private key path is missing or invalid
2025-05-08 06:39:53,989 - INFO - Starting stream_live_streaming.py
2025-05-08 06:39:54,002 - ERROR - Snowflake private key path is missing or invalid
2025-05-08 06:39:54,003 - ERROR - Error in stream_live_streaming.py: Snowflake private key path is missing or invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 35, in <module>
    raise ValueError("Snowflake private key path is missing or invalid")
ValueError: Snowflake private key path is missing or invalid
2025-05-08 06:40:13,839 - INFO - Starting stream_live_streaming.py
2025-05-08 06:40:13,847 - ERROR - Snowflake private key path is missing or invalid
2025-05-08 06:40:13,848 - ERROR - Error in stream_live_streaming.py: Snowflake private key path is missing or invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 35, in <module>
    raise ValueError("Snowflake private key path is missing or invalid")
ValueError: Snowflake private key path is missing or invalid
2025-05-08 06:40:34,357 - INFO - Starting stream_live_streaming.py
2025-05-08 06:40:34,391 - ERROR - Snowflake private key path is missing or invalid
2025-05-08 06:40:34,392 - ERROR - Error in stream_live_streaming.py: Snowflake private key path is missing or invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 35, in <module>
    raise ValueError("Snowflake private key path is missing or invalid")
ValueError: Snowflake private key path is missing or invalid
2025-05-08 06:40:53,960 - INFO - Starting stream_live_streaming.py
2025-05-08 06:40:53,969 - ERROR - Snowflake private key path is missing or invalid
2025-05-08 06:40:53,969 - ERROR - Error in stream_live_streaming.py: Snowflake private key path is missing or invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 35, in <module>
    raise ValueError("Snowflake private key path is missing or invalid")
ValueError: Snowflake private key path is missing or invalid
2025-05-08 06:41:14,130 - INFO - Starting stream_live_streaming.py
2025-05-08 06:41:14,138 - ERROR - Snowflake private key path is missing or invalid
2025-05-08 06:41:14,139 - ERROR - Error in stream_live_streaming.py: Snowflake private key path is missing or invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 35, in <module>
    raise ValueError("Snowflake private key path is missing or invalid")
ValueError: Snowflake private key path is missing or invalid
2025-05-08 06:41:34,339 - INFO - Starting stream_live_streaming.py
2025-05-08 06:41:34,347 - ERROR - Snowflake private key path is missing or invalid
2025-05-08 06:41:34,348 - ERROR - Error in stream_live_streaming.py: Snowflake private key path is missing or invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 35, in <module>
    raise ValueError("Snowflake private key path is missing or invalid")
ValueError: Snowflake private key path is missing or invalid
2025-05-08 06:41:54,312 - INFO - Starting stream_live_streaming.py
2025-05-08 06:41:54,419 - INFO - Snowflake configurations loaded
2025-05-08 06:41:54,427 - INFO - AWS configurations loaded
2025-05-08 06:41:54,940 - INFO - Error while receiving.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty
2025-05-08 06:41:54,942 - INFO - Closing down clientserver connection
2025-05-08 06:41:54,943 - ERROR - Exception while sending command.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
2025-05-08 06:41:54,947 - INFO - Closing down clientserver connection
2025-05-08 06:41:54,948 - ERROR - Error in stream_live_streaming.py: An error occurred while calling None.org.apache.spark.SparkConf
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 60, in <module>
    spark = SparkSession.builder \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/session.py", line 477, in getOrCreate
    sc = SparkContext.getOrCreate(sparkConf)
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/context.py", line 512, in getOrCreate
    SparkContext(conf=conf or SparkConf())
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/context.py", line 200, in __init__
    self._do_init(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/context.py", line 242, in _do_init
    self._conf = SparkConf(_jvm=SparkContext._jvm)
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/conf.py", line 131, in __init__
    self._jconf = _jvm.SparkConf(loadDefaults)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1587, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 334, in get_return_value
    raise Py4JError(
py4j.protocol.Py4JError: An error occurred while calling None.org.apache.spark.SparkConf
2025-05-08 06:42:46,839 - INFO - Starting stream_live_streaming.py
2025-05-08 06:42:46,920 - INFO - Snowflake configurations loaded
2025-05-08 06:42:46,932 - INFO - AWS configurations loaded
2025-05-08 06:43:08,623 - INFO - Spark session initialized
2025-05-08 06:43:08,624 - INFO - Schema defined
2025-05-08 06:43:49,245 - INFO - Starting stream_live_streaming.py
2025-05-08 06:43:49,338 - INFO - Snowflake configurations loaded
2025-05-08 06:43:49,345 - INFO - AWS configurations loaded
2025-05-08 06:43:55,931 - INFO - Kafka stream initialized
2025-05-08 06:44:01,048 - INFO - JSON parsing completed
2025-05-08 06:44:21,536 - INFO - Spark session initialized
2025-05-08 06:44:21,552 - INFO - Schema defined
2025-05-08 06:44:26,035 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 06:44:28,146 - ERROR - Error in stream_live_streaming.py: A snowflake password or private key path or OAuth token must be provided with 'sfpassword or pem_private_key' or 'sftoken' parameter, e.g. 'password'
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 289, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 280, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: A snowflake password or private key path or OAuth token must be provided with 'sfpassword or pem_private_key' or 'sftoken' parameter, e.g. 'password'
2025-05-08 06:44:28,552 - INFO - Closing down clientserver connection
2025-05-08 06:45:14,972 - INFO - Kafka stream initialized
2025-05-08 06:45:19,356 - INFO - JSON parsing completed
2025-05-08 06:45:23,752 - INFO - Starting stream_live_streaming.py
2025-05-08 06:45:23,767 - INFO - Snowflake configurations loaded
2025-05-08 06:45:23,771 - INFO - AWS configurations loaded
2025-05-08 06:45:37,874 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 06:45:38,969 - ERROR - Error in stream_live_streaming.py: A snowflake password or private key path or OAuth token must be provided with 'sfpassword or pem_private_key' or 'sftoken' parameter, e.g. 'password'
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 289, in <module>
    dataframe.writeStream \
  File "/app/scripts/stream_live_streaming.py", line 280, in write_to_snowflake
    col("IngestionTimestamp").cast("timestamp")
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: A snowflake password or private key path or OAuth token must be provided with 'sfpassword or pem_private_key' or 'sftoken' parameter, e.g. 'password'
2025-05-08 06:45:39,351 - INFO - Closing down clientserver connection
2025-05-08 06:45:47,767 - INFO - Spark session initialized
2025-05-08 06:45:47,769 - INFO - Schema defined
2025-05-08 06:46:16,468 - INFO - Kafka stream initialized
2025-05-08 06:46:18,889 - INFO - JSON parsing completed
2025-05-08 06:46:29,768 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 06:46:30,296 - ERROR - Error in stream_live_streaming.py: A snowflake password or private key path or OAuth token must be provided with 'sfpassword or pem_private_key' or 'sftoken' parameter, e.g. 'password'
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 289, in <module>
    dataframe.writeStream \
  File "/app/scripts/stream_live_streaming.py", line 280, in write_to_snowflake
    col("IngestionTimestamp").cast("timestamp")
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: A snowflake password or private key path or OAuth token must be provided with 'sfpassword or pem_private_key' or 'sftoken' parameter, e.g. 'password'
2025-05-08 06:46:30,586 - INFO - Closing down clientserver connection
2025-05-08 06:46:53,623 - INFO - Starting stream_live_streaming.py
2025-05-08 06:46:53,631 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 06:46:53,638 - INFO - Private key loaded successfully
2025-05-08 06:46:53,639 - INFO - Snowflake configurations loaded
2025-05-08 06:46:53,647 - INFO - AWS configurations loaded
2025-05-08 06:47:07,689 - INFO - Spark session initialized
2025-05-08 06:47:07,690 - INFO - Schema defined
2025-05-08 06:47:28,173 - INFO - Kafka stream initialized
2025-05-08 06:47:30,398 - INFO - JSON parsing completed
2025-05-08 06:47:39,176 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 06:47:39,585 - ERROR - Error in stream_live_streaming.py: A snowflake password or private key path or OAuth token must be provided with 'sfpassword or pem_private_key' or 'sftoken' parameter, e.g. 'password'
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 298, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 289, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: A snowflake password or private key path or OAuth token must be provided with 'sfpassword or pem_private_key' or 'sftoken' parameter, e.g. 'password'
2025-05-08 06:47:39,689 - INFO - Closing down clientserver connection
2025-05-08 06:48:04,123 - INFO - Starting stream_live_streaming.py
2025-05-08 06:48:04,197 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 06:48:04,206 - INFO - Private key loaded successfully
2025-05-08 06:48:04,207 - INFO - Snowflake configurations loaded
2025-05-08 06:48:04,222 - INFO - AWS configurations loaded
2025-05-08 06:48:16,184 - INFO - Spark session initialized
2025-05-08 06:48:16,186 - INFO - Schema defined
2025-05-08 06:48:39,308 - INFO - Kafka stream initialized
2025-05-08 06:48:41,794 - INFO - JSON parsing completed
2025-05-08 06:48:51,792 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 06:48:52,502 - ERROR - Error in stream_live_streaming.py: A snowflake password or private key path or OAuth token must be provided with 'sfpassword or pem_private_key' or 'sftoken' parameter, e.g. 'password'
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 298, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 289, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: A snowflake password or private key path or OAuth token must be provided with 'sfpassword or pem_private_key' or 'sftoken' parameter, e.g. 'password'
2025-05-08 06:48:52,609 - INFO - Closing down clientserver connection
2025-05-08 06:49:18,002 - INFO - Starting stream_live_streaming.py
2025-05-08 06:49:18,008 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 06:49:18,016 - INFO - Private key loaded successfully
2025-05-08 06:49:18,017 - INFO - Snowflake configurations loaded
2025-05-08 06:49:18,022 - INFO - AWS configurations loaded
2025-05-08 06:49:31,620 - INFO - Spark session initialized
2025-05-08 06:49:31,621 - INFO - Schema defined
2025-05-08 06:49:52,625 - INFO - Kafka stream initialized
2025-05-08 06:49:55,318 - INFO - JSON parsing completed
2025-05-08 06:50:06,419 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 06:50:08,717 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 298, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 289, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 06:50:08,838 - INFO - Closing down clientserver connection
2025-05-08 06:50:32,816 - INFO - Starting stream_live_streaming.py
2025-05-08 06:50:32,826 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 06:50:32,833 - INFO - Private key loaded successfully
2025-05-08 06:50:32,834 - INFO - Snowflake configurations loaded
2025-05-08 06:50:32,840 - INFO - AWS configurations loaded
2025-05-08 06:50:45,434 - INFO - Spark session initialized
2025-05-08 06:50:45,435 - INFO - Schema defined
2025-05-08 06:51:04,632 - INFO - Kafka stream initialized
2025-05-08 06:51:05,929 - INFO - JSON parsing completed
2025-05-08 06:51:14,543 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 06:51:16,744 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 298, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 289, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 06:51:16,930 - INFO - Closing down clientserver connection
2025-05-08 06:51:39,239 - INFO - Starting stream_live_streaming.py
2025-05-08 06:51:39,254 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 06:51:39,330 - INFO - Private key loaded successfully
2025-05-08 06:51:39,331 - INFO - Snowflake configurations loaded
2025-05-08 06:51:39,340 - INFO - AWS configurations loaded
2025-05-08 06:51:51,243 - INFO - Spark session initialized
2025-05-08 06:51:51,246 - INFO - Schema defined
2025-05-08 06:52:12,634 - INFO - Kafka stream initialized
2025-05-08 06:52:14,732 - INFO - JSON parsing completed
2025-05-08 06:52:24,931 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 06:52:26,629 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 298, in <module>
    .start()
  File "/app/scripts/stream_live_streaming.py", line 289, in write_to_snowflake
    logger.error(f"DataFrame for table {table_name} is None")
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 06:52:26,766 - INFO - Closing down clientserver connection
2025-05-08 06:52:50,639 - INFO - Starting stream_live_streaming.py
2025-05-08 06:52:50,647 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 06:52:50,656 - INFO - Private key loaded successfully
2025-05-08 06:52:50,656 - INFO - Snowflake configurations loaded
2025-05-08 06:52:50,663 - INFO - AWS configurations loaded
2025-05-08 06:53:01,732 - INFO - Spark session initialized
2025-05-08 06:53:01,737 - INFO - Schema defined
2025-05-08 06:53:23,886 - INFO - Error while receiving.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty
2025-05-08 06:53:23,934 - INFO - Closing down clientserver connection
2025-05-08 06:53:23,936 - ERROR - Exception while sending command.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
2025-05-08 06:53:23,939 - INFO - Closing down clientserver connection
2025-05-08 06:53:23,940 - ERROR - Error in stream_live_streaming.py: An error occurred while calling o42.load
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 99, in <module>
    kafka_df = spark.readStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 277, in load
    return self._df(self._jreader.load())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 169, in deco
    return f(*a, **kw)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 334, in get_return_value
    raise Py4JError(
py4j.protocol.Py4JError: An error occurred while calling o42.load
2025-05-08 06:53:23,948 - INFO - Closing down clientserver connection
2025-05-08 06:54:11,744 - INFO - Starting stream_live_streaming.py
2025-05-08 06:54:11,751 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 06:54:11,758 - INFO - Private key loaded successfully
2025-05-08 06:54:11,759 - INFO - Snowflake configurations loaded
2025-05-08 06:54:11,766 - INFO - AWS configurations loaded
2025-05-08 06:54:29,543 - INFO - Spark session initialized
2025-05-08 06:54:29,553 - INFO - Schema defined
2025-05-08 06:55:00,943 - INFO - Kafka stream initialized
2025-05-08 06:55:05,554 - INFO - JSON parsing completed
2025-05-08 06:55:20,767 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 06:55:23,866 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 06:55:24,156 - INFO - Closing down clientserver connection
2025-05-08 06:56:01,851 - INFO - Starting stream_live_streaming.py
2025-05-08 06:56:01,859 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 06:56:01,872 - INFO - Private key loaded successfully
2025-05-08 06:56:01,874 - INFO - Snowflake configurations loaded
2025-05-08 06:56:01,884 - INFO - AWS configurations loaded
2025-05-08 06:56:15,457 - INFO - Spark session initialized
2025-05-08 06:56:15,459 - INFO - Schema defined
2025-05-08 06:56:37,080 - INFO - Kafka stream initialized
2025-05-08 06:56:39,861 - INFO - JSON parsing completed
2025-05-08 06:56:49,363 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 06:56:51,380 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 06:56:51,571 - INFO - Closing down clientserver connection
2025-05-08 06:57:14,572 - INFO - Starting stream_live_streaming.py
2025-05-08 06:57:14,581 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 06:57:14,589 - INFO - Private key loaded successfully
2025-05-08 06:57:14,590 - INFO - Snowflake configurations loaded
2025-05-08 06:57:14,597 - INFO - AWS configurations loaded
2025-05-08 06:57:26,264 - INFO - Spark session initialized
2025-05-08 06:57:26,270 - INFO - Schema defined
2025-05-08 06:57:44,866 - INFO - Kafka stream initialized
2025-05-08 06:57:47,371 - INFO - JSON parsing completed
2025-05-08 06:57:58,167 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 06:58:00,577 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 06:58:00,776 - INFO - Closing down clientserver connection
2025-05-08 06:58:29,209 - INFO - Starting stream_live_streaming.py
2025-05-08 06:58:29,273 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 06:58:29,287 - INFO - Private key loaded successfully
2025-05-08 06:58:29,288 - INFO - Snowflake configurations loaded
2025-05-08 06:58:29,296 - INFO - AWS configurations loaded
2025-05-08 06:58:40,887 - INFO - Spark session initialized
2025-05-08 06:58:40,893 - INFO - Schema defined
2025-05-08 06:58:59,788 - INFO - Kafka stream initialized
2025-05-08 06:59:01,892 - INFO - JSON parsing completed
2025-05-08 06:59:11,500 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 06:59:13,685 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 06:59:13,798 - INFO - Closing down clientserver connection
2025-05-08 06:59:36,222 - INFO - Starting stream_live_streaming.py
2025-05-08 06:59:36,282 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 06:59:36,290 - INFO - Private key loaded successfully
2025-05-08 06:59:36,291 - INFO - Snowflake configurations loaded
2025-05-08 06:59:36,295 - INFO - AWS configurations loaded
2025-05-08 06:59:47,000 - INFO - Spark session initialized
2025-05-08 06:59:47,002 - INFO - Schema defined
2025-05-08 07:00:06,796 - INFO - Kafka stream initialized
2025-05-08 07:00:08,996 - INFO - JSON parsing completed
2025-05-08 07:00:18,292 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 07:00:20,991 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 07:00:21,099 - INFO - Closing down clientserver connection
2025-05-08 07:00:43,908 - INFO - Starting stream_live_streaming.py
2025-05-08 07:00:43,915 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 07:00:43,923 - INFO - Private key loaded successfully
2025-05-08 07:00:43,990 - INFO - Snowflake configurations loaded
2025-05-08 07:00:43,997 - INFO - AWS configurations loaded
2025-05-08 07:00:56,006 - INFO - Spark session initialized
2025-05-08 07:00:56,008 - INFO - Schema defined
2025-05-08 07:01:19,101 - INFO - Kafka stream initialized
2025-05-08 07:01:20,694 - INFO - JSON parsing completed
2025-05-08 07:01:29,617 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 07:01:32,307 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 07:01:32,496 - INFO - Closing down clientserver connection
2025-05-08 07:01:55,635 - INFO - Starting stream_live_streaming.py
2025-05-08 07:01:55,708 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 07:01:55,717 - INFO - Private key loaded successfully
2025-05-08 07:01:55,719 - INFO - Snowflake configurations loaded
2025-05-08 07:01:55,728 - INFO - AWS configurations loaded
2025-05-08 07:02:10,307 - INFO - Spark session initialized
2025-05-08 07:02:10,311 - INFO - Schema defined
2025-05-08 07:02:34,114 - INFO - Kafka stream initialized
2025-05-08 07:02:36,732 - INFO - JSON parsing completed
2025-05-08 07:02:46,223 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 07:02:48,219 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 07:02:48,314 - INFO - Closing down clientserver connection
2025-05-08 07:03:15,143 - INFO - Starting stream_live_streaming.py
2025-05-08 07:03:15,215 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 07:03:15,240 - INFO - Private key loaded successfully
2025-05-08 07:03:15,241 - INFO - Snowflake configurations loaded
2025-05-08 07:03:15,246 - INFO - AWS configurations loaded
2025-05-08 07:03:27,221 - INFO - Spark session initialized
2025-05-08 07:03:27,222 - INFO - Schema defined
2025-05-08 07:03:54,047 - INFO - Kafka stream initialized
2025-05-08 07:03:56,430 - INFO - JSON parsing completed
2025-05-08 07:04:07,337 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 07:04:09,628 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 07:04:09,744 - INFO - Closing down clientserver connection
2025-05-08 07:04:34,525 - INFO - Starting stream_live_streaming.py
2025-05-08 07:04:34,535 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 07:04:34,545 - INFO - Private key loaded successfully
2025-05-08 07:04:34,546 - INFO - Snowflake configurations loaded
2025-05-08 07:04:34,553 - INFO - AWS configurations loaded
2025-05-08 07:04:46,930 - INFO - Spark session initialized
2025-05-08 07:04:46,931 - INFO - Schema defined
2025-05-08 07:05:12,940 - INFO - Kafka stream initialized
2025-05-08 07:05:15,221 - INFO - JSON parsing completed
2025-05-08 07:05:23,652 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 07:05:25,839 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 07:05:26,029 - INFO - Closing down clientserver connection
2025-05-08 07:05:50,244 - INFO - Starting stream_live_streaming.py
2025-05-08 07:05:50,314 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 07:05:50,323 - INFO - Private key loaded successfully
2025-05-08 07:05:50,324 - INFO - Snowflake configurations loaded
2025-05-08 07:05:50,331 - INFO - AWS configurations loaded
2025-05-08 07:06:02,523 - INFO - Spark session initialized
2025-05-08 07:06:02,524 - INFO - Schema defined
2025-05-08 07:06:26,129 - INFO - Kafka stream initialized
2025-05-08 07:06:29,618 - INFO - JSON parsing completed
2025-05-08 07:06:39,734 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 07:06:42,028 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 07:06:42,150 - INFO - Closing down clientserver connection
2025-05-08 07:07:06,254 - INFO - Starting stream_live_streaming.py
2025-05-08 07:07:06,263 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 07:07:06,332 - INFO - Private key loaded successfully
2025-05-08 07:07:06,333 - INFO - Snowflake configurations loaded
2025-05-08 07:07:06,340 - INFO - AWS configurations loaded
2025-05-08 07:07:18,338 - INFO - Spark session initialized
2025-05-08 07:07:18,340 - INFO - Schema defined
2025-05-08 07:07:40,839 - INFO - Kafka stream initialized
2025-05-08 07:07:43,255 - INFO - JSON parsing completed
2025-05-08 07:07:53,954 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 07:07:55,832 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 07:07:56,037 - INFO - Closing down clientserver connection
2025-05-08 07:08:20,268 - INFO - Starting stream_live_streaming.py
2025-05-08 07:08:20,274 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 07:08:20,278 - INFO - Private key loaded successfully
2025-05-08 07:08:20,279 - INFO - Snowflake configurations loaded
2025-05-08 07:08:20,340 - INFO - AWS configurations loaded
2025-05-08 07:08:29,839 - INFO - Spark session initialized
2025-05-08 07:08:29,841 - INFO - Schema defined
2025-05-08 07:08:49,758 - INFO - Kafka stream initialized
2025-05-08 07:08:52,342 - INFO - JSON parsing completed
2025-05-08 07:09:02,136 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 07:09:04,541 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 07:09:04,653 - INFO - Closing down clientserver connection
2025-05-08 07:09:30,543 - INFO - Starting stream_live_streaming.py
2025-05-08 07:09:30,549 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 07:09:30,556 - INFO - Private key loaded successfully
2025-05-08 07:09:30,556 - INFO - Snowflake configurations loaded
2025-05-08 07:09:30,562 - INFO - AWS configurations loaded
2025-05-08 07:09:40,927 - INFO - Spark session initialized
2025-05-08 07:09:40,930 - INFO - Schema defined
2025-05-08 07:10:03,846 - INFO - Kafka stream initialized
2025-05-08 07:10:05,630 - INFO - JSON parsing completed
2025-05-08 07:10:15,152 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 07:10:17,151 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 07:10:17,434 - INFO - Closing down clientserver connection
2025-05-08 07:10:43,387 - INFO - Starting stream_live_streaming.py
2025-05-08 07:10:43,442 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 07:10:43,454 - INFO - Private key loaded successfully
2025-05-08 07:10:43,456 - INFO - Snowflake configurations loaded
2025-05-08 07:10:43,462 - INFO - AWS configurations loaded
2025-05-08 07:10:55,241 - INFO - Spark session initialized
2025-05-08 07:10:55,243 - INFO - Schema defined
2025-05-08 07:11:17,241 - INFO - Kafka stream initialized
2025-05-08 07:11:19,447 - INFO - JSON parsing completed
2025-05-08 07:11:28,462 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 07:11:30,779 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 07:11:30,951 - INFO - Closing down clientserver connection
2025-05-08 07:11:58,577 - INFO - Starting stream_live_streaming.py
2025-05-08 07:11:58,652 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 07:11:58,667 - INFO - Private key loaded successfully
2025-05-08 07:11:58,669 - INFO - Snowflake configurations loaded
2025-05-08 07:11:58,681 - INFO - AWS configurations loaded
2025-05-08 07:12:08,959 - INFO - Spark session initialized
2025-05-08 07:12:08,960 - INFO - Schema defined
2025-05-08 07:12:30,974 - INFO - Kafka stream initialized
2025-05-08 07:12:33,278 - INFO - JSON parsing completed
2025-05-08 07:12:41,959 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 07:12:43,774 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 07:12:43,964 - INFO - Closing down clientserver connection
2025-05-08 07:13:08,882 - INFO - Starting stream_live_streaming.py
2025-05-08 07:13:08,887 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 07:13:08,961 - INFO - Private key loaded successfully
2025-05-08 07:13:08,962 - INFO - Snowflake configurations loaded
2025-05-08 07:13:08,967 - INFO - AWS configurations loaded
2025-05-08 07:13:20,755 - INFO - Spark session initialized
2025-05-08 07:13:20,756 - INFO - Schema defined
2025-05-08 07:13:41,276 - INFO - Kafka stream initialized
2025-05-08 07:13:43,388 - INFO - JSON parsing completed
2025-05-08 07:13:52,267 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 07:13:54,057 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 07:13:54,170 - INFO - Closing down clientserver connection
2025-05-08 07:14:17,288 - INFO - Starting stream_live_streaming.py
2025-05-08 07:14:17,296 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 07:14:17,367 - INFO - Private key loaded successfully
2025-05-08 07:14:17,369 - INFO - Snowflake configurations loaded
2025-05-08 07:14:17,377 - INFO - AWS configurations loaded
2025-05-08 07:14:30,173 - INFO - Spark session initialized
2025-05-08 07:14:30,174 - INFO - Schema defined
2025-05-08 07:14:53,570 - INFO - Kafka stream initialized
2025-05-08 07:14:55,377 - INFO - JSON parsing completed
2025-05-08 07:15:03,867 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 07:15:05,597 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 07:15:05,695 - INFO - Closing down clientserver connection
2025-05-08 07:15:29,201 - INFO - Starting stream_live_streaming.py
2025-05-08 07:15:29,209 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 07:15:29,277 - INFO - Private key loaded successfully
2025-05-08 07:15:29,278 - INFO - Snowflake configurations loaded
2025-05-08 07:15:29,285 - INFO - AWS configurations loaded
2025-05-08 07:15:41,985 - INFO - Spark session initialized
2025-05-08 07:15:41,986 - INFO - Schema defined
2025-05-08 07:16:02,882 - INFO - Kafka stream initialized
2025-05-08 07:16:05,194 - INFO - JSON parsing completed
2025-05-08 07:16:14,977 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 07:16:16,805 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 07:16:16,981 - INFO - Closing down clientserver connection
2025-05-08 07:16:40,602 - INFO - Starting stream_live_streaming.py
2025-05-08 07:16:40,610 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 07:16:40,617 - INFO - Private key loaded successfully
2025-05-08 07:16:40,618 - INFO - Snowflake configurations loaded
2025-05-08 07:16:40,625 - INFO - AWS configurations loaded
2025-05-08 07:16:51,815 - INFO - Spark session initialized
2025-05-08 07:16:51,816 - INFO - Schema defined
2025-05-08 07:17:15,419 - INFO - Kafka stream initialized
2025-05-08 07:17:17,690 - INFO - JSON parsing completed
2025-05-08 07:17:27,002 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 07:17:28,982 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 07:17:29,092 - INFO - Closing down clientserver connection
2025-05-08 07:17:52,434 - INFO - Starting stream_live_streaming.py
2025-05-08 07:17:52,497 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 07:17:52,515 - INFO - Private key loaded successfully
2025-05-08 07:17:52,517 - INFO - Snowflake configurations loaded
2025-05-08 07:17:52,524 - INFO - AWS configurations loaded
2025-05-08 07:18:04,200 - INFO - Spark session initialized
2025-05-08 07:18:04,201 - INFO - Schema defined
2025-05-08 07:18:28,929 - INFO - Kafka stream initialized
2025-05-08 07:18:31,224 - INFO - JSON parsing completed
2025-05-08 07:18:40,243 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 07:18:41,927 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 07:18:42,115 - INFO - Closing down clientserver connection
2025-05-08 07:19:05,405 - INFO - Starting stream_live_streaming.py
2025-05-08 07:19:05,415 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 07:19:05,423 - INFO - Private key loaded successfully
2025-05-08 07:19:05,424 - INFO - Snowflake configurations loaded
2025-05-08 07:19:05,430 - INFO - AWS configurations loaded
2025-05-08 07:19:18,540 - INFO - Spark session initialized
2025-05-08 07:19:18,541 - INFO - Schema defined
2025-05-08 07:19:39,819 - INFO - Kafka stream initialized
2025-05-08 07:19:42,017 - INFO - JSON parsing completed
2025-05-08 07:19:52,022 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 07:19:54,112 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 07:19:54,235 - INFO - Closing down clientserver connection
2025-05-08 07:20:16,914 - INFO - Starting stream_live_streaming.py
2025-05-08 07:20:16,920 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 07:20:16,927 - INFO - Private key loaded successfully
2025-05-08 07:20:16,928 - INFO - Snowflake configurations loaded
2025-05-08 07:20:16,934 - INFO - AWS configurations loaded
2025-05-08 07:20:29,630 - INFO - Spark session initialized
2025-05-08 07:20:29,632 - INFO - Schema defined
2025-05-08 07:20:49,745 - INFO - Kafka stream initialized
2025-05-08 07:20:51,739 - INFO - JSON parsing completed
2025-05-08 07:21:00,737 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 07:21:02,625 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 07:21:02,736 - INFO - Closing down clientserver connection
2025-05-08 07:21:25,733 - INFO - Starting stream_live_streaming.py
2025-05-08 07:21:25,741 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 07:21:25,751 - INFO - Private key loaded successfully
2025-05-08 07:21:25,752 - INFO - Snowflake configurations loaded
2025-05-08 07:21:25,760 - INFO - AWS configurations loaded
2025-05-08 07:21:38,333 - INFO - Spark session initialized
2025-05-08 07:21:38,334 - INFO - Schema defined
2025-05-08 07:21:59,339 - INFO - Kafka stream initialized
2025-05-08 07:22:01,249 - INFO - JSON parsing completed
2025-05-08 07:22:09,619 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 07:22:11,452 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 07:22:11,557 - INFO - Closing down clientserver connection
2025-05-08 07:22:34,868 - INFO - Starting stream_live_streaming.py
2025-05-08 07:22:34,876 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 07:22:34,922 - INFO - Private key loaded successfully
2025-05-08 07:22:34,923 - INFO - Snowflake configurations loaded
2025-05-08 07:22:34,930 - INFO - AWS configurations loaded
2025-05-08 07:22:47,639 - INFO - Spark session initialized
2025-05-08 07:22:47,640 - INFO - Schema defined
2025-05-08 07:23:06,340 - INFO - Kafka stream initialized
2025-05-08 07:23:08,037 - INFO - JSON parsing completed
2025-05-08 07:23:16,126 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 07:23:17,751 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 07:23:17,928 - INFO - Closing down clientserver connection
2025-05-08 07:23:42,271 - INFO - Starting stream_live_streaming.py
2025-05-08 07:23:42,279 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 07:23:42,287 - INFO - Private key loaded successfully
2025-05-08 07:23:42,326 - INFO - Snowflake configurations loaded
2025-05-08 07:23:42,333 - INFO - AWS configurations loaded
2025-05-08 07:23:54,726 - INFO - Spark session initialized
2025-05-08 07:23:54,728 - INFO - Schema defined
2025-05-08 07:24:14,554 - INFO - Kafka stream initialized
2025-05-08 07:24:16,953 - INFO - JSON parsing completed
2025-05-08 07:24:25,746 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 07:24:27,646 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 07:24:27,761 - INFO - Closing down clientserver connection
2025-05-08 07:24:51,362 - INFO - Starting stream_live_streaming.py
2025-05-08 07:24:51,371 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 07:24:51,379 - INFO - Private key loaded successfully
2025-05-08 07:24:51,381 - INFO - Snowflake configurations loaded
2025-05-08 07:24:51,389 - INFO - AWS configurations loaded
2025-05-08 07:25:04,637 - INFO - Spark session initialized
2025-05-08 07:25:04,639 - INFO - Schema defined
2025-05-08 07:25:24,855 - INFO - Kafka stream initialized
2025-05-08 07:25:27,143 - INFO - JSON parsing completed
2025-05-08 07:25:35,549 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 07:25:37,044 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 07:25:37,078 - INFO - Closing down clientserver connection
2025-05-08 07:26:00,136 - INFO - Starting stream_live_streaming.py
2025-05-08 07:26:00,144 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 07:26:00,153 - INFO - Private key loaded successfully
2025-05-08 07:26:00,154 - INFO - Snowflake configurations loaded
2025-05-08 07:26:00,160 - INFO - AWS configurations loaded
2025-05-08 07:26:13,341 - INFO - Spark session initialized
2025-05-08 07:26:13,344 - INFO - Schema defined
2025-05-08 07:26:32,950 - INFO - Kafka stream initialized
2025-05-08 07:26:34,452 - INFO - JSON parsing completed
2025-05-08 07:26:42,751 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 07:26:44,841 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 07:26:44,950 - INFO - Closing down clientserver connection
2025-05-08 07:27:09,648 - INFO - Starting stream_live_streaming.py
2025-05-08 07:27:09,655 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 07:27:09,670 - INFO - Private key loaded successfully
2025-05-08 07:27:09,728 - INFO - Snowflake configurations loaded
2025-05-08 07:27:09,734 - INFO - AWS configurations loaded
2025-05-08 07:27:23,354 - INFO - Spark session initialized
2025-05-08 07:27:23,356 - INFO - Schema defined
2025-05-08 07:27:43,830 - INFO - Kafka stream initialized
2025-05-08 07:27:45,937 - INFO - JSON parsing completed
2025-05-08 07:27:52,668 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 07:27:54,564 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 07:27:54,741 - INFO - Closing down clientserver connection
2025-05-08 07:28:15,693 - INFO - Starting stream_live_streaming.py
2025-05-08 07:28:15,698 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 07:28:15,709 - INFO - Private key loaded successfully
2025-05-08 07:28:15,710 - INFO - Snowflake configurations loaded
2025-05-08 07:28:15,719 - INFO - AWS configurations loaded
2025-05-08 07:28:27,344 - INFO - Spark session initialized
2025-05-08 07:28:27,346 - INFO - Schema defined
2025-05-08 07:28:46,856 - INFO - Kafka stream initialized
2025-05-08 07:28:49,235 - INFO - JSON parsing completed
2025-05-08 07:28:57,467 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 07:28:59,150 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 07:28:59,248 - INFO - Closing down clientserver connection
2025-05-08 07:29:21,483 - INFO - Starting stream_live_streaming.py
2025-05-08 07:29:21,490 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 07:29:21,496 - INFO - Private key loaded successfully
2025-05-08 07:29:21,497 - INFO - Snowflake configurations loaded
2025-05-08 07:29:21,502 - INFO - AWS configurations loaded
2025-05-08 07:29:32,446 - INFO - Spark session initialized
2025-05-08 07:29:32,448 - INFO - Schema defined
2025-05-08 07:29:52,072 - INFO - Kafka stream initialized
2025-05-08 07:29:53,567 - INFO - JSON parsing completed
2025-05-08 07:30:00,573 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 07:30:02,265 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 07:30:02,369 - INFO - Closing down clientserver connection
2025-05-08 07:30:22,680 - INFO - Starting stream_live_streaming.py
2025-05-08 07:30:22,746 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 07:30:22,756 - INFO - Private key loaded successfully
2025-05-08 07:30:22,758 - INFO - Snowflake configurations loaded
2025-05-08 07:30:22,764 - INFO - AWS configurations loaded
2025-05-08 07:30:33,257 - INFO - Spark session initialized
2025-05-08 07:30:33,258 - INFO - Schema defined
2025-05-08 07:30:51,446 - INFO - Kafka stream initialized
2025-05-08 07:30:53,545 - INFO - JSON parsing completed
2025-05-08 07:31:00,857 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 07:31:02,774 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 07:31:03,050 - INFO - Closing down clientserver connection
2025-05-08 07:31:26,271 - INFO - Starting stream_live_streaming.py
2025-05-08 07:31:26,342 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 07:31:26,357 - INFO - Private key loaded successfully
2025-05-08 07:31:26,358 - INFO - Snowflake configurations loaded
2025-05-08 07:31:26,365 - INFO - AWS configurations loaded
2025-05-08 07:31:37,146 - INFO - Spark session initialized
2025-05-08 07:31:37,147 - INFO - Schema defined
2025-05-08 07:31:55,351 - INFO - Kafka stream initialized
2025-05-08 07:31:57,246 - INFO - JSON parsing completed
2025-05-08 07:32:05,446 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 07:32:06,939 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 07:32:07,043 - INFO - Closing down clientserver connection
2025-05-08 07:32:27,445 - INFO - Starting stream_live_streaming.py
2025-05-08 07:32:27,453 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 07:32:27,461 - INFO - Private key loaded successfully
2025-05-08 07:32:27,462 - INFO - Snowflake configurations loaded
2025-05-08 07:32:27,468 - INFO - AWS configurations loaded
2025-05-08 07:32:38,264 - INFO - Spark session initialized
2025-05-08 07:32:38,265 - INFO - Schema defined
2025-05-08 07:32:55,257 - INFO - Kafka stream initialized
2025-05-08 07:32:57,155 - INFO - JSON parsing completed
2025-05-08 07:33:05,542 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 07:33:07,346 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 07:33:07,457 - INFO - Closing down clientserver connection
2025-05-08 07:33:29,072 - INFO - Starting stream_live_streaming.py
2025-05-08 07:33:29,078 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 07:33:29,086 - INFO - Private key loaded successfully
2025-05-08 07:33:29,136 - INFO - Snowflake configurations loaded
2025-05-08 07:33:29,144 - INFO - AWS configurations loaded
2025-05-08 07:33:39,355 - INFO - Spark session initialized
2025-05-08 07:33:39,357 - INFO - Schema defined
2025-05-08 07:33:56,354 - INFO - Kafka stream initialized
2025-05-08 07:33:58,257 - INFO - JSON parsing completed
2025-05-08 07:34:06,442 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 07:34:08,454 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 07:34:08,555 - INFO - Closing down clientserver connection
2025-05-08 07:34:31,553 - INFO - Starting stream_live_streaming.py
2025-05-08 07:34:31,559 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 07:34:31,568 - INFO - Private key loaded successfully
2025-05-08 07:34:31,569 - INFO - Snowflake configurations loaded
2025-05-08 07:34:31,575 - INFO - AWS configurations loaded
2025-05-08 07:34:42,573 - INFO - Spark session initialized
2025-05-08 07:34:42,574 - INFO - Schema defined
2025-05-08 07:35:01,539 - INFO - Kafka stream initialized
2025-05-08 07:35:03,339 - INFO - JSON parsing completed
2025-05-08 07:35:12,038 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 07:35:13,959 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 07:35:14,141 - INFO - Closing down clientserver connection
2025-05-08 07:35:36,646 - INFO - Starting stream_live_streaming.py
2025-05-08 07:35:36,652 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 07:35:36,658 - INFO - Private key loaded successfully
2025-05-08 07:35:36,658 - INFO - Snowflake configurations loaded
2025-05-08 07:35:36,663 - INFO - AWS configurations loaded
2025-05-08 07:35:48,160 - INFO - Spark session initialized
2025-05-08 07:35:48,161 - INFO - Schema defined
2025-05-08 07:36:05,659 - INFO - Kafka stream initialized
2025-05-08 07:36:07,263 - INFO - JSON parsing completed
2025-05-08 07:36:15,368 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 07:36:17,349 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 07:36:17,466 - INFO - Closing down clientserver connection
2025-05-08 07:36:41,990 - INFO - Starting stream_live_streaming.py
2025-05-08 07:36:42,035 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 07:36:42,042 - INFO - Private key loaded successfully
2025-05-08 07:36:42,043 - INFO - Snowflake configurations loaded
2025-05-08 07:36:42,049 - INFO - AWS configurations loaded
2025-05-08 07:36:53,261 - INFO - Spark session initialized
2025-05-08 07:36:53,334 - INFO - Schema defined
2025-05-08 07:37:11,251 - INFO - Kafka stream initialized
2025-05-08 07:37:13,536 - INFO - JSON parsing completed
2025-05-08 07:37:20,255 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 07:37:21,645 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 07:37:21,687 - INFO - Closing down clientserver connection
2025-05-08 07:37:43,433 - INFO - Starting stream_live_streaming.py
2025-05-08 07:37:43,438 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 07:37:43,444 - INFO - Private key loaded successfully
2025-05-08 07:37:43,445 - INFO - Snowflake configurations loaded
2025-05-08 07:37:43,451 - INFO - AWS configurations loaded
2025-05-08 07:37:54,732 - INFO - Spark session initialized
2025-05-08 07:37:54,733 - INFO - Schema defined
2025-05-08 07:38:13,249 - INFO - Kafka stream initialized
2025-05-08 07:38:15,163 - INFO - JSON parsing completed
2025-05-08 07:38:23,147 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 07:38:24,663 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 07:38:24,833 - INFO - Closing down clientserver connection
2025-05-08 07:38:47,162 - INFO - Starting stream_live_streaming.py
2025-05-08 07:38:47,244 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 07:38:47,252 - INFO - Private key loaded successfully
2025-05-08 07:38:47,253 - INFO - Snowflake configurations loaded
2025-05-08 07:38:47,260 - INFO - AWS configurations loaded
2025-05-08 07:38:59,249 - INFO - Spark session initialized
2025-05-08 07:38:59,250 - INFO - Schema defined
2025-05-08 07:39:19,249 - INFO - Kafka stream initialized
2025-05-08 07:39:20,944 - INFO - JSON parsing completed
2025-05-08 07:39:28,837 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 07:39:30,556 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 07:39:30,736 - INFO - Closing down clientserver connection
2025-05-08 07:39:52,625 - INFO - Starting stream_live_streaming.py
2025-05-08 07:39:52,637 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 07:39:52,726 - INFO - Private key loaded successfully
2025-05-08 07:39:52,728 - INFO - Snowflake configurations loaded
2025-05-08 07:39:52,742 - INFO - AWS configurations loaded
2025-05-08 07:40:05,139 - INFO - Spark session initialized
2025-05-08 07:40:05,140 - INFO - Schema defined
2025-05-08 07:40:23,544 - INFO - Kafka stream initialized
2025-05-08 07:40:25,527 - INFO - JSON parsing completed
2025-05-08 07:40:32,062 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 07:40:33,334 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 07:40:33,456 - INFO - Closing down clientserver connection
2025-05-08 07:40:54,337 - INFO - Starting stream_live_streaming.py
2025-05-08 07:40:54,345 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 07:40:54,353 - INFO - Private key loaded successfully
2025-05-08 07:40:54,354 - INFO - Snowflake configurations loaded
2025-05-08 07:40:54,361 - INFO - AWS configurations loaded
2025-05-08 07:41:05,634 - INFO - Spark session initialized
2025-05-08 07:41:05,635 - INFO - Schema defined
2025-05-08 07:41:24,224 - INFO - Kafka stream initialized
2025-05-08 07:41:26,541 - INFO - JSON parsing completed
2025-05-08 07:41:34,437 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 07:41:36,044 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 07:41:36,143 - INFO - Closing down clientserver connection
2025-05-08 07:41:55,659 - INFO - Starting stream_live_streaming.py
2025-05-08 07:41:55,665 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 07:41:55,671 - INFO - Private key loaded successfully
2025-05-08 07:41:55,672 - INFO - Snowflake configurations loaded
2025-05-08 07:41:55,726 - INFO - AWS configurations loaded
2025-05-08 07:42:06,932 - INFO - Spark session initialized
2025-05-08 07:42:06,933 - INFO - Schema defined
2025-05-08 07:42:27,952 - INFO - Kafka stream initialized
2025-05-08 07:42:30,164 - INFO - JSON parsing completed
2025-05-08 07:42:37,425 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 07:42:38,733 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 07:42:38,767 - INFO - Closing down clientserver connection
2025-05-08 07:42:59,547 - INFO - Starting stream_live_streaming.py
2025-05-08 07:42:59,553 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 07:42:59,558 - INFO - Private key loaded successfully
2025-05-08 07:42:59,559 - INFO - Snowflake configurations loaded
2025-05-08 07:42:59,564 - INFO - AWS configurations loaded
2025-05-08 07:43:10,447 - INFO - Spark session initialized
2025-05-08 07:43:10,448 - INFO - Schema defined
2025-05-08 07:43:30,552 - INFO - Kafka stream initialized
2025-05-08 07:43:32,647 - INFO - JSON parsing completed
2025-05-08 07:43:41,426 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 07:43:42,854 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 07:43:42,973 - INFO - Closing down clientserver connection
2025-05-08 07:44:05,846 - INFO - Starting stream_live_streaming.py
2025-05-08 07:44:05,852 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 07:44:05,856 - INFO - Private key loaded successfully
2025-05-08 07:44:05,857 - INFO - Snowflake configurations loaded
2025-05-08 07:44:05,862 - INFO - AWS configurations loaded
2025-05-08 07:44:16,626 - INFO - Spark session initialized
2025-05-08 07:44:16,627 - INFO - Schema defined
2025-05-08 07:44:34,944 - INFO - Kafka stream initialized
2025-05-08 07:44:37,134 - INFO - JSON parsing completed
2025-05-08 07:44:46,236 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 07:44:47,836 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 07:44:47,953 - INFO - Closing down clientserver connection
2025-05-08 07:45:07,420 - INFO - Starting stream_live_streaming.py
2025-05-08 07:45:07,425 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 07:45:07,430 - INFO - Private key loaded successfully
2025-05-08 07:45:07,431 - INFO - Snowflake configurations loaded
2025-05-08 07:45:07,435 - INFO - AWS configurations loaded
2025-05-08 07:45:18,240 - INFO - Spark session initialized
2025-05-08 07:45:18,241 - INFO - Schema defined
2025-05-08 07:45:35,243 - INFO - Kafka stream initialized
2025-05-08 07:45:37,229 - INFO - JSON parsing completed
2025-05-08 07:45:45,237 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 07:45:47,225 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 07:45:47,328 - INFO - Closing down clientserver connection
2025-05-08 07:46:08,229 - INFO - Starting stream_live_streaming.py
2025-05-08 07:46:08,236 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 07:46:08,247 - INFO - Private key loaded successfully
2025-05-08 07:46:08,248 - INFO - Snowflake configurations loaded
2025-05-08 07:46:08,253 - INFO - AWS configurations loaded
2025-05-08 07:46:18,930 - INFO - Spark session initialized
2025-05-08 07:46:18,931 - INFO - Schema defined
2025-05-08 07:46:37,312 - INFO - Kafka stream initialized
2025-05-08 07:46:39,235 - INFO - JSON parsing completed
2025-05-08 07:46:46,918 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 07:46:48,742 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 07:46:48,851 - INFO - Closing down clientserver connection
2025-05-08 07:47:13,037 - INFO - Starting stream_live_streaming.py
2025-05-08 07:47:13,045 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 07:47:13,051 - INFO - Private key loaded successfully
2025-05-08 07:47:13,052 - INFO - Snowflake configurations loaded
2025-05-08 07:47:13,114 - INFO - AWS configurations loaded
2025-05-08 07:47:24,725 - INFO - Spark session initialized
2025-05-08 07:47:24,726 - INFO - Schema defined
2025-05-08 07:47:43,221 - INFO - Kafka stream initialized
2025-05-08 07:47:45,010 - INFO - JSON parsing completed
2025-05-08 07:47:52,817 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 07:47:54,340 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 07:47:54,441 - INFO - Closing down clientserver connection
2025-05-08 07:48:19,125 - INFO - Starting stream_live_streaming.py
2025-05-08 07:48:19,131 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 07:48:19,137 - INFO - Private key loaded successfully
2025-05-08 07:48:19,138 - INFO - Snowflake configurations loaded
2025-05-08 07:48:19,144 - INFO - AWS configurations loaded
2025-05-08 07:48:31,320 - INFO - Spark session initialized
2025-05-08 07:48:31,321 - INFO - Schema defined
2025-05-08 07:48:48,707 - INFO - Kafka stream initialized
2025-05-08 07:48:50,628 - INFO - JSON parsing completed
2025-05-08 07:48:58,422 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 07:49:00,136 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 07:49:00,313 - INFO - Closing down clientserver connection
2025-05-08 07:49:23,311 - INFO - Starting stream_live_streaming.py
2025-05-08 07:49:23,319 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 07:49:23,323 - INFO - Private key loaded successfully
2025-05-08 07:49:23,324 - INFO - Snowflake configurations loaded
2025-05-08 07:49:23,328 - INFO - AWS configurations loaded
2025-05-08 07:49:33,829 - INFO - Spark session initialized
2025-05-08 07:49:33,830 - INFO - Schema defined
2025-05-08 07:49:51,034 - INFO - Kafka stream initialized
2025-05-08 07:49:53,026 - INFO - JSON parsing completed
2025-05-08 07:50:01,323 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 07:50:03,705 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 07:50:03,908 - INFO - Closing down clientserver connection
2025-05-08 07:50:27,616 - INFO - Starting stream_live_streaming.py
2025-05-08 07:50:27,624 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 07:50:27,630 - INFO - Private key loaded successfully
2025-05-08 07:50:27,631 - INFO - Snowflake configurations loaded
2025-05-08 07:50:27,636 - INFO - AWS configurations loaded
2025-05-08 07:50:39,103 - INFO - Spark session initialized
2025-05-08 07:50:39,104 - INFO - Schema defined
2025-05-08 07:50:55,112 - INFO - Kafka stream initialized
2025-05-08 07:50:56,815 - INFO - JSON parsing completed
2025-05-08 07:51:04,602 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 07:51:06,424 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 07:51:06,531 - INFO - Closing down clientserver connection
2025-05-08 07:51:28,335 - INFO - Starting stream_live_streaming.py
2025-05-08 07:51:28,404 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 07:51:28,410 - INFO - Private key loaded successfully
2025-05-08 07:51:28,410 - INFO - Snowflake configurations loaded
2025-05-08 07:51:28,416 - INFO - AWS configurations loaded
2025-05-08 07:51:40,707 - INFO - Spark session initialized
2025-05-08 07:51:40,708 - INFO - Schema defined
2025-05-08 07:51:59,429 - INFO - Kafka stream initialized
2025-05-08 07:52:00,743 - INFO - JSON parsing completed
2025-05-08 07:52:07,825 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 07:52:09,499 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 07:52:09,604 - INFO - Closing down clientserver connection
2025-05-08 07:52:28,658 - INFO - Starting stream_live_streaming.py
2025-05-08 07:52:28,663 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 07:52:28,699 - INFO - Private key loaded successfully
2025-05-08 07:52:28,700 - INFO - Snowflake configurations loaded
2025-05-08 07:52:28,705 - INFO - AWS configurations loaded
2025-05-08 07:52:40,416 - INFO - Spark session initialized
2025-05-08 07:52:40,416 - INFO - Schema defined
2025-05-08 07:53:00,218 - INFO - Kafka stream initialized
2025-05-08 07:53:02,492 - INFO - JSON parsing completed
2025-05-08 07:53:10,317 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 07:53:11,729 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 07:53:11,826 - INFO - Closing down clientserver connection
2025-05-08 07:53:32,026 - INFO - Starting stream_live_streaming.py
2025-05-08 07:53:32,096 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 07:53:32,102 - INFO - Private key loaded successfully
2025-05-08 07:53:32,103 - INFO - Snowflake configurations loaded
2025-05-08 07:53:32,108 - INFO - AWS configurations loaded
2025-05-08 07:53:42,997 - INFO - Spark session initialized
2025-05-08 07:53:42,998 - INFO - Schema defined
2025-05-08 07:54:02,790 - INFO - Kafka stream initialized
2025-05-08 07:54:04,997 - INFO - JSON parsing completed
2025-05-08 07:54:12,896 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 07:54:14,618 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 07:54:14,797 - INFO - Closing down clientserver connection
2025-05-08 07:54:35,306 - INFO - Starting stream_live_streaming.py
2025-05-08 07:54:35,314 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 07:54:35,323 - INFO - Private key loaded successfully
2025-05-08 07:54:35,323 - INFO - Snowflake configurations loaded
2025-05-08 07:54:35,331 - INFO - AWS configurations loaded
2025-05-08 07:54:45,089 - INFO - Spark session initialized
2025-05-08 07:54:45,090 - INFO - Schema defined
2025-05-08 07:55:04,892 - INFO - Kafka stream initialized
2025-05-08 07:55:06,902 - INFO - JSON parsing completed
2025-05-08 07:55:13,906 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 07:55:15,727 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 07:55:15,901 - INFO - Closing down clientserver connection
2025-05-08 07:55:37,198 - INFO - Starting stream_live_streaming.py
2025-05-08 07:55:37,206 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 07:55:37,217 - INFO - Private key loaded successfully
2025-05-08 07:55:37,218 - INFO - Snowflake configurations loaded
2025-05-08 07:55:37,225 - INFO - AWS configurations loaded
2025-05-08 07:55:47,996 - INFO - Spark session initialized
2025-05-08 07:55:47,997 - INFO - Schema defined
2025-05-08 07:56:06,472 - INFO - Kafka stream initialized
2025-05-08 07:56:08,580 - INFO - JSON parsing completed
2025-05-08 07:56:16,682 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 07:56:18,272 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 07:56:18,393 - INFO - Closing down clientserver connection
2025-05-08 07:56:37,668 - INFO - Starting stream_live_streaming.py
2025-05-08 07:56:37,675 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 07:56:37,682 - INFO - Private key loaded successfully
2025-05-08 07:56:37,683 - INFO - Snowflake configurations loaded
2025-05-08 07:56:37,689 - INFO - AWS configurations loaded
2025-05-08 07:56:48,008 - INFO - Spark session initialized
2025-05-08 07:56:48,009 - INFO - Schema defined
2025-05-08 07:57:04,700 - INFO - Kafka stream initialized
2025-05-08 07:57:06,670 - INFO - JSON parsing completed
2025-05-08 07:57:14,579 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 07:57:16,193 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 07:57:16,371 - INFO - Closing down clientserver connection
2025-05-08 07:57:36,470 - INFO - Starting stream_live_streaming.py
2025-05-08 07:57:36,476 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 07:57:36,481 - INFO - Private key loaded successfully
2025-05-08 07:57:36,482 - INFO - Snowflake configurations loaded
2025-05-08 07:57:36,486 - INFO - AWS configurations loaded
2025-05-08 07:57:47,369 - INFO - Spark session initialized
2025-05-08 07:57:47,371 - INFO - Schema defined
2025-05-08 07:58:04,278 - INFO - Kafka stream initialized
2025-05-08 07:58:06,168 - INFO - JSON parsing completed
2025-05-08 07:58:14,265 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 07:58:16,157 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 07:58:16,269 - INFO - Closing down clientserver connection
2025-05-08 07:58:39,269 - INFO - Starting stream_live_streaming.py
2025-05-08 07:58:39,275 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 07:58:39,281 - INFO - Private key loaded successfully
2025-05-08 07:58:39,282 - INFO - Snowflake configurations loaded
2025-05-08 07:58:39,286 - INFO - AWS configurations loaded
2025-05-08 07:58:51,081 - INFO - Spark session initialized
2025-05-08 07:58:51,083 - INFO - Schema defined
2025-05-08 07:59:09,473 - INFO - Kafka stream initialized
2025-05-08 07:59:11,471 - INFO - JSON parsing completed
2025-05-08 07:59:19,859 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 07:59:21,353 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 07:59:21,456 - INFO - Closing down clientserver connection
2025-05-08 07:59:44,393 - INFO - Starting stream_live_streaming.py
2025-05-08 07:59:44,453 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 07:59:44,464 - INFO - Private key loaded successfully
2025-05-08 07:59:44,465 - INFO - Snowflake configurations loaded
2025-05-08 07:59:44,470 - INFO - AWS configurations loaded
2025-05-08 07:59:56,258 - INFO - Spark session initialized
2025-05-08 07:59:56,259 - INFO - Schema defined
2025-05-08 08:00:14,158 - INFO - Kafka stream initialized
2025-05-08 08:00:15,943 - INFO - JSON parsing completed
2025-05-08 08:00:23,854 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 08:00:25,764 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 08:00:25,869 - INFO - Closing down clientserver connection
2025-05-08 08:00:47,870 - INFO - Starting stream_live_streaming.py
2025-05-08 08:00:47,876 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 08:00:47,882 - INFO - Private key loaded successfully
2025-05-08 08:00:47,883 - INFO - Snowflake configurations loaded
2025-05-08 08:00:47,887 - INFO - AWS configurations loaded
2025-05-08 08:00:59,462 - INFO - Spark session initialized
2025-05-08 08:00:59,463 - INFO - Schema defined
2025-05-08 08:01:17,068 - INFO - Kafka stream initialized
2025-05-08 08:01:19,069 - INFO - JSON parsing completed
2025-05-08 08:01:26,857 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 08:01:28,480 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 08:01:28,569 - INFO - Closing down clientserver connection
2025-05-08 08:01:48,857 - INFO - Starting stream_live_streaming.py
2025-05-08 08:01:48,864 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 08:01:48,871 - INFO - Private key loaded successfully
2025-05-08 08:01:48,871 - INFO - Snowflake configurations loaded
2025-05-08 08:01:48,876 - INFO - AWS configurations loaded
2025-05-08 08:02:01,155 - INFO - Spark session initialized
2025-05-08 08:02:01,156 - INFO - Schema defined
2025-05-08 08:02:19,974 - INFO - Kafka stream initialized
2025-05-08 08:02:21,368 - INFO - JSON parsing completed
2025-05-08 08:02:28,950 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 08:02:30,886 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 08:02:30,967 - INFO - Closing down clientserver connection
2025-05-08 08:02:51,656 - INFO - Starting stream_live_streaming.py
2025-05-08 08:02:51,665 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 08:02:51,671 - INFO - Private key loaded successfully
2025-05-08 08:02:51,672 - INFO - Snowflake configurations loaded
2025-05-08 08:02:51,679 - INFO - AWS configurations loaded
2025-05-08 08:03:04,145 - INFO - Spark session initialized
2025-05-08 08:03:04,146 - INFO - Schema defined
2025-05-08 08:03:22,455 - INFO - Kafka stream initialized
2025-05-08 08:03:24,741 - INFO - JSON parsing completed
2025-05-08 08:03:31,635 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 08:03:33,166 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 08:03:33,325 - INFO - Closing down clientserver connection
2025-05-08 08:03:52,968 - INFO - Starting stream_live_streaming.py
2025-05-08 08:03:52,975 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 08:03:52,982 - INFO - Private key loaded successfully
2025-05-08 08:03:52,982 - INFO - Snowflake configurations loaded
2025-05-08 08:03:52,988 - INFO - AWS configurations loaded
2025-05-08 08:04:05,022 - INFO - Spark session initialized
2025-05-08 08:04:05,027 - INFO - Schema defined
2025-05-08 08:04:25,935 - INFO - Kafka stream initialized
2025-05-08 08:04:27,847 - INFO - JSON parsing completed
2025-05-08 08:04:36,631 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 08:04:38,328 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 08:04:38,440 - INFO - Closing down clientserver connection
2025-05-08 08:04:58,924 - INFO - Starting stream_live_streaming.py
2025-05-08 08:04:58,936 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 08:04:58,944 - INFO - Private key loaded successfully
2025-05-08 08:04:58,945 - INFO - Snowflake configurations loaded
2025-05-08 08:04:58,950 - INFO - AWS configurations loaded
2025-05-08 08:05:11,022 - INFO - Spark session initialized
2025-05-08 08:05:11,024 - INFO - Schema defined
2025-05-08 08:05:31,800 - INFO - Kafka stream initialized
2025-05-08 08:05:33,216 - INFO - JSON parsing completed
2025-05-08 08:05:41,701 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 08:05:43,403 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 08:05:43,502 - INFO - Closing down clientserver connection
2025-05-08 08:06:03,330 - INFO - Starting stream_live_streaming.py
2025-05-08 08:06:03,335 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 08:06:03,341 - INFO - Private key loaded successfully
2025-05-08 08:06:03,341 - INFO - Snowflake configurations loaded
2025-05-08 08:06:03,406 - INFO - AWS configurations loaded
2025-05-08 08:06:13,616 - INFO - Spark session initialized
2025-05-08 08:06:13,617 - INFO - Schema defined
2025-05-08 08:06:31,523 - INFO - Kafka stream initialized
2025-05-08 08:06:33,299 - INFO - JSON parsing completed
2025-05-08 08:06:40,906 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 08:06:42,618 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 08:06:42,715 - INFO - Closing down clientserver connection
2025-05-08 08:07:03,613 - INFO - Starting stream_live_streaming.py
2025-05-08 08:07:03,619 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 08:07:03,626 - INFO - Private key loaded successfully
2025-05-08 08:07:03,627 - INFO - Snowflake configurations loaded
2025-05-08 08:07:03,695 - INFO - AWS configurations loaded
2025-05-08 08:07:14,699 - INFO - Spark session initialized
2025-05-08 08:07:14,700 - INFO - Schema defined
2025-05-08 08:07:31,992 - INFO - Kafka stream initialized
2025-05-08 08:07:34,108 - INFO - JSON parsing completed
2025-05-08 08:07:42,488 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 08:07:44,122 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 08:07:44,299 - INFO - Closing down clientserver connection
2025-05-08 08:08:05,049 - INFO - Starting stream_live_streaming.py
2025-05-08 08:08:05,061 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 08:08:05,070 - INFO - Private key loaded successfully
2025-05-08 08:08:05,084 - INFO - Snowflake configurations loaded
2025-05-08 08:08:05,089 - INFO - AWS configurations loaded
2025-05-08 08:08:16,006 - INFO - Spark session initialized
2025-05-08 08:08:16,007 - INFO - Schema defined
2025-05-08 08:08:34,185 - INFO - Kafka stream initialized
2025-05-08 08:08:36,378 - INFO - JSON parsing completed
2025-05-08 08:08:45,178 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 08:08:47,292 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 08:08:47,481 - INFO - Closing down clientserver connection
2025-05-08 08:09:08,009 - INFO - Starting stream_live_streaming.py
2025-05-08 08:09:08,017 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 08:09:08,080 - INFO - Private key loaded successfully
2025-05-08 08:09:08,081 - INFO - Snowflake configurations loaded
2025-05-08 08:09:08,090 - INFO - AWS configurations loaded
2025-05-08 08:09:18,079 - INFO - Spark session initialized
2025-05-08 08:09:18,080 - INFO - Schema defined
2025-05-08 08:09:37,190 - INFO - Kafka stream initialized
2025-05-08 08:09:39,391 - INFO - JSON parsing completed
2025-05-08 08:09:47,170 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 08:09:49,295 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 08:09:49,392 - INFO - Closing down clientserver connection
2025-05-08 08:10:10,828 - INFO - Starting stream_live_streaming.py
2025-05-08 08:10:10,834 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 08:10:10,870 - INFO - Private key loaded successfully
2025-05-08 08:10:10,870 - INFO - Snowflake configurations loaded
2025-05-08 08:10:10,875 - INFO - AWS configurations loaded
2025-05-08 08:10:23,178 - INFO - Spark session initialized
2025-05-08 08:10:23,179 - INFO - Schema defined
2025-05-08 08:10:40,875 - INFO - Kafka stream initialized
2025-05-08 08:10:42,598 - INFO - JSON parsing completed
2025-05-08 08:10:49,878 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 08:10:51,399 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 08:10:51,598 - INFO - Closing down clientserver connection
2025-05-08 08:11:14,606 - INFO - Starting stream_live_streaming.py
2025-05-08 08:11:14,613 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 08:11:14,619 - INFO - Private key loaded successfully
2025-05-08 08:11:14,620 - INFO - Snowflake configurations loaded
2025-05-08 08:11:14,666 - INFO - AWS configurations loaded
2025-05-08 08:11:25,368 - INFO - Spark session initialized
2025-05-08 08:11:25,369 - INFO - Schema defined
2025-05-08 08:11:42,856 - INFO - Kafka stream initialized
2025-05-08 08:11:44,670 - INFO - JSON parsing completed
2025-05-08 08:11:52,068 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 08:11:53,888 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 08:11:53,986 - INFO - Closing down clientserver connection
2025-05-08 08:12:15,212 - INFO - Starting stream_live_streaming.py
2025-05-08 08:12:15,254 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 08:12:15,260 - INFO - Private key loaded successfully
2025-05-08 08:12:15,261 - INFO - Snowflake configurations loaded
2025-05-08 08:12:15,266 - INFO - AWS configurations loaded
2025-05-08 08:12:26,362 - INFO - Spark session initialized
2025-05-08 08:12:26,363 - INFO - Schema defined
2025-05-08 08:12:43,588 - INFO - Kafka stream initialized
2025-05-08 08:12:45,488 - INFO - JSON parsing completed
2025-05-08 08:12:53,369 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 08:12:55,357 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 08:12:55,472 - INFO - Closing down clientserver connection
2025-05-08 08:13:16,402 - INFO - Starting stream_live_streaming.py
2025-05-08 08:13:16,446 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 08:13:16,452 - INFO - Private key loaded successfully
2025-05-08 08:13:16,454 - INFO - Snowflake configurations loaded
2025-05-08 08:13:16,459 - INFO - AWS configurations loaded
2025-05-08 08:13:28,761 - INFO - Spark session initialized
2025-05-08 08:13:28,762 - INFO - Schema defined
2025-05-08 08:13:45,963 - INFO - Kafka stream initialized
2025-05-08 08:13:47,671 - INFO - JSON parsing completed
2025-05-08 08:13:54,461 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 08:13:56,356 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 08:13:56,553 - INFO - Closing down clientserver connection
2025-05-08 08:14:18,054 - INFO - Starting stream_live_streaming.py
2025-05-08 08:14:18,063 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 08:14:18,070 - INFO - Private key loaded successfully
2025-05-08 08:14:18,072 - INFO - Snowflake configurations loaded
2025-05-08 08:14:18,079 - INFO - AWS configurations loaded
2025-05-08 08:14:30,037 - INFO - Spark session initialized
2025-05-08 08:14:30,039 - INFO - Schema defined
2025-05-08 08:14:48,931 - INFO - Kafka stream initialized
2025-05-08 08:14:51,163 - INFO - JSON parsing completed
2025-05-08 08:14:59,274 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 08:15:00,656 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 08:15:00,755 - INFO - Closing down clientserver connection
2025-05-08 08:15:21,357 - INFO - Starting stream_live_streaming.py
2025-05-08 08:15:21,363 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 08:15:21,369 - INFO - Private key loaded successfully
2025-05-08 08:15:21,370 - INFO - Snowflake configurations loaded
2025-05-08 08:15:21,376 - INFO - AWS configurations loaded
2025-05-08 08:15:33,347 - INFO - Spark session initialized
2025-05-08 08:15:33,348 - INFO - Schema defined
2025-05-08 08:15:53,737 - INFO - Kafka stream initialized
2025-05-08 08:15:55,236 - INFO - JSON parsing completed
2025-05-08 08:16:03,611 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 08:16:05,637 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 08:16:05,822 - INFO - Closing down clientserver connection
2025-05-08 08:16:28,250 - INFO - Starting stream_live_streaming.py
2025-05-08 08:16:28,308 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 08:16:28,317 - INFO - Private key loaded successfully
2025-05-08 08:16:28,318 - INFO - Snowflake configurations loaded
2025-05-08 08:16:28,324 - INFO - AWS configurations loaded
2025-05-08 08:16:42,414 - INFO - Spark session initialized
2025-05-08 08:16:42,415 - INFO - Schema defined
2025-05-08 08:17:04,925 - INFO - Kafka stream initialized
2025-05-08 08:17:07,730 - INFO - JSON parsing completed
2025-05-08 08:17:15,527 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 08:17:17,204 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 08:17:17,398 - INFO - Closing down clientserver connection
2025-05-08 08:17:41,128 - INFO - Starting stream_live_streaming.py
2025-05-08 08:17:41,198 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 08:17:41,209 - INFO - Private key loaded successfully
2025-05-08 08:17:41,210 - INFO - Snowflake configurations loaded
2025-05-08 08:17:41,215 - INFO - AWS configurations loaded
2025-05-08 08:17:53,726 - INFO - Spark session initialized
2025-05-08 08:17:53,728 - INFO - Schema defined
2025-05-08 08:18:15,314 - INFO - Kafka stream initialized
2025-05-08 08:18:17,703 - INFO - JSON parsing completed
2025-05-08 08:18:25,901 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 08:18:27,701 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 08:18:27,826 - INFO - Closing down clientserver connection
2025-05-08 08:18:47,018 - INFO - Starting stream_live_streaming.py
2025-05-08 08:18:47,023 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 08:18:47,029 - INFO - Private key loaded successfully
2025-05-08 08:18:47,029 - INFO - Snowflake configurations loaded
2025-05-08 08:18:47,034 - INFO - AWS configurations loaded
2025-05-08 08:18:59,095 - INFO - Spark session initialized
2025-05-08 08:18:59,096 - INFO - Schema defined
2025-05-08 08:19:18,492 - INFO - Kafka stream initialized
2025-05-08 08:19:20,693 - INFO - JSON parsing completed
2025-05-08 08:19:29,774 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 08:19:31,697 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 08:19:31,887 - INFO - Closing down clientserver connection
2025-05-08 08:19:55,217 - INFO - Starting stream_live_streaming.py
2025-05-08 08:19:55,265 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 08:19:55,271 - INFO - Private key loaded successfully
2025-05-08 08:19:55,272 - INFO - Snowflake configurations loaded
2025-05-08 08:19:55,278 - INFO - AWS configurations loaded
2025-05-08 08:20:07,376 - INFO - Spark session initialized
2025-05-08 08:20:07,380 - INFO - Schema defined
2025-05-08 08:20:25,567 - INFO - Kafka stream initialized
2025-05-08 08:20:27,472 - INFO - JSON parsing completed
2025-05-08 08:20:34,977 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 08:20:36,375 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 08:20:36,411 - INFO - Closing down clientserver connection
2025-05-08 08:20:59,154 - INFO - Starting stream_live_streaming.py
2025-05-08 08:20:59,160 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 08:20:59,167 - INFO - Private key loaded successfully
2025-05-08 08:20:59,167 - INFO - Snowflake configurations loaded
2025-05-08 08:20:59,173 - INFO - AWS configurations loaded
2025-05-08 08:21:10,951 - INFO - Spark session initialized
2025-05-08 08:21:10,952 - INFO - Schema defined
2025-05-08 08:21:27,945 - INFO - Kafka stream initialized
2025-05-08 08:21:29,645 - INFO - JSON parsing completed
2025-05-08 08:21:37,457 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 08:21:39,166 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 08:21:39,341 - INFO - Closing down clientserver connection
2025-05-08 08:22:03,370 - INFO - Starting stream_live_streaming.py
2025-05-08 08:22:03,376 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 08:22:03,382 - INFO - Private key loaded successfully
2025-05-08 08:22:03,383 - INFO - Snowflake configurations loaded
2025-05-08 08:22:03,388 - INFO - AWS configurations loaded
2025-05-08 08:22:16,450 - INFO - Spark session initialized
2025-05-08 08:22:16,451 - INFO - Schema defined
2025-05-08 08:22:34,450 - INFO - Kafka stream initialized
2025-05-08 08:22:37,124 - INFO - JSON parsing completed
2025-05-08 08:22:44,440 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 08:22:46,240 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 08:22:46,354 - INFO - Closing down clientserver connection
2025-05-08 08:23:12,951 - INFO - Starting stream_live_streaming.py
2025-05-08 08:23:13,020 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 08:23:13,028 - INFO - Private key loaded successfully
2025-05-08 08:23:13,029 - INFO - Snowflake configurations loaded
2025-05-08 08:23:13,034 - INFO - AWS configurations loaded
2025-05-08 08:23:24,940 - INFO - Spark session initialized
2025-05-08 08:23:24,941 - INFO - Schema defined
2025-05-08 08:23:44,532 - INFO - Kafka stream initialized
2025-05-08 08:23:46,816 - INFO - JSON parsing completed
2025-05-08 08:23:55,030 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 08:23:57,121 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 08:23:57,309 - INFO - Closing down clientserver connection
2025-05-08 08:24:23,037 - INFO - Starting stream_live_streaming.py
2025-05-08 08:24:23,111 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 08:24:23,125 - INFO - Private key loaded successfully
2025-05-08 08:24:23,126 - INFO - Snowflake configurations loaded
2025-05-08 08:24:23,133 - INFO - AWS configurations loaded
2025-05-08 08:24:34,706 - INFO - Spark session initialized
2025-05-08 08:24:34,707 - INFO - Schema defined
2025-05-08 08:24:53,432 - INFO - Kafka stream initialized
2025-05-08 08:24:55,423 - INFO - JSON parsing completed
2025-05-08 08:25:03,310 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 08:25:05,095 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 08:25:05,206 - INFO - Closing down clientserver connection
2025-05-08 08:25:25,590 - INFO - Starting stream_live_streaming.py
2025-05-08 08:25:25,597 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 08:25:25,614 - INFO - Private key loaded successfully
2025-05-08 08:25:25,615 - INFO - Snowflake configurations loaded
2025-05-08 08:25:25,623 - INFO - AWS configurations loaded
2025-05-08 08:25:39,004 - INFO - Spark session initialized
2025-05-08 08:25:39,004 - INFO - Schema defined
2025-05-08 08:25:58,296 - INFO - Kafka stream initialized
2025-05-08 08:26:00,083 - INFO - JSON parsing completed
2025-05-08 08:26:08,204 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 08:26:10,007 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 08:26:10,116 - INFO - Closing down clientserver connection
2025-05-08 08:26:32,111 - INFO - Starting stream_live_streaming.py
2025-05-08 08:26:32,124 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 08:26:32,176 - INFO - Private key loaded successfully
2025-05-08 08:26:32,177 - INFO - Snowflake configurations loaded
2025-05-08 08:26:32,182 - INFO - AWS configurations loaded
2025-05-08 08:26:45,883 - INFO - Spark session initialized
2025-05-08 08:26:45,889 - INFO - Schema defined
2025-05-08 08:27:04,662 - INFO - Kafka stream initialized
2025-05-08 08:27:07,399 - INFO - JSON parsing completed
2025-05-08 08:27:15,080 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 08:27:16,374 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 08:27:16,473 - INFO - Closing down clientserver connection
2025-05-08 08:27:38,454 - INFO - Starting stream_live_streaming.py
2025-05-08 08:27:38,463 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 08:27:38,470 - INFO - Private key loaded successfully
2025-05-08 08:27:38,471 - INFO - Snowflake configurations loaded
2025-05-08 08:27:38,477 - INFO - AWS configurations loaded
2025-05-08 08:27:50,462 - INFO - Spark session initialized
2025-05-08 08:27:50,464 - INFO - Schema defined
2025-05-08 08:28:12,084 - INFO - Kafka stream initialized
2025-05-08 08:28:14,780 - INFO - JSON parsing completed
2025-05-08 08:28:23,859 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 11:20:39,945 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 11:20:43,671 - INFO - Closing down clientserver connection
2025-05-08 11:21:19,688 - INFO - Starting stream_live_streaming.py
2025-05-08 11:21:19,696 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 11:21:19,763 - INFO - Private key loaded successfully
2025-05-08 11:21:19,764 - INFO - Snowflake configurations loaded
2025-05-08 11:21:19,770 - INFO - AWS configurations loaded
2025-05-08 11:21:28,883 - INFO - Spark session initialized
2025-05-08 11:21:28,884 - INFO - Schema defined
2025-05-08 11:21:49,727 - INFO - Kafka stream initialized
2025-05-08 11:21:51,762 - INFO - JSON parsing completed
2025-05-08 11:21:59,027 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 11:22:00,651 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 11:22:00,754 - INFO - Closing down clientserver connection
2025-05-08 11:22:20,374 - INFO - Starting stream_live_streaming.py
2025-05-08 11:22:20,382 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 11:22:20,446 - INFO - Private key loaded successfully
2025-05-08 11:22:20,448 - INFO - Snowflake configurations loaded
2025-05-08 11:22:20,455 - INFO - AWS configurations loaded
2025-05-08 11:22:30,145 - INFO - Spark session initialized
2025-05-08 11:22:30,146 - INFO - Schema defined
2025-05-08 11:22:48,826 - INFO - Kafka stream initialized
2025-05-08 11:22:50,643 - INFO - JSON parsing completed
2025-05-08 11:22:59,045 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 11:23:00,531 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 11:23:00,574 - INFO - Closing down clientserver connection
2025-05-08 11:23:20,148 - INFO - Starting stream_live_streaming.py
2025-05-08 11:23:20,223 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 11:23:20,232 - INFO - Private key loaded successfully
2025-05-08 11:23:20,233 - INFO - Snowflake configurations loaded
2025-05-08 11:23:20,240 - INFO - AWS configurations loaded
2025-05-08 11:23:31,930 - INFO - Spark session initialized
2025-05-08 11:23:31,931 - INFO - Schema defined
2025-05-08 11:23:53,022 - INFO - Kafka stream initialized
2025-05-08 11:23:55,730 - INFO - JSON parsing completed
2025-05-08 11:24:03,913 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 11:24:06,012 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 11:24:06,129 - INFO - Closing down clientserver connection
2025-05-08 11:24:27,929 - INFO - Starting stream_live_streaming.py
2025-05-08 11:24:27,934 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 11:24:27,941 - INFO - Private key loaded successfully
2025-05-08 11:24:27,942 - INFO - Snowflake configurations loaded
2025-05-08 11:24:27,947 - INFO - AWS configurations loaded
2025-05-08 11:24:38,579 - INFO - Spark session initialized
2025-05-08 11:24:38,580 - INFO - Schema defined
2025-05-08 11:24:56,877 - INFO - Kafka stream initialized
2025-05-08 11:24:58,697 - INFO - JSON parsing completed
2025-05-08 11:25:06,737 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 11:25:09,051 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 11:25:09,246 - INFO - Closing down clientserver connection
2025-05-08 11:25:29,945 - INFO - Starting stream_live_streaming.py
2025-05-08 11:25:29,953 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 11:25:29,961 - INFO - Private key loaded successfully
2025-05-08 11:25:29,961 - INFO - Snowflake configurations loaded
2025-05-08 11:25:29,967 - INFO - AWS configurations loaded
2025-05-08 11:25:40,911 - INFO - Spark session initialized
2025-05-08 11:25:40,912 - INFO - Schema defined
2025-05-08 11:26:00,216 - INFO - Kafka stream initialized
2025-05-08 11:26:01,902 - INFO - JSON parsing completed
2025-05-08 11:26:10,395 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 11:26:12,408 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 11:26:12,595 - INFO - Closing down clientserver connection
2025-05-08 11:26:35,084 - INFO - Starting stream_live_streaming.py
2025-05-08 11:26:35,092 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 11:26:35,097 - INFO - Private key loaded successfully
2025-05-08 11:26:35,098 - INFO - Snowflake configurations loaded
2025-05-08 11:26:35,102 - INFO - AWS configurations loaded
2025-05-08 11:26:47,472 - INFO - Spark session initialized
2025-05-08 11:26:47,474 - INFO - Schema defined
2025-05-08 11:27:04,966 - INFO - Kafka stream initialized
2025-05-08 11:27:07,137 - INFO - JSON parsing completed
2025-05-08 11:27:15,369 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 11:27:17,373 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 11:27:17,480 - INFO - Closing down clientserver connection
2025-05-08 11:27:39,620 - INFO - Starting stream_live_streaming.py
2025-05-08 11:27:39,628 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 11:27:39,634 - INFO - Private key loaded successfully
2025-05-08 11:27:39,703 - INFO - Snowflake configurations loaded
2025-05-08 11:27:39,711 - INFO - AWS configurations loaded
2025-05-08 11:27:50,838 - INFO - Spark session initialized
2025-05-08 11:27:50,839 - INFO - Schema defined
2025-05-08 11:28:09,387 - INFO - Kafka stream initialized
2025-05-08 11:28:11,108 - INFO - JSON parsing completed
2025-05-08 11:28:19,504 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 11:28:20,912 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 11:28:21,021 - INFO - Closing down clientserver connection
2025-05-08 11:28:42,998 - INFO - Starting stream_live_streaming.py
2025-05-08 11:28:43,003 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 11:28:43,008 - INFO - Private key loaded successfully
2025-05-08 11:28:43,009 - INFO - Snowflake configurations loaded
2025-05-08 11:28:43,013 - INFO - AWS configurations loaded
2025-05-08 11:28:54,685 - INFO - Spark session initialized
2025-05-08 11:28:54,686 - INFO - Schema defined
2025-05-08 11:29:12,672 - INFO - Kafka stream initialized
2025-05-08 12:00:28,977 - INFO - JSON parsing completed
2025-05-08 12:00:40,372 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 12:00:42,378 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 12:00:42,490 - INFO - Closing down clientserver connection
2025-05-08 12:01:08,691 - INFO - Starting stream_live_streaming.py
2025-05-08 12:01:08,698 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 12:01:08,713 - INFO - Private key loaded successfully
2025-05-08 12:01:08,757 - INFO - Snowflake configurations loaded
2025-05-08 12:01:08,763 - INFO - AWS configurations loaded
2025-05-08 12:01:29,285 - INFO - Spark session initialized
2025-05-08 12:01:29,289 - INFO - Schema defined
2025-05-08 12:02:01,639 - INFO - Kafka stream initialized
2025-05-08 12:02:04,627 - INFO - JSON parsing completed
2025-05-08 12:02:16,263 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 12:02:19,073 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 12:02:19,254 - INFO - Closing down clientserver connection
2025-05-08 12:02:51,099 - INFO - Starting stream_live_streaming.py
2025-05-08 12:02:51,107 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 12:02:51,116 - INFO - Private key loaded successfully
2025-05-08 12:02:51,117 - INFO - Snowflake configurations loaded
2025-05-08 12:02:51,128 - INFO - AWS configurations loaded
2025-05-08 12:03:10,507 - INFO - Spark session initialized
2025-05-08 12:03:10,509 - INFO - Schema defined
2025-05-08 12:03:39,617 - INFO - Kafka stream initialized
2025-05-08 12:03:41,647 - INFO - JSON parsing completed
2025-05-08 12:03:49,962 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 12:03:51,157 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 12:03:51,361 - INFO - Closing down clientserver connection
2025-05-08 12:04:11,696 - INFO - Starting stream_live_streaming.py
2025-05-08 12:04:11,701 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 12:04:11,709 - INFO - Private key loaded successfully
2025-05-08 12:04:11,710 - INFO - Snowflake configurations loaded
2025-05-08 12:04:11,718 - INFO - AWS configurations loaded
2025-05-08 12:04:23,273 - INFO - Spark session initialized
2025-05-08 12:04:23,275 - INFO - Schema defined
2025-05-08 12:04:44,707 - INFO - Kafka stream initialized
2025-05-08 12:04:46,633 - INFO - JSON parsing completed
2025-05-08 12:04:55,388 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 12:04:56,914 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 12:04:57,125 - INFO - Closing down clientserver connection
2025-05-08 12:05:20,030 - INFO - Starting stream_live_streaming.py
2025-05-08 12:05:20,035 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 12:05:20,042 - INFO - Private key loaded successfully
2025-05-08 12:05:20,043 - INFO - Snowflake configurations loaded
2025-05-08 12:05:20,047 - INFO - AWS configurations loaded
2025-05-08 12:05:31,864 - INFO - Spark session initialized
2025-05-08 12:05:31,865 - INFO - Schema defined
2025-05-08 12:05:53,559 - INFO - Kafka stream initialized
2025-05-08 12:05:55,335 - INFO - JSON parsing completed
2025-05-08 12:06:03,833 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 12:06:05,446 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 12:06:05,561 - INFO - Closing down clientserver connection
2025-05-08 12:06:33,871 - INFO - Starting stream_live_streaming.py
2025-05-08 12:06:33,877 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 12:06:33,883 - INFO - Private key loaded successfully
2025-05-08 12:06:33,884 - INFO - Snowflake configurations loaded
2025-05-08 12:06:33,889 - INFO - AWS configurations loaded
2025-05-08 12:06:44,329 - INFO - Spark session initialized
2025-05-08 12:06:44,330 - INFO - Schema defined
2025-05-08 12:07:03,394 - INFO - Kafka stream initialized
2025-05-08 12:07:05,310 - INFO - JSON parsing completed
2025-05-08 12:07:14,211 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 12:07:15,529 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 12:07:15,630 - INFO - Closing down clientserver connection
2025-05-08 12:07:34,104 - INFO - Starting stream_live_streaming.py
2025-05-08 12:07:34,109 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 12:07:34,117 - INFO - Private key loaded successfully
2025-05-08 12:07:34,118 - INFO - Snowflake configurations loaded
2025-05-08 12:07:34,125 - INFO - AWS configurations loaded
2025-05-08 12:07:44,537 - INFO - Spark session initialized
2025-05-08 12:07:44,539 - INFO - Schema defined
2025-05-08 12:08:03,332 - INFO - Kafka stream initialized
2025-05-08 12:08:05,435 - INFO - JSON parsing completed
2025-05-08 12:08:16,563 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 12:08:18,374 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 12:08:18,473 - INFO - Closing down clientserver connection
2025-05-08 12:08:39,410 - INFO - Starting stream_live_streaming.py
2025-05-08 12:08:39,416 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 12:08:39,422 - INFO - Private key loaded successfully
2025-05-08 12:08:39,491 - INFO - Snowflake configurations loaded
2025-05-08 12:08:39,516 - INFO - AWS configurations loaded
2025-05-08 12:08:49,899 - INFO - Spark session initialized
2025-05-08 12:08:49,900 - INFO - Schema defined
2025-05-08 12:09:08,302 - INFO - Kafka stream initialized
2025-05-08 12:09:09,927 - INFO - JSON parsing completed
2025-05-08 12:09:17,518 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 12:09:19,558 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 12:09:19,649 - INFO - Closing down clientserver connection
2025-05-08 12:09:41,783 - INFO - Starting stream_live_streaming.py
2025-05-08 12:09:41,789 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 12:09:41,846 - INFO - Private key loaded successfully
2025-05-08 12:09:41,847 - INFO - Snowflake configurations loaded
2025-05-08 12:09:41,857 - INFO - AWS configurations loaded
2025-05-08 12:09:53,744 - INFO - Spark session initialized
2025-05-08 12:09:53,747 - INFO - Schema defined
2025-05-08 12:10:13,976 - INFO - Kafka stream initialized
2025-05-08 12:10:15,778 - INFO - JSON parsing completed
2025-05-08 12:10:22,464 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 12:10:24,090 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 12:10:24,196 - INFO - Closing down clientserver connection
2025-05-08 12:10:47,811 - INFO - Starting stream_live_streaming.py
2025-05-08 12:10:47,818 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 12:10:47,825 - INFO - Private key loaded successfully
2025-05-08 12:10:47,884 - INFO - Snowflake configurations loaded
2025-05-08 12:10:47,903 - INFO - AWS configurations loaded
2025-05-08 12:11:00,387 - INFO - Spark session initialized
2025-05-08 12:11:00,388 - INFO - Schema defined
2025-05-08 12:11:19,406 - INFO - Kafka stream initialized
2025-05-08 12:11:21,216 - INFO - JSON parsing completed
2025-05-08 12:11:28,322 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 12:11:29,925 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 12:11:30,210 - INFO - Closing down clientserver connection
2025-05-08 12:11:55,694 - INFO - Starting stream_live_streaming.py
2025-05-08 12:11:55,699 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 12:11:55,735 - INFO - Private key loaded successfully
2025-05-08 12:11:55,736 - INFO - Snowflake configurations loaded
2025-05-08 12:11:55,740 - INFO - AWS configurations loaded
2025-05-08 12:12:10,665 - INFO - Spark session initialized
2025-05-08 12:12:10,666 - INFO - Schema defined
2025-05-08 12:12:30,985 - INFO - Kafka stream initialized
2025-05-08 12:12:32,603 - INFO - JSON parsing completed
2025-05-08 12:12:41,390 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 12:12:42,887 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 12:12:42,922 - INFO - Closing down clientserver connection
2025-05-08 12:13:05,797 - INFO - Starting stream_live_streaming.py
2025-05-08 12:13:05,802 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 12:13:05,807 - INFO - Private key loaded successfully
2025-05-08 12:13:05,808 - INFO - Snowflake configurations loaded
2025-05-08 12:13:05,812 - INFO - AWS configurations loaded
2025-05-08 12:13:19,028 - INFO - Spark session initialized
2025-05-08 12:13:19,029 - INFO - Schema defined
2025-05-08 12:13:37,559 - INFO - Kafka stream initialized
2025-05-08 12:13:39,062 - INFO - JSON parsing completed
2025-05-08 12:13:47,226 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 12:13:48,727 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 12:13:48,850 - INFO - Closing down clientserver connection
2025-05-08 12:14:09,960 - INFO - Starting stream_live_streaming.py
2025-05-08 12:14:09,972 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 12:14:10,045 - INFO - Private key loaded successfully
2025-05-08 12:14:10,046 - INFO - Snowflake configurations loaded
2025-05-08 12:14:10,070 - INFO - AWS configurations loaded
2025-05-08 12:14:22,946 - INFO - Spark session initialized
2025-05-08 12:14:22,952 - INFO - Schema defined
2025-05-08 12:14:43,891 - INFO - Kafka stream initialized
2025-05-08 12:14:45,983 - INFO - JSON parsing completed
2025-05-08 12:14:52,985 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 12:14:54,689 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 12:14:54,792 - INFO - Closing down clientserver connection
2025-05-08 12:15:15,193 - INFO - Starting stream_live_streaming.py
2025-05-08 12:15:15,199 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 12:15:15,207 - INFO - Private key loaded successfully
2025-05-08 12:15:15,210 - INFO - Snowflake configurations loaded
2025-05-08 12:15:15,215 - INFO - AWS configurations loaded
2025-05-08 12:15:26,597 - INFO - Spark session initialized
2025-05-08 12:15:26,600 - INFO - Schema defined
2025-05-08 12:15:45,027 - INFO - Kafka stream initialized
2025-05-08 12:15:46,936 - INFO - JSON parsing completed
2025-05-08 12:15:56,534 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 12:15:59,120 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 12:15:59,244 - INFO - Closing down clientserver connection
2025-05-08 12:16:24,391 - INFO - Starting stream_live_streaming.py
2025-05-08 12:16:24,434 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 12:16:24,448 - INFO - Private key loaded successfully
2025-05-08 12:16:24,449 - INFO - Snowflake configurations loaded
2025-05-08 12:16:24,465 - INFO - AWS configurations loaded
2025-05-08 12:16:39,176 - INFO - Spark session initialized
2025-05-08 12:16:39,178 - INFO - Schema defined
2025-05-08 12:17:03,171 - INFO - Kafka stream initialized
2025-05-08 12:17:05,382 - INFO - JSON parsing completed
2025-05-08 12:17:14,400 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 12:17:16,396 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 12:17:16,511 - INFO - Closing down clientserver connection
2025-05-08 12:17:40,622 - INFO - Starting stream_live_streaming.py
2025-05-08 12:17:40,627 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 12:17:40,631 - INFO - Private key loaded successfully
2025-05-08 12:17:40,632 - INFO - Snowflake configurations loaded
2025-05-08 12:17:40,641 - INFO - AWS configurations loaded
2025-05-08 12:17:53,914 - INFO - Spark session initialized
2025-05-08 12:17:53,915 - INFO - Schema defined
2025-05-08 12:18:16,321 - INFO - Kafka stream initialized
2025-05-08 12:18:19,222 - INFO - JSON parsing completed
2025-05-08 12:18:29,236 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 12:18:31,416 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 12:18:31,624 - INFO - Closing down clientserver connection
2025-05-08 12:18:57,051 - INFO - Starting stream_live_streaming.py
2025-05-08 12:18:57,059 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 12:18:57,066 - INFO - Private key loaded successfully
2025-05-08 12:18:57,067 - INFO - Snowflake configurations loaded
2025-05-08 12:18:57,136 - INFO - AWS configurations loaded
2025-05-08 12:19:10,171 - INFO - Spark session initialized
2025-05-08 12:19:10,173 - INFO - Schema defined
2025-05-08 12:19:30,885 - INFO - Kafka stream initialized
2025-05-08 12:19:34,076 - INFO - JSON parsing completed
2025-05-08 12:19:43,377 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 12:19:45,481 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 12:19:45,589 - INFO - Closing down clientserver connection
2025-05-08 12:20:08,908 - INFO - Starting stream_live_streaming.py
2025-05-08 12:20:08,914 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 12:20:08,921 - INFO - Private key loaded successfully
2025-05-08 12:20:08,922 - INFO - Snowflake configurations loaded
2025-05-08 12:20:08,944 - INFO - AWS configurations loaded
2025-05-08 12:20:08,958 - INFO - Error while receiving.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "/opt/bitnami/python/lib/python3.9/socket.py", line 704, in readinto
    return self._sock.recv_into(b)
ConnectionResetError: [Errno 104] Connection reset by peer
2025-05-08 12:20:08,962 - INFO - Closing down clientserver connection
2025-05-08 12:20:08,963 - ERROR - Error in stream_live_streaming.py: Error while sending or receiving
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "/opt/bitnami/python/lib/python3.9/socket.py", line 704, in readinto
    return self._sock.recv_into(b)
ConnectionResetError: [Errno 104] Connection reset by peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 72, in <module>
    spark = SparkSession.builder \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/session.py", line 477, in getOrCreate
    sc = SparkContext.getOrCreate(sparkConf)
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/context.py", line 512, in getOrCreate
    SparkContext(conf=conf or SparkConf())
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/context.py", line 198, in __init__
    SparkContext._ensure_initialized(self, gateway=gateway, conf=conf)
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/context.py", line 432, in _ensure_initialized
    SparkContext._gateway = gateway or launch_gateway(conf)
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/java_gateway.py", line 150, in launch_gateway
    java_import(gateway.jvm, "org.apache.spark.SparkConf")
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 180, in java_import
    answer = gateway_client.send_command(command)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1036, in send_command
    connection = self._get_connection()
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 284, in _get_connection
    connection = self._create_new_connection()
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 291, in _create_new_connection
    connection.connect_to_java_server()
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 443, in connect_to_java_server
    self._authenticate_connection()
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 459, in _authenticate_connection
    answer = self.send_command(cmd)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
2025-05-08 12:20:08,995 - INFO - Closing down clientserver connection
2025-05-08 12:20:59,067 - INFO - Starting stream_live_streaming.py
2025-05-08 12:20:59,074 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 12:20:59,082 - INFO - Private key loaded successfully
2025-05-08 12:20:59,082 - INFO - Snowflake configurations loaded
2025-05-08 12:20:59,119 - INFO - AWS configurations loaded
2025-05-08 12:21:13,067 - INFO - Spark session initialized
2025-05-08 12:21:13,068 - INFO - Schema defined
2025-05-08 12:21:36,681 - INFO - Kafka stream initialized
2025-05-08 12:21:41,205 - INFO - JSON parsing completed
2025-05-08 12:21:52,879 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 12:21:55,077 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 12:21:55,194 - INFO - Closing down clientserver connection
2025-05-08 12:22:20,299 - INFO - Starting stream_live_streaming.py
2025-05-08 12:22:20,309 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 12:22:20,314 - INFO - Private key loaded successfully
2025-05-08 12:22:20,315 - INFO - Snowflake configurations loaded
2025-05-08 12:22:20,320 - INFO - AWS configurations loaded
2025-05-08 12:22:37,104 - INFO - Spark session initialized
2025-05-08 12:22:37,105 - INFO - Schema defined
2025-05-08 12:23:07,630 - INFO - Kafka stream initialized
2025-05-08 12:23:11,117 - INFO - JSON parsing completed
2025-05-08 12:23:26,326 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 12:23:30,032 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 12:23:30,419 - INFO - Closing down clientserver connection
2025-05-08 12:23:58,862 - INFO - Starting stream_live_streaming.py
2025-05-08 12:23:58,937 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 12:23:58,948 - INFO - Private key loaded successfully
2025-05-08 12:23:58,950 - INFO - Snowflake configurations loaded
2025-05-08 12:23:58,968 - INFO - AWS configurations loaded
2025-05-08 12:24:15,381 - INFO - Spark session initialized
2025-05-08 12:24:15,383 - INFO - Schema defined
2025-05-08 12:24:41,183 - INFO - Kafka stream initialized
2025-05-08 12:24:43,786 - INFO - JSON parsing completed
2025-05-08 12:24:54,093 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 12:24:56,587 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 12:24:56,779 - INFO - Closing down clientserver connection
2025-05-08 12:25:22,416 - INFO - Starting stream_live_streaming.py
2025-05-08 12:25:22,423 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 12:25:22,490 - INFO - Private key loaded successfully
2025-05-08 12:25:22,492 - INFO - Snowflake configurations loaded
2025-05-08 12:25:22,502 - INFO - AWS configurations loaded
2025-05-08 12:25:37,226 - INFO - Spark session initialized
2025-05-08 12:25:37,227 - INFO - Schema defined
2025-05-08 12:25:56,930 - INFO - Kafka stream initialized
2025-05-08 12:25:59,433 - INFO - JSON parsing completed
2025-05-08 12:26:09,027 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 12:26:11,125 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 12:26:11,237 - INFO - Closing down clientserver connection
2025-05-08 12:26:37,250 - INFO - Starting stream_live_streaming.py
2025-05-08 12:26:37,258 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 12:26:37,265 - INFO - Private key loaded successfully
2025-05-08 12:26:37,277 - INFO - Snowflake configurations loaded
2025-05-08 12:26:37,349 - INFO - AWS configurations loaded
2025-05-08 12:26:50,248 - INFO - Spark session initialized
2025-05-08 12:26:50,250 - INFO - Schema defined
2025-05-08 12:27:11,051 - INFO - Kafka stream initialized
2025-05-08 12:27:13,567 - INFO - JSON parsing completed
2025-05-08 12:27:23,978 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 12:27:25,975 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 12:27:26,076 - INFO - Closing down clientserver connection
2025-05-08 12:27:50,108 - INFO - Starting stream_live_streaming.py
2025-05-08 12:27:50,116 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 12:27:50,127 - INFO - Private key loaded successfully
2025-05-08 12:27:50,128 - INFO - Snowflake configurations loaded
2025-05-08 12:27:50,184 - INFO - AWS configurations loaded
2025-05-08 12:28:03,802 - INFO - Spark session initialized
2025-05-08 12:28:03,803 - INFO - Schema defined
2025-05-08 12:28:25,691 - INFO - Kafka stream initialized
2025-05-08 12:28:28,107 - INFO - JSON parsing completed
2025-05-08 12:28:38,009 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 12:28:39,559 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 12:28:39,632 - INFO - Closing down clientserver connection
2025-05-08 12:29:04,755 - INFO - Starting stream_live_streaming.py
2025-05-08 12:29:04,762 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 12:29:04,771 - INFO - Private key loaded successfully
2025-05-08 12:29:04,772 - INFO - Snowflake configurations loaded
2025-05-08 12:29:04,827 - INFO - AWS configurations loaded
2025-05-08 12:29:21,828 - INFO - Spark session initialized
2025-05-08 12:29:21,829 - INFO - Schema defined
2025-05-08 12:29:45,060 - INFO - Kafka stream initialized
2025-05-08 12:29:47,144 - INFO - JSON parsing completed
2025-05-08 12:29:56,658 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 12:29:58,655 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 12:29:58,778 - INFO - Closing down clientserver connection
2025-05-08 12:30:22,803 - INFO - Starting stream_live_streaming.py
2025-05-08 12:30:22,871 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 12:30:22,892 - INFO - Private key loaded successfully
2025-05-08 12:30:22,893 - INFO - Snowflake configurations loaded
2025-05-08 12:30:22,904 - INFO - AWS configurations loaded
2025-05-08 12:30:38,270 - INFO - Spark session initialized
2025-05-08 12:30:38,272 - INFO - Schema defined
2025-05-08 12:30:59,869 - INFO - Kafka stream initialized
2025-05-08 12:31:01,615 - INFO - JSON parsing completed
2025-05-08 12:31:13,122 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 12:31:14,789 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 12:31:14,828 - INFO - Closing down clientserver connection
2025-05-08 12:31:37,808 - INFO - Starting stream_live_streaming.py
2025-05-08 12:31:37,815 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 12:31:37,825 - INFO - Private key loaded successfully
2025-05-08 12:31:37,825 - INFO - Snowflake configurations loaded
2025-05-08 12:31:37,834 - INFO - AWS configurations loaded
2025-05-08 12:31:50,542 - INFO - Spark session initialized
2025-05-08 12:31:50,543 - INFO - Schema defined
2025-05-08 12:32:13,254 - INFO - Kafka stream initialized
2025-05-08 12:32:14,945 - INFO - JSON parsing completed
2025-05-08 12:32:23,322 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 12:32:25,418 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 12:32:25,536 - INFO - Closing down clientserver connection
2025-05-08 12:32:47,342 - INFO - Starting stream_live_streaming.py
2025-05-08 12:32:47,347 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 12:32:47,360 - INFO - Private key loaded successfully
2025-05-08 12:32:47,361 - INFO - Snowflake configurations loaded
2025-05-08 12:32:47,365 - INFO - AWS configurations loaded
2025-05-08 12:32:59,560 - INFO - Spark session initialized
2025-05-08 12:32:59,562 - INFO - Schema defined
2025-05-08 12:33:21,976 - INFO - Kafka stream initialized
2025-05-08 12:33:24,970 - INFO - JSON parsing completed
2025-05-08 12:33:35,485 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 12:33:37,410 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 12:33:37,498 - INFO - Closing down clientserver connection
2025-05-08 12:34:05,669 - INFO - Starting stream_live_streaming.py
2025-05-08 12:34:05,677 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 12:34:05,687 - INFO - Private key loaded successfully
2025-05-08 12:34:05,688 - INFO - Snowflake configurations loaded
2025-05-08 12:34:05,697 - INFO - AWS configurations loaded
2025-05-08 12:34:18,373 - INFO - Spark session initialized
2025-05-08 12:34:18,374 - INFO - Schema defined
2025-05-08 12:34:41,812 - INFO - Kafka stream initialized
2025-05-08 12:34:44,816 - INFO - JSON parsing completed
2025-05-08 12:34:56,016 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 12:34:58,326 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 12:34:58,507 - INFO - Closing down clientserver connection
2025-05-08 12:35:20,052 - INFO - Starting stream_live_streaming.py
2025-05-08 12:35:20,059 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 12:35:20,065 - INFO - Private key loaded successfully
2025-05-08 12:35:20,066 - INFO - Snowflake configurations loaded
2025-05-08 12:35:20,071 - INFO - AWS configurations loaded
2025-05-08 12:35:31,754 - INFO - Spark session initialized
2025-05-08 12:35:31,756 - INFO - Schema defined
2025-05-08 12:35:54,638 - INFO - Kafka stream initialized
2025-05-08 12:35:57,549 - INFO - JSON parsing completed
2025-05-08 12:36:08,559 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 12:36:11,148 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 12:36:11,271 - INFO - Closing down clientserver connection
2025-05-08 12:36:35,187 - INFO - Starting stream_live_streaming.py
2025-05-08 12:36:35,262 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 12:36:35,272 - INFO - Private key loaded successfully
2025-05-08 12:36:35,273 - INFO - Snowflake configurations loaded
2025-05-08 12:36:35,280 - INFO - AWS configurations loaded
2025-05-08 12:36:48,957 - INFO - Spark session initialized
2025-05-08 12:36:48,959 - INFO - Schema defined
2025-05-08 12:37:10,981 - INFO - Kafka stream initialized
2025-05-08 12:37:13,566 - INFO - JSON parsing completed
2025-05-08 12:37:24,084 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 12:37:25,890 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 12:37:25,998 - INFO - Closing down clientserver connection
2025-05-08 12:37:51,702 - INFO - Starting stream_live_streaming.py
2025-05-08 12:37:51,782 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 12:37:51,788 - INFO - Private key loaded successfully
2025-05-08 12:37:51,789 - INFO - Snowflake configurations loaded
2025-05-08 12:37:51,796 - INFO - AWS configurations loaded
2025-05-08 12:38:05,597 - INFO - Spark session initialized
2025-05-08 12:38:05,598 - INFO - Schema defined
2025-05-08 12:38:26,514 - INFO - Kafka stream initialized
2025-05-08 12:38:28,840 - INFO - JSON parsing completed
2025-05-08 12:38:39,022 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 12:38:41,107 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 12:38:41,152 - INFO - Closing down clientserver connection
2025-05-08 12:39:05,721 - INFO - Starting stream_live_streaming.py
2025-05-08 12:39:05,727 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 12:39:05,735 - INFO - Private key loaded successfully
2025-05-08 12:39:05,737 - INFO - Snowflake configurations loaded
2025-05-08 12:39:05,742 - INFO - AWS configurations loaded
2025-05-08 12:39:18,842 - INFO - Spark session initialized
2025-05-08 12:39:18,844 - INFO - Schema defined
2025-05-08 12:39:40,932 - INFO - Kafka stream initialized
2025-05-08 12:39:43,629 - INFO - JSON parsing completed
2025-05-08 12:39:53,546 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 12:39:54,853 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 12:39:55,055 - INFO - Closing down clientserver connection
2025-05-08 12:40:21,193 - INFO - Starting stream_live_streaming.py
2025-05-08 12:40:21,200 - INFO - Attempting to load private key from: /app/config/snowflake_key.pem
2025-05-08 12:40:21,201 - ERROR - Private key file does not exist at: /app/config/snowflake_key.pem
2025-05-08 12:40:21,202 - ERROR - Error in stream_live_streaming.py: Private key file does not exist at: /app/config/snowflake_key.pem
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 39, in <module>
    raise ValueError(f"Private key file does not exist at: {private_key_path}")
ValueError: Private key file does not exist at: /app/config/snowflake_key.pem
2025-05-08 12:40:44,591 - INFO - Starting stream_live_streaming.py
2025-05-08 12:40:44,659 - INFO - Attempting to load private key from: /app/config/rsa_key.pem
2025-05-08 12:40:44,666 - INFO - Private key loaded successfully
2025-05-08 12:40:44,667 - INFO - Snowflake configurations loaded
2025-05-08 12:40:44,672 - INFO - AWS configurations loaded
2025-05-08 12:40:56,977 - INFO - Spark session initialized
2025-05-08 12:40:56,978 - INFO - Schema defined
2025-05-08 12:41:19,886 - INFO - Kafka stream initialized
2025-05-08 12:41:22,284 - INFO - JSON parsing completed
2025-05-08 12:41:32,205 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 12:41:34,613 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 12:41:34,805 - INFO - Closing down clientserver connection
2025-05-08 12:42:01,194 - INFO - Starting stream_live_streaming.py
2025-05-08 12:42:01,202 - INFO - Attempting to load private key from: /app/config/rsa_key.pem
2025-05-08 12:42:01,211 - INFO - Private key loaded successfully
2025-05-08 12:42:01,212 - INFO - Snowflake configurations loaded
2025-05-08 12:42:01,219 - INFO - AWS configurations loaded
2025-05-08 12:42:14,612 - INFO - Spark session initialized
2025-05-08 12:42:14,613 - INFO - Schema defined
2025-05-08 12:42:26,149 - INFO - Error while receiving.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty
2025-05-08 12:42:26,156 - INFO - Closing down clientserver connection
2025-05-08 12:42:26,160 - ERROR - Exception while sending command.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
2025-05-08 12:42:26,163 - INFO - Closing down clientserver connection
2025-05-08 12:42:26,164 - ERROR - Error in stream_live_streaming.py: An error occurred while calling o42.load
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 99, in <module>
    kafka_df = spark.readStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 277, in load
    return self._df(self._jreader.load())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 169, in deco
    return f(*a, **kw)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 334, in get_return_value
    raise Py4JError(
py4j.protocol.Py4JError: An error occurred while calling o42.load
2025-05-08 12:42:26,191 - INFO - Closing down clientserver connection
2025-05-08 12:43:16,421 - INFO - Starting stream_live_streaming.py
2025-05-08 12:43:16,428 - INFO - Attempting to load private key from: /app/config/rsa_key.pem
2025-05-08 12:43:16,434 - INFO - Private key loaded successfully
2025-05-08 12:43:16,435 - INFO - Snowflake configurations loaded
2025-05-08 12:43:16,440 - INFO - AWS configurations loaded
2025-05-08 12:43:30,525 - INFO - Spark session initialized
2025-05-08 12:43:30,526 - INFO - Schema defined
2025-05-08 12:43:53,319 - INFO - Kafka stream initialized
2025-05-08 12:43:55,646 - INFO - JSON parsing completed
2025-05-08 12:44:06,742 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 12:44:08,773 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 12:44:08,943 - INFO - Closing down clientserver connection
2025-05-08 12:44:31,675 - INFO - Starting stream_live_streaming.py
2025-05-08 12:44:31,679 - INFO - Attempting to load private key from: /app/config/rsa_key.pem
2025-05-08 12:44:31,684 - INFO - Private key loaded successfully
2025-05-08 12:44:31,685 - INFO - Snowflake configurations loaded
2025-05-08 12:44:31,690 - INFO - AWS configurations loaded
2025-05-08 12:44:43,078 - INFO - Spark session initialized
2025-05-08 12:44:43,080 - INFO - Schema defined
2025-05-08 12:45:06,006 - INFO - Kafka stream initialized
2025-05-08 12:45:08,604 - INFO - JSON parsing completed
2025-05-08 12:45:18,714 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 12:45:20,803 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 12:45:21,107 - INFO - Closing down clientserver connection
2025-05-08 12:45:46,506 - INFO - Starting stream_live_streaming.py
2025-05-08 12:45:46,514 - INFO - Attempting to load private key from: /app/config/rsa_key.pem
2025-05-08 12:45:46,524 - INFO - Private key loaded successfully
2025-05-08 12:45:46,586 - INFO - Snowflake configurations loaded
2025-05-08 12:45:46,594 - INFO - AWS configurations loaded
2025-05-08 12:45:58,942 - INFO - Spark session initialized
2025-05-08 12:45:58,944 - INFO - Schema defined
2025-05-08 12:46:19,266 - INFO - Kafka stream initialized
2025-05-08 12:46:21,363 - INFO - JSON parsing completed
2025-05-08 12:46:30,262 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 12:46:32,386 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 12:46:32,565 - INFO - Closing down clientserver connection
2025-05-08 12:46:57,897 - INFO - Starting stream_live_streaming.py
2025-05-08 12:46:57,906 - INFO - Attempting to load private key from: /app/config/rsa_key.pem
2025-05-08 12:46:57,915 - INFO - Private key loaded successfully
2025-05-08 12:46:57,967 - INFO - Snowflake configurations loaded
2025-05-08 12:46:57,974 - INFO - AWS configurations loaded
2025-05-08 12:47:10,783 - INFO - Spark session initialized
2025-05-08 12:47:10,784 - INFO - Schema defined
2025-05-08 12:47:32,876 - INFO - Kafka stream initialized
2025-05-08 12:47:35,883 - INFO - JSON parsing completed
2025-05-08 12:47:46,371 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 12:47:48,789 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 12:47:48,987 - INFO - Closing down clientserver connection
2025-05-08 12:48:14,992 - INFO - Starting stream_live_streaming.py
2025-05-08 12:48:15,000 - INFO - Attempting to load private key from: /app/config/rsa_key.pem
2025-05-08 12:48:15,009 - INFO - Private key loaded successfully
2025-05-08 12:48:15,011 - INFO - Snowflake configurations loaded
2025-05-08 12:48:15,017 - INFO - AWS configurations loaded
2025-05-08 12:48:26,190 - INFO - Spark session initialized
2025-05-08 12:48:26,191 - INFO - Schema defined
2025-05-08 12:48:48,197 - INFO - Error while receiving.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty
2025-05-08 12:48:48,199 - INFO - Closing down clientserver connection
2025-05-08 12:48:48,200 - ERROR - Exception while sending command.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
2025-05-08 12:48:48,203 - INFO - Closing down clientserver connection
2025-05-08 12:48:48,204 - ERROR - Error in stream_live_streaming.py: An error occurred while calling o42.load
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 99, in <module>
    kafka_df = spark.readStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 277, in load
    return self._df(self._jreader.load())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 169, in deco
    return f(*a, **kw)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 334, in get_return_value
    raise Py4JError(
py4j.protocol.Py4JError: An error occurred while calling o42.load
2025-05-08 12:48:48,211 - INFO - Closing down clientserver connection
2025-05-08 12:49:35,041 - INFO - Starting stream_live_streaming.py
2025-05-08 12:49:35,047 - INFO - Attempting to load private key from: /app/config/rsa_key.pem
2025-05-08 12:49:35,052 - INFO - Private key loaded successfully
2025-05-08 12:49:35,053 - INFO - Snowflake configurations loaded
2025-05-08 12:49:35,058 - INFO - AWS configurations loaded
2025-05-08 12:49:47,651 - INFO - Spark session initialized
2025-05-08 12:49:47,653 - INFO - Schema defined
2025-05-08 12:50:11,026 - INFO - Kafka stream initialized
2025-05-08 12:50:13,453 - INFO - JSON parsing completed
2025-05-08 12:50:25,272 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 12:50:27,360 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 12:50:27,487 - INFO - Closing down clientserver connection
2025-05-08 12:50:49,142 - INFO - Starting stream_live_streaming.py
2025-05-08 12:50:49,149 - INFO - Attempting to load private key from: /app/config/rsa_key.pem
2025-05-08 12:50:49,192 - INFO - Private key loaded successfully
2025-05-08 12:50:49,193 - INFO - Snowflake configurations loaded
2025-05-08 12:50:49,201 - INFO - AWS configurations loaded
2025-05-08 12:51:00,804 - INFO - Spark session initialized
2025-05-08 12:51:00,805 - INFO - Schema defined
2025-05-08 12:51:24,000 - INFO - Kafka stream initialized
2025-05-08 12:51:26,729 - INFO - JSON parsing completed
2025-05-08 12:51:37,608 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 12:51:40,602 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 12:51:40,810 - INFO - Closing down clientserver connection
2025-05-08 12:52:07,124 - INFO - Starting stream_live_streaming.py
2025-05-08 12:52:07,134 - INFO - Attempting to load private key from: /app/config/rsa_key.pem
2025-05-08 12:52:07,141 - INFO - Private key loaded successfully
2025-05-08 12:52:07,142 - INFO - Snowflake configurations loaded
2025-05-08 12:52:07,150 - INFO - AWS configurations loaded
2025-05-08 12:52:20,243 - INFO - Spark session initialized
2025-05-08 12:52:20,244 - INFO - Schema defined
2025-05-08 12:52:41,564 - INFO - Kafka stream initialized
2025-05-08 12:52:43,545 - INFO - JSON parsing completed
2025-05-08 12:52:54,245 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 12:52:56,357 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 12:52:56,544 - INFO - Closing down clientserver connection
2025-05-08 12:53:22,757 - INFO - Starting stream_live_streaming.py
2025-05-08 12:53:22,765 - INFO - Attempting to load private key from: /app/config/rsa_key.pem
2025-05-08 12:53:22,774 - INFO - Private key loaded successfully
2025-05-08 12:53:22,776 - INFO - Snowflake configurations loaded
2025-05-08 12:53:22,783 - INFO - AWS configurations loaded
2025-05-08 12:53:34,878 - INFO - Spark session initialized
2025-05-08 12:53:34,879 - INFO - Schema defined
2025-05-08 12:53:55,850 - INFO - Kafka stream initialized
2025-05-08 12:53:58,256 - INFO - JSON parsing completed
2025-05-08 12:54:07,136 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 12:54:09,130 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 12:54:09,232 - INFO - Closing down clientserver connection
2025-05-08 12:54:34,512 - INFO - Starting stream_live_streaming.py
2025-05-08 12:54:34,521 - INFO - Attempting to load private key from: /app/config/rsa_key.pem
2025-05-08 12:54:34,531 - INFO - Private key loaded successfully
2025-05-08 12:54:34,532 - INFO - Snowflake configurations loaded
2025-05-08 12:54:34,540 - INFO - AWS configurations loaded
2025-05-08 12:54:45,635 - INFO - Spark session initialized
2025-05-08 12:54:45,636 - INFO - Schema defined
2025-05-08 12:55:06,722 - INFO - Kafka stream initialized
2025-05-08 12:55:09,121 - INFO - JSON parsing completed
2025-05-08 12:55:10,755 - INFO - Error while receiving.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty
2025-05-08 12:55:10,758 - INFO - Closing down clientserver connection
2025-05-08 12:55:10,759 - ERROR - Exception while sending command.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
2025-05-08 12:55:10,767 - INFO - Closing down clientserver connection
2025-05-08 12:55:10,771 - INFO - Error while receiving.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty
2025-05-08 12:55:10,772 - INFO - Closing down clientserver connection
2025-05-08 12:55:10,773 - ERROR - Exception while sending command.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
2025-05-08 12:55:10,775 - INFO - Closing down clientserver connection
2025-05-08 12:55:10,776 - INFO - Closing down clientserver connection
2025-05-08 12:55:10,777 - INFO - Closing down clientserver connection
2025-05-08 12:55:10,795 - INFO - Closing down clientserver connection
2025-05-08 12:55:10,796 - INFO - Closing down clientserver connection
2025-05-08 12:55:10,797 - INFO - Closing down clientserver connection
2025-05-08 12:55:10,769 - ERROR - Error in stream_live_streaming.py: An error occurred while calling z:org.apache.spark.sql.functions.col
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 165, in <module>
    when(col("EventType").isin("Sports", "Concert", "Gaming", "Q&A"), "LiveStream").otherwise(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 159, in wrapped
    return f(*args, **kwargs)
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/functions.py", line 221, in col
    return _invoke_function("col", col)
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/functions.py", line 95, in _invoke_function
    return Column(jf(*args))
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 169, in deco
    return f(*a, **kw)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 334, in get_return_value
    raise Py4JError(
py4j.protocol.Py4JError: An error occurred while calling z:org.apache.spark.sql.functions.col
2025-05-08 12:55:10,798 - INFO - Closing down clientserver connection
2025-05-08 12:55:56,990 - INFO - Starting stream_live_streaming.py
2025-05-08 12:55:56,998 - INFO - Attempting to load private key from: /app/config/rsa_key.pem
2025-05-08 12:55:57,007 - INFO - Private key loaded successfully
2025-05-08 12:55:57,008 - INFO - Snowflake configurations loaded
2025-05-08 12:55:57,013 - INFO - AWS configurations loaded
2025-05-08 12:56:10,463 - INFO - Spark session initialized
2025-05-08 12:56:10,464 - INFO - Schema defined
2025-05-08 12:56:34,557 - INFO - Kafka stream initialized
2025-05-08 12:56:37,453 - INFO - JSON parsing completed
2025-05-08 12:56:49,168 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 12:56:52,100 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 12:56:52,290 - INFO - Closing down clientserver connection
2025-05-08 12:57:16,507 - INFO - Starting stream_live_streaming.py
2025-05-08 12:57:16,514 - INFO - Attempting to load private key from: /app/config/rsa_key.pem
2025-05-08 12:57:16,522 - INFO - Private key loaded successfully
2025-05-08 12:57:16,575 - INFO - Snowflake configurations loaded
2025-05-08 12:57:16,581 - INFO - AWS configurations loaded
2025-05-08 12:57:28,303 - INFO - Spark session initialized
2025-05-08 12:57:28,304 - INFO - Schema defined
2025-05-08 12:57:49,085 - INFO - Kafka stream initialized
2025-05-08 12:57:51,695 - INFO - JSON parsing completed
2025-05-08 12:58:02,484 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 12:58:05,785 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 12:58:06,085 - INFO - Closing down clientserver connection
2025-05-08 12:58:44,793 - INFO - Starting stream_live_streaming.py
2025-05-08 12:58:44,803 - INFO - Attempting to load private key from: /app/config/rsa_key.pem
2025-05-08 12:58:44,828 - INFO - Private key loaded successfully
2025-05-08 12:58:44,829 - INFO - Snowflake configurations loaded
2025-05-08 12:58:44,897 - INFO - AWS configurations loaded
2025-05-08 12:59:04,985 - INFO - Spark session initialized
2025-05-08 12:59:04,986 - INFO - Schema defined
2025-05-08 12:59:39,099 - INFO - Kafka stream initialized
2025-05-08 12:59:43,995 - INFO - JSON parsing completed
2025-05-08 12:59:57,421 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 12:59:59,724 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 12:59:59,938 - INFO - Closing down clientserver connection
2025-05-08 13:00:32,926 - INFO - Starting stream_live_streaming.py
2025-05-08 13:00:32,932 - INFO - Attempting to load private key from: /app/config/rsa_key.pem
2025-05-08 13:00:32,938 - INFO - Private key loaded successfully
2025-05-08 13:00:32,939 - INFO - Snowflake configurations loaded
2025-05-08 13:00:32,992 - INFO - AWS configurations loaded
2025-05-08 13:00:46,103 - INFO - Spark session initialized
2025-05-08 13:00:46,104 - INFO - Schema defined
2025-05-08 13:01:07,210 - INFO - Kafka stream initialized
2025-05-08 13:01:09,599 - INFO - JSON parsing completed
2025-05-08 13:01:19,425 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 13:01:21,413 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 13:01:21,609 - INFO - Closing down clientserver connection
2025-05-08 13:01:46,098 - INFO - Starting stream_live_streaming.py
2025-05-08 13:01:46,104 - INFO - Attempting to load private key from: /app/config/rsa_key.pem
2025-05-08 13:01:46,110 - INFO - Private key loaded successfully
2025-05-08 13:01:46,111 - INFO - Snowflake configurations loaded
2025-05-08 13:01:46,115 - INFO - AWS configurations loaded
2025-05-08 13:01:58,005 - INFO - Spark session initialized
2025-05-08 13:01:58,007 - INFO - Schema defined
2025-05-08 13:02:18,426 - INFO - Kafka stream initialized
2025-05-08 13:02:20,501 - INFO - JSON parsing completed
2025-05-08 13:02:29,699 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 13:02:30,910 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 13:02:31,136 - INFO - Closing down clientserver connection
2025-05-08 13:02:55,008 - INFO - Starting stream_live_streaming.py
2025-05-08 13:02:55,017 - INFO - Attempting to load private key from: /app/config/rsa_key.pem
2025-05-08 13:02:55,026 - INFO - Private key loaded successfully
2025-05-08 13:02:55,027 - INFO - Snowflake configurations loaded
2025-05-08 13:02:55,104 - INFO - AWS configurations loaded
2025-05-08 13:03:07,024 - INFO - Spark session initialized
2025-05-08 13:03:07,025 - INFO - Schema defined
2025-05-08 13:03:28,638 - INFO - Kafka stream initialized
2025-05-08 13:03:29,736 - INFO - JSON parsing completed
2025-05-08 13:03:39,732 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 13:03:41,438 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 13:03:41,625 - INFO - Closing down clientserver connection
2025-05-08 13:04:48,056 - INFO - Starting stream_live_streaming.py
2025-05-08 13:04:48,066 - INFO - Attempting to load private key from: /app/config/rsa_key.pem
2025-05-08 13:04:48,077 - INFO - Private key loaded successfully
2025-05-08 13:04:48,078 - INFO - Snowflake configurations loaded
2025-05-08 13:04:48,087 - INFO - AWS configurations loaded
2025-05-08 13:05:01,442 - INFO - Spark session initialized
2025-05-08 13:05:01,443 - INFO - Schema defined
2025-05-08 13:05:22,742 - INFO - Kafka stream initialized
2025-05-08 13:05:25,277 - INFO - JSON parsing completed
2025-05-08 13:05:35,228 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 13:05:38,316 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 13:05:38,521 - INFO - Closing down clientserver connection
2025-05-08 13:06:13,621 - INFO - Starting stream_live_streaming.py
2025-05-08 13:06:13,698 - INFO - Attempting to load private key from: /app/config/rsa_key.pem
2025-05-08 13:06:13,710 - INFO - Private key loaded successfully
2025-05-08 13:06:13,712 - INFO - Snowflake configurations loaded
2025-05-08 13:06:13,724 - INFO - AWS configurations loaded
2025-05-08 13:06:29,617 - INFO - Spark session initialized
2025-05-08 13:06:29,618 - INFO - Schema defined
2025-05-08 13:07:03,426 - INFO - Kafka stream initialized
2025-05-08 13:07:07,431 - INFO - JSON parsing completed
2025-05-08 13:07:21,931 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 13:07:24,034 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 13:07:24,328 - INFO - Closing down clientserver connection
2025-05-08 13:07:48,041 - INFO - Starting stream_live_streaming.py
2025-05-08 13:07:48,048 - INFO - Attempting to load private key from: /app/config/rsa_key.pem
2025-05-08 13:07:48,056 - INFO - Private key loaded successfully
2025-05-08 13:07:48,058 - INFO - Snowflake configurations loaded
2025-05-08 13:07:48,123 - INFO - AWS configurations loaded
2025-05-08 13:08:01,132 - INFO - Spark session initialized
2025-05-08 13:08:01,133 - INFO - Schema defined
2025-05-08 13:08:20,815 - INFO - Kafka stream initialized
2025-05-08 13:08:22,617 - INFO - JSON parsing completed
2025-05-08 13:08:31,910 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 13:08:34,044 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 13:08:34,135 - INFO - Closing down clientserver connection
2025-05-08 13:09:01,018 - INFO - Starting stream_live_streaming.py
2025-05-08 13:09:01,026 - INFO - Attempting to load private key from: /app/config/rsa_key.pem
2025-05-08 13:09:01,094 - INFO - Private key loaded successfully
2025-05-08 13:09:01,095 - INFO - Snowflake configurations loaded
2025-05-08 13:09:01,102 - INFO - AWS configurations loaded
2025-05-08 13:09:13,389 - INFO - Spark session initialized
2025-05-08 13:09:13,391 - INFO - Schema defined
2025-05-08 13:09:32,401 - INFO - Kafka stream initialized
2025-05-08 13:09:34,811 - INFO - JSON parsing completed
2025-05-08 13:09:44,599 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 13:09:46,502 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 13:09:46,696 - INFO - Closing down clientserver connection
2025-05-08 13:10:09,973 - INFO - Starting stream_live_streaming.py
2025-05-08 13:10:09,982 - INFO - Attempting to load private key from: /app/config/rsa_key.pem
2025-05-08 13:10:09,992 - INFO - Private key loaded successfully
2025-05-08 13:10:09,993 - INFO - Snowflake configurations loaded
2025-05-08 13:10:10,001 - INFO - AWS configurations loaded
2025-05-08 13:10:21,163 - INFO - Spark session initialized
2025-05-08 13:10:21,165 - INFO - Schema defined
2025-05-08 13:10:43,062 - INFO - Kafka stream initialized
2025-05-08 13:10:45,384 - INFO - JSON parsing completed
2025-05-08 13:10:54,568 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 13:10:56,861 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 13:10:57,072 - INFO - Closing down clientserver connection
2025-05-08 13:11:19,058 - INFO - Starting stream_live_streaming.py
2025-05-08 13:11:19,064 - INFO - Attempting to load private key from: /app/config/rsa_key.pem
2025-05-08 13:11:19,072 - INFO - Private key loaded successfully
2025-05-08 13:11:19,073 - INFO - Snowflake configurations loaded
2025-05-08 13:11:19,078 - INFO - AWS configurations loaded
2025-05-08 13:11:32,646 - INFO - Spark session initialized
2025-05-08 13:11:32,659 - INFO - Schema defined
2025-05-08 13:11:54,660 - INFO - Kafka stream initialized
2025-05-08 13:11:56,333 - INFO - JSON parsing completed
2025-05-08 13:12:05,443 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 13:12:07,432 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 13:12:07,649 - INFO - Closing down clientserver connection
2025-05-08 13:12:31,215 - INFO - Starting stream_live_streaming.py
2025-05-08 13:12:31,222 - INFO - Attempting to load private key from: /app/config/rsa_key.pem
2025-05-08 13:12:31,251 - INFO - Private key loaded successfully
2025-05-08 13:12:31,252 - INFO - Snowflake configurations loaded
2025-05-08 13:12:31,318 - INFO - AWS configurations loaded
2025-05-08 13:12:43,025 - INFO - Spark session initialized
2025-05-08 13:12:43,026 - INFO - Schema defined
2025-05-08 13:13:06,112 - INFO - Kafka stream initialized
2025-05-08 13:13:08,504 - INFO - JSON parsing completed
2025-05-08 13:13:18,222 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 13:13:19,809 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 13:13:19,921 - INFO - Closing down clientserver connection
2025-05-08 13:13:45,487 - INFO - Starting stream_live_streaming.py
2025-05-08 13:13:45,496 - INFO - Attempting to load private key from: /app/config/rsa_key.pem
2025-05-08 13:13:45,505 - INFO - Private key loaded successfully
2025-05-08 13:13:45,508 - INFO - Snowflake configurations loaded
2025-05-08 13:13:45,516 - INFO - AWS configurations loaded
2025-05-08 13:13:56,394 - INFO - Spark session initialized
2025-05-08 13:13:56,395 - INFO - Schema defined
2025-05-08 13:14:21,207 - INFO - Kafka stream initialized
2025-05-08 13:14:23,211 - INFO - JSON parsing completed
2025-05-08 13:14:32,899 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 13:14:35,081 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 13:14:35,187 - INFO - Closing down clientserver connection
2025-05-08 13:15:00,093 - INFO - Starting stream_live_streaming.py
2025-05-08 13:15:00,105 - INFO - Attempting to load private key from: /app/config/rsa_key.pem
2025-05-08 13:15:00,179 - INFO - Private key loaded successfully
2025-05-08 13:15:00,181 - INFO - Snowflake configurations loaded
2025-05-08 13:15:00,279 - INFO - AWS configurations loaded
2025-05-08 13:15:11,286 - INFO - Spark session initialized
2025-05-08 13:15:11,288 - INFO - Schema defined
2025-05-08 13:15:29,691 - INFO - Error while receiving.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty
2025-05-08 13:15:29,693 - INFO - Closing down clientserver connection
2025-05-08 13:15:29,701 - ERROR - Exception while sending command.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
2025-05-08 13:15:29,705 - INFO - Closing down clientserver connection
2025-05-08 13:15:29,705 - ERROR - Error in stream_live_streaming.py: An error occurred while calling o42.load
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 99, in <module>
    kafka_df = spark.readStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 277, in load
    return self._df(self._jreader.load())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 169, in deco
    return f(*a, **kw)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 334, in get_return_value
    raise Py4JError(
py4j.protocol.Py4JError: An error occurred while calling o42.load
2025-05-08 13:15:29,712 - INFO - Closing down clientserver connection
2025-05-08 13:16:17,125 - INFO - Starting stream_live_streaming.py
2025-05-08 13:16:17,132 - INFO - Attempting to load private key from: /app/config/rsa_key.pem
2025-05-08 13:16:17,138 - INFO - Private key loaded successfully
2025-05-08 13:16:17,141 - INFO - Snowflake configurations loaded
2025-05-08 13:16:17,146 - INFO - AWS configurations loaded
2025-05-08 13:16:30,427 - INFO - Spark session initialized
2025-05-08 13:16:30,428 - INFO - Schema defined
2025-05-08 13:16:51,820 - INFO - Kafka stream initialized
2025-05-08 13:16:55,013 - INFO - JSON parsing completed
2025-05-08 13:17:08,298 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 13:17:09,894 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 13:17:10,077 - INFO - Closing down clientserver connection
2025-05-08 13:17:38,191 - INFO - Starting stream_live_streaming.py
2025-05-08 13:17:38,200 - INFO - Attempting to load private key from: /app/config/rsa_key.pem
2025-05-08 13:17:38,207 - INFO - Private key loaded successfully
2025-05-08 13:17:38,208 - INFO - Snowflake configurations loaded
2025-05-08 13:17:38,272 - INFO - AWS configurations loaded
2025-05-08 13:17:44,730 - INFO - Error while receiving.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty
2025-05-08 13:17:44,732 - INFO - Closing down clientserver connection
2025-05-08 13:17:44,733 - ERROR - Exception while sending command.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
2025-05-08 13:17:44,736 - INFO - Closing down clientserver connection
2025-05-08 13:17:44,737 - ERROR - Error in stream_live_streaming.py: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 72, in <module>
    spark = SparkSession.builder \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/session.py", line 477, in getOrCreate
    sc = SparkContext.getOrCreate(sparkConf)
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/context.py", line 512, in getOrCreate
    SparkContext(conf=conf or SparkConf())
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/context.py", line 200, in __init__
    self._do_init(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/context.py", line 287, in _do_init
    self._jsc = jsc or self._initialize_context(self._conf._jconf)
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/context.py", line 417, in _initialize_context
    return self._jvm.JavaSparkContext(jconf)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1587, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 334, in get_return_value
    raise Py4JError(
py4j.protocol.Py4JError: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext
2025-05-08 13:17:44,742 - INFO - Closing down clientserver connection
2025-05-08 13:18:31,763 - INFO - Starting stream_live_streaming.py
2025-05-08 13:18:31,770 - INFO - Attempting to load private key from: /app/config/rsa_key.pem
2025-05-08 13:18:31,777 - INFO - Private key loaded successfully
2025-05-08 13:18:31,778 - INFO - Snowflake configurations loaded
2025-05-08 13:18:31,785 - INFO - AWS configurations loaded
2025-05-08 13:18:45,322 - INFO - Spark session initialized
2025-05-08 13:18:45,323 - INFO - Schema defined
2025-05-08 13:19:08,701 - INFO - Kafka stream initialized
2025-05-08 13:19:10,910 - INFO - JSON parsing completed
2025-05-08 13:19:22,689 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 13:19:25,230 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 13:19:25,420 - INFO - Closing down clientserver connection
2025-05-08 13:19:48,985 - INFO - Starting stream_live_streaming.py
2025-05-08 13:19:48,993 - INFO - Attempting to load private key from: /app/config/rsa_key.pem
2025-05-08 13:19:49,000 - INFO - Private key loaded successfully
2025-05-08 13:19:49,001 - INFO - Snowflake configurations loaded
2025-05-08 13:19:49,007 - INFO - AWS configurations loaded
2025-05-08 13:20:00,884 - INFO - Spark session initialized
2025-05-08 13:20:00,886 - INFO - Schema defined
2025-05-08 13:20:23,151 - INFO - Kafka stream initialized
2025-05-08 13:20:25,946 - INFO - JSON parsing completed
2025-05-08 13:20:36,119 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 13:20:39,126 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 13:20:39,628 - INFO - Closing down clientserver connection
2025-05-08 13:21:14,956 - INFO - Starting stream_live_streaming.py
2025-05-08 13:21:14,998 - INFO - Attempting to load private key from: /app/config/rsa_key.pem
2025-05-08 13:21:15,005 - INFO - Private key loaded successfully
2025-05-08 13:21:15,005 - INFO - Snowflake configurations loaded
2025-05-08 13:21:15,016 - INFO - AWS configurations loaded
2025-05-08 13:21:30,100 - INFO - Spark session initialized
2025-05-08 13:21:30,105 - INFO - Schema defined
2025-05-08 13:21:53,494 - INFO - Kafka stream initialized
2025-05-08 13:21:56,002 - INFO - JSON parsing completed
2025-05-08 13:22:06,556 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 13:22:08,960 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 13:22:09,074 - INFO - Closing down clientserver connection
2025-05-08 13:22:37,367 - INFO - Starting stream_live_streaming.py
2025-05-08 13:22:37,376 - INFO - Attempting to load private key from: /app/config/rsa_key.pem
2025-05-08 13:22:37,425 - INFO - Private key loaded successfully
2025-05-08 13:22:37,427 - INFO - Snowflake configurations loaded
2025-05-08 13:22:37,437 - INFO - AWS configurations loaded
2025-05-08 13:22:49,547 - INFO - Spark session initialized
2025-05-08 13:22:49,548 - INFO - Schema defined
2025-05-08 13:23:11,527 - INFO - Kafka stream initialized
2025-05-08 13:23:14,214 - INFO - JSON parsing completed
2025-05-08 13:23:24,416 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 13:23:27,002 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 13:23:27,218 - INFO - Closing down clientserver connection
2025-05-08 13:23:53,297 - INFO - Starting stream_live_streaming.py
2025-05-08 13:23:53,305 - INFO - Attempting to load private key from: /app/config/rsa_key.pem
2025-05-08 13:23:53,388 - INFO - Private key loaded successfully
2025-05-08 13:23:53,389 - INFO - Snowflake configurations loaded
2025-05-08 13:23:53,394 - INFO - AWS configurations loaded
2025-05-08 13:24:04,458 - INFO - Spark session initialized
2025-05-08 13:24:04,460 - INFO - Schema defined
2025-05-08 13:24:27,595 - INFO - Kafka stream initialized
2025-05-08 13:24:29,248 - INFO - JSON parsing completed
2025-05-08 13:24:40,243 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 13:24:42,436 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 13:24:42,569 - INFO - Closing down clientserver connection
2025-05-08 13:25:09,413 - INFO - Starting stream_live_streaming.py
2025-05-08 13:25:09,420 - INFO - Attempting to load private key from: /app/config/rsa_key.pem
2025-05-08 13:25:09,428 - INFO - Private key loaded successfully
2025-05-08 13:25:09,490 - INFO - Snowflake configurations loaded
2025-05-08 13:25:09,503 - INFO - AWS configurations loaded
2025-05-08 13:25:23,414 - INFO - Spark session initialized
2025-05-08 13:25:23,415 - INFO - Schema defined
2025-05-08 13:25:45,874 - INFO - Kafka stream initialized
2025-05-08 13:25:47,657 - INFO - JSON parsing completed
2025-05-08 13:25:56,950 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 13:25:58,750 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 13:25:58,970 - INFO - Closing down clientserver connection
2025-05-08 13:26:24,471 - INFO - Starting stream_live_streaming.py
2025-05-08 13:26:24,478 - INFO - Attempting to load private key from: /app/config/rsa_key.pem
2025-05-08 13:26:24,487 - INFO - Private key loaded successfully
2025-05-08 13:26:24,488 - INFO - Snowflake configurations loaded
2025-05-08 13:26:24,495 - INFO - AWS configurations loaded
2025-05-08 13:26:36,723 - INFO - Spark session initialized
2025-05-08 13:26:36,724 - INFO - Schema defined
2025-05-08 13:26:38,277 - INFO - Error while receiving.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty
2025-05-08 13:26:38,280 - INFO - Closing down clientserver connection
2025-05-08 13:26:38,283 - ERROR - Exception while sending command.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 516, in send_command
    raise Py4JNetworkError("Answer from Java side is empty")
py4j.protocol.Py4JNetworkError: Answer from Java side is empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
2025-05-08 13:26:38,322 - INFO - Closing down clientserver connection
2025-05-08 13:26:38,323 - ERROR - Error in stream_live_streaming.py: An error occurred while calling o36.readStream
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 99, in <module>
    kafka_df = spark.readStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/session.py", line 1549, in readStream
    return DataStreamReader(self)
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 68, in __init__
    self._jreader = spark._jsparkSession.readStream()
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 169, in deco
    return f(*a, **kw)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 334, in get_return_value
    raise Py4JError(
py4j.protocol.Py4JError: An error occurred while calling o36.readStream
2025-05-08 13:26:38,331 - INFO - Closing down clientserver connection
2025-05-08 13:27:41,616 - INFO - Starting stream_live_streaming.py
2025-05-08 13:27:41,623 - INFO - Attempting to load private key from: /app/config/rsa_key.pem
2025-05-08 13:27:41,668 - INFO - Private key loaded successfully
2025-05-08 13:27:41,669 - INFO - Snowflake configurations loaded
2025-05-08 13:27:41,676 - INFO - AWS configurations loaded
2025-05-08 13:27:54,348 - INFO - Spark session initialized
2025-05-08 13:27:54,349 - INFO - Schema defined
2025-05-08 13:28:17,550 - INFO - Kafka stream initialized
2025-05-08 13:28:20,554 - INFO - JSON parsing completed
2025-05-08 13:28:31,996 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 13:28:34,610 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 13:28:34,905 - INFO - Closing down clientserver connection
2025-05-08 13:31:10,560 - INFO - Starting stream_live_streaming.py
2025-05-08 13:31:10,565 - INFO - Attempting to load private key from: /app/config/rsa_key.pem
2025-05-08 13:31:10,571 - INFO - Private key loaded successfully
2025-05-08 13:31:10,572 - INFO - Snowflake configurations loaded
2025-05-08 13:31:10,577 - INFO - AWS configurations loaded
2025-05-08 13:31:23,632 - INFO - Spark session initialized
2025-05-08 13:31:23,714 - INFO - Schema defined
2025-05-08 13:31:45,130 - INFO - Kafka stream initialized
2025-05-08 13:31:47,204 - INFO - JSON parsing completed
2025-05-08 13:31:59,255 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 13:32:01,477 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 13:32:01,748 - INFO - Closing down clientserver connection
2025-05-08 13:36:16,437 - INFO - Starting stream_live_streaming.py
2025-05-08 13:36:16,444 - INFO - Attempting to load private key from: /app/config/rsa_key.pem
2025-05-08 13:36:16,451 - INFO - Private key loaded successfully
2025-05-08 13:36:16,452 - INFO - Snowflake configurations loaded
2025-05-08 13:36:16,459 - INFO - AWS configurations loaded
2025-05-08 13:36:30,339 - INFO - Spark session initialized
2025-05-08 13:36:30,340 - INFO - Schema defined
2025-05-08 13:36:51,315 - INFO - Kafka stream initialized
2025-05-08 13:36:53,776 - INFO - JSON parsing completed
2025-05-08 13:37:06,770 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 13:37:08,985 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 13:37:09,176 - INFO - Closing down clientserver connection
2025-05-08 13:42:35,244 - INFO - Starting stream_live_streaming.py
2025-05-08 13:42:35,250 - INFO - Attempting to load private key from: /app/config/rsa_key.pem
2025-05-08 13:42:35,256 - INFO - Private key loaded successfully
2025-05-08 13:42:35,256 - INFO - Snowflake configurations loaded
2025-05-08 13:42:35,263 - INFO - AWS configurations loaded
2025-05-08 13:42:48,660 - INFO - Spark session initialized
2025-05-08 13:42:48,661 - INFO - Schema defined
2025-05-08 13:43:12,005 - INFO - Kafka stream initialized
2025-05-08 13:43:15,179 - INFO - JSON parsing completed
2025-05-08 13:43:27,569 - INFO - Writing DataFrame to Snowflake table DIM_TIME
2025-05-08 13:43:29,377 - ERROR - Error in stream_live_streaming.py: Input PEM private key is invalid
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 301, in <module>
    write_to_snowflake(time_df, "DIM_TIME")
  File "/app/scripts/stream_live_streaming.py", line 292, in write_to_snowflake
    dataframe.writeStream \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/readwriter.py", line 1385, in start
    return self._sq(self._jwrite.start())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Input PEM private key is invalid
2025-05-08 13:43:29,509 - INFO - Closing down clientserver connection
2025-05-08 13:47:21,248 - INFO - Starting stream_live_streaming.py
2025-05-08 13:47:21,256 - INFO - Attempting to load private key from: /app/config/rsa_key.der
2025-05-08 13:47:21,264 - ERROR - Failed to read private key file: 'utf-8' codec can't decode byte 0x82 in position 1: invalid start byte
2025-05-08 13:47:21,265 - ERROR - Error in stream_live_streaming.py: 'utf-8' codec can't decode byte 0x82 in position 1: invalid start byte
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 43, in <module>
    private_key = key_file.read()
  File "/opt/bitnami/python/lib/python3.9/codecs.py", line 322, in decode
    (result, consumed) = self._buffer_decode(data, self.errors, final)
UnicodeDecodeError: 'utf-8' codec can't decode byte 0x82 in position 1: invalid start byte
2025-05-08 14:18:32,832 - INFO - Starting stream_live_streaming.py
2025-05-08 14:18:32,841 - INFO - Attempting to load private key from: /app/config/rsa_key.der
2025-05-08 14:18:32,850 - ERROR - Failed to read private key file: 'utf-8' codec can't decode byte 0x82 in position 1: invalid start byte
2025-05-08 14:18:32,915 - ERROR - Error in stream_live_streaming.py: 'utf-8' codec can't decode byte 0x82 in position 1: invalid start byte
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 43, in <module>
    private_key = key_file.read()
  File "/opt/bitnami/python/lib/python3.9/codecs.py", line 322, in decode
    (result, consumed) = self._buffer_decode(data, self.errors, final)
UnicodeDecodeError: 'utf-8' codec can't decode byte 0x82 in position 1: invalid start byte
2025-05-08 14:43:41,005 - INFO - Starting stream_live_streaming.py
2025-05-08 14:43:41,012 - INFO - AWS configurations loaded
2025-05-08 14:43:45,122 - INFO - Spark session initialized
2025-05-08 14:43:45,124 - INFO - Schema defined
2025-05-08 14:43:50,982 - INFO - Kafka stream initialized
2025-05-08 14:43:51,589 - INFO - JSON parsing completed
2025-05-08 14:43:52,467 - INFO - Writing cleaned data to s3a://datastreaming-analytics-1/staging/live_streaming
2025-05-08 14:55:48,086 - INFO - Starting stream_live_streaming.py
2025-05-08 14:55:48,172 - INFO - AWS configurations loaded
2025-05-08 14:56:07,670 - INFO - Spark session initialized
2025-05-08 14:56:07,671 - INFO - Schema defined
2025-05-08 14:56:41,738 - INFO - Kafka stream initialized
2025-05-08 14:56:45,535 - INFO - JSON parsing completed
2025-05-08 14:56:51,654 - INFO - Writing cleaned data to s3a://datastreaming-analytics-1/staging/live_streaming
2025-05-08 18:59:26,773 - INFO - Starting stream_live_streaming.py
2025-05-08 18:59:26,798 - INFO - AWS configurations loaded
2025-05-08 18:59:31,089 - INFO - Spark session initialized
2025-05-08 18:59:31,091 - INFO - Schema defined
2025-05-08 18:59:37,334 - INFO - Kafka stream initialized
2025-05-08 18:59:37,923 - INFO - JSON parsing completed
2025-05-08 18:59:38,902 - INFO - Writing cleaned data to s3a://datastreaming-analytics-1/staging/live_streaming
2025-05-08 19:00:10,982 - INFO - Error while receiving.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
RuntimeError: reentrant call inside <_io.BufferedReader name=4>
2025-05-08 19:00:10,985 - INFO - Closing down clientserver connection
2025-05-08 19:00:10,986 - ERROR - Exception while sending command.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
RuntimeError: reentrant call inside <_io.BufferedReader name=4>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
2025-05-08 19:00:10,990 - INFO - Error while receiving.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "/opt/bitnami/python/lib/python3.9/socket.py", line 704, in readinto
    return self._sock.recv_into(b)
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/context.py", line 377, in signal_handler
    self.cancelAllJobs()
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/context.py", line 2255, in cancelAllJobs
    self._jsc.sc().cancelAllJobs()
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 169, in deco
    return f(*a, **kw)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 334, in get_return_value
    raise Py4JError(
py4j.protocol.Py4JError: An error occurred while calling o23.sc
2025-05-08 19:00:11,001 - INFO - Closing down clientserver connection
2025-05-08 19:00:11,002 - ERROR - Exception while sending command.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "/opt/bitnami/python/lib/python3.9/socket.py", line 704, in readinto
    return self._sock.recv_into(b)
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/context.py", line 377, in signal_handler
    self.cancelAllJobs()
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/context.py", line 2255, in cancelAllJobs
    self._jsc.sc().cancelAllJobs()
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 169, in deco
    return f(*a, **kw)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 334, in get_return_value
    raise Py4JError(
py4j.protocol.Py4JError: An error occurred while calling o23.sc

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
2025-05-08 19:00:11,003 - INFO - Closing down clientserver connection
2025-05-08 19:00:11,004 - ERROR - Error in stream_live_streaming.py: An error occurred while calling o185.awaitTermination
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 156, in <module>
    query.awaitTermination()
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/query.py", line 201, in awaitTermination
    return self._jsq.awaitTermination()
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 169, in deco
    return f(*a, **kw)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 334, in get_return_value
    raise Py4JError(
py4j.protocol.Py4JError: An error occurred while calling o185.awaitTermination
2025-05-08 20:00:32,827 - INFO - Starting stream_live_streaming.py
2025-05-08 20:00:32,833 - INFO - AWS configurations loaded
2025-05-08 20:00:48,732 - INFO - Spark session initialized
2025-05-08 20:00:48,738 - INFO - Schema defined
2025-05-08 20:01:13,258 - INFO - Kafka stream initialized
2025-05-08 20:01:17,067 - INFO - JSON parsing completed
2025-05-08 20:01:23,167 - INFO - Writing cleaned data to s3a://datastreaming-analytics-1/staging/live_streaming
2025-05-08 20:01:49,318 - ERROR - Error in stream_live_streaming.py: [STREAM_FAILED] Query [id = e2b2ed6a-b6bd-4214-85ab-4d3d5ba4141c, runId = 2d29c8c3-bb47-45d2-acc3-64ddeb76a808] terminated with exception: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 156, in <module>
    query.awaitTermination()
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/query.py", line 201, in awaitTermination
    return self._jsq.awaitTermination()
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.StreamingQueryException: [STREAM_FAILED] Query [id = e2b2ed6a-b6bd-4214-85ab-4d3d5ba4141c, runId = 2d29c8c3-bb47-45d2-acc3-64ddeb76a808] terminated with exception: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
2025-05-08 20:01:50,346 - INFO - Spark session stopped
2025-05-08 20:01:50,441 - INFO - Closing down clientserver connection
2025-05-08 20:02:54,861 - INFO - Starting stream_live_streaming.py
2025-05-08 20:02:54,869 - INFO - AWS configurations loaded
2025-05-08 20:03:03,153 - INFO - Spark session initialized
2025-05-08 20:03:03,154 - INFO - Schema defined
2025-05-08 20:03:14,249 - INFO - Kafka stream initialized
2025-05-08 20:03:16,329 - INFO - JSON parsing completed
2025-05-08 20:03:20,336 - INFO - Writing cleaned data to s3a://datastreaming-analytics-1/staging/live_streaming
2025-05-08 20:07:27,841 - INFO - Error while receiving.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
RuntimeError: reentrant call inside <_io.BufferedReader name=4>
2025-05-08 20:07:27,845 - INFO - Closing down clientserver connection
2025-05-08 20:07:27,846 - ERROR - Exception while sending command.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
RuntimeError: reentrant call inside <_io.BufferedReader name=4>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
2025-05-08 20:07:27,850 - INFO - Error while receiving.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "/opt/bitnami/python/lib/python3.9/socket.py", line 704, in readinto
    return self._sock.recv_into(b)
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/context.py", line 377, in signal_handler
    self.cancelAllJobs()
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/context.py", line 2255, in cancelAllJobs
    self._jsc.sc().cancelAllJobs()
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 169, in deco
    return f(*a, **kw)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 334, in get_return_value
    raise Py4JError(
py4j.protocol.Py4JError: An error occurred while calling o23.sc
2025-05-08 20:07:27,854 - INFO - Closing down clientserver connection
2025-05-08 20:07:27,854 - ERROR - Exception while sending command.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "/opt/bitnami/python/lib/python3.9/socket.py", line 704, in readinto
    return self._sock.recv_into(b)
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/context.py", line 377, in signal_handler
    self.cancelAllJobs()
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/context.py", line 2255, in cancelAllJobs
    self._jsc.sc().cancelAllJobs()
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 169, in deco
    return f(*a, **kw)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 334, in get_return_value
    raise Py4JError(
py4j.protocol.Py4JError: An error occurred while calling o23.sc

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
2025-05-08 20:07:27,855 - INFO - Closing down clientserver connection
2025-05-08 20:07:27,856 - ERROR - Error in stream_live_streaming.py: An error occurred while calling o185.awaitTermination
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 156, in <module>
    query.awaitTermination()
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/query.py", line 201, in awaitTermination
    return self._jsq.awaitTermination()
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 169, in deco
    return f(*a, **kw)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 334, in get_return_value
    raise Py4JError(
py4j.protocol.Py4JError: An error occurred while calling o185.awaitTermination
2025-05-08 20:14:10,240 - INFO - Starting stream_live_streaming.py
2025-05-08 20:14:10,246 - INFO - AWS configurations loaded
2025-05-08 20:14:13,184 - INFO - Spark session initialized
2025-05-08 20:14:13,185 - INFO - Schema defined
2025-05-08 20:14:18,890 - INFO - Kafka stream initialized
2025-05-08 20:14:19,680 - INFO - JSON parsing completed
2025-05-08 20:14:20,640 - INFO - Writing cleaned data to s3a://datastreaming-analytics-1/staging/live_streaming
2025-05-08 20:14:24,146 - INFO - Streaming query started: None, ID: e2b2ed6a-b6bd-4214-85ab-4d3d5ba4141c
2025-05-08 20:42:18,355 - INFO - Error while receiving.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
RuntimeError: reentrant call inside <_io.BufferedReader name=4>
2025-05-08 20:42:18,359 - INFO - Closing down clientserver connection
2025-05-08 20:42:18,360 - ERROR - Exception while sending command.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
RuntimeError: reentrant call inside <_io.BufferedReader name=4>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
2025-05-08 20:42:18,363 - INFO - Error while receiving.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "/opt/bitnami/python/lib/python3.9/socket.py", line 704, in readinto
    return self._sock.recv_into(b)
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/context.py", line 377, in signal_handler
    self.cancelAllJobs()
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/context.py", line 2255, in cancelAllJobs
    self._jsc.sc().cancelAllJobs()
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 169, in deco
    return f(*a, **kw)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 334, in get_return_value
    raise Py4JError(
py4j.protocol.Py4JError: An error occurred while calling o25.sc
2025-05-08 20:42:18,366 - INFO - Closing down clientserver connection
2025-05-08 20:42:18,367 - ERROR - Exception while sending command.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "/opt/bitnami/python/lib/python3.9/socket.py", line 704, in readinto
    return self._sock.recv_into(b)
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/context.py", line 377, in signal_handler
    self.cancelAllJobs()
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/context.py", line 2255, in cancelAllJobs
    self._jsc.sc().cancelAllJobs()
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 169, in deco
    return f(*a, **kw)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 334, in get_return_value
    raise Py4JError(
py4j.protocol.Py4JError: An error occurred while calling o25.sc

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
2025-05-08 20:42:18,368 - INFO - Closing down clientserver connection
2025-05-08 20:42:18,369 - ERROR - Error in stream_live_streaming.py: An error occurred while calling o177.awaitTermination
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 162, in <module>
    query.awaitTermination()
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/query.py", line 201, in awaitTermination
    return self._jsq.awaitTermination()
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 169, in deco
    return f(*a, **kw)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 334, in get_return_value
    raise Py4JError(
py4j.protocol.Py4JError: An error occurred while calling o177.awaitTermination
2025-05-08 20:42:18,604 - INFO - Spark session stopped
2025-05-08 20:42:18,617 - INFO - Closing down clientserver connection
2025-05-09 01:04:38,143 - INFO - Starting stream_live_streaming.py
2025-05-09 01:04:38,149 - INFO - AWS configurations loaded
2025-05-09 01:04:46,961 - INFO - Spark session initialized
2025-05-09 01:04:46,962 - INFO - Schema defined
2025-05-09 01:04:59,318 - INFO - Kafka stream initialized
2025-05-09 01:05:01,623 - INFO - JSON parsing completed
2025-05-09 01:05:04,703 - INFO - Writing cleaned data to s3a://datastreaming-analytics-1/staging/live_streaming
2025-05-09 01:05:13,922 - INFO - Streaming query started: None, ID: e2b2ed6a-b6bd-4214-85ab-4d3d5ba4141c
2025-05-09 01:05:24,250 - ERROR - Error in stream_live_streaming.py: [STREAM_FAILED] Query [id = e2b2ed6a-b6bd-4214-85ab-4d3d5ba4141c, runId = 6937c85c-de56-4184-84a5-ff0b2fa08ee4] terminated with exception: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 162, in <module>
    query.awaitTermination()
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/query.py", line 201, in awaitTermination
    return self._jsq.awaitTermination()
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.StreamingQueryException: [STREAM_FAILED] Query [id = e2b2ed6a-b6bd-4214-85ab-4d3d5ba4141c, runId = 6937c85c-de56-4184-84a5-ff0b2fa08ee4] terminated with exception: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
2025-05-09 01:05:25,027 - INFO - Spark session stopped
2025-05-09 01:05:25,055 - INFO - Closing down clientserver connection
2025-05-09 13:46:41,536 - INFO - Starting stream_live_streaming.py
2025-05-09 13:46:41,571 - INFO - AWS configurations loaded
2025-05-09 13:46:46,709 - INFO - Spark session initialized
2025-05-09 13:46:46,710 - INFO - Schema defined
2025-05-09 13:46:56,738 - INFO - Kafka stream initialized
2025-05-09 13:46:57,624 - INFO - JSON parsing completed
2025-05-09 13:46:59,108 - INFO - Writing cleaned data to s3a://datastreaming-analytics-1/staging/live_streaming
2025-05-09 13:47:01,237 - INFO - Streaming query started: None, ID: e2b2ed6a-b6bd-4214-85ab-4d3d5ba4141c
2025-05-09 13:47:31,225 - ERROR - Error in stream_live_streaming.py: [STREAM_FAILED] Query [id = e2b2ed6a-b6bd-4214-85ab-4d3d5ba4141c, runId = f0ed8470-f9d5-431b-bd6a-cba5fa7c4003] terminated with exception: Unable to find batch s3a://datastreaming-analytics-1/staging/live_streaming/_spark_metadata/9.compact.
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 162, in <module>
    query.awaitTermination()
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/query.py", line 201, in awaitTermination
    return self._jsq.awaitTermination()
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.StreamingQueryException: [STREAM_FAILED] Query [id = e2b2ed6a-b6bd-4214-85ab-4d3d5ba4141c, runId = f0ed8470-f9d5-431b-bd6a-cba5fa7c4003] terminated with exception: Unable to find batch s3a://datastreaming-analytics-1/staging/live_streaming/_spark_metadata/9.compact.
2025-05-09 13:47:32,013 - INFO - Spark session stopped
2025-05-09 13:47:32,042 - INFO - Closing down clientserver connection
2025-05-09 14:10:43,844 - INFO - Starting stream_live_streaming.py
2025-05-09 14:10:43,855 - INFO - AWS configurations loaded
2025-05-09 14:10:54,653 - INFO - Spark session initialized
2025-05-09 14:10:54,655 - INFO - Schema defined
2025-05-09 14:11:09,181 - INFO - Kafka stream initialized
2025-05-09 14:11:11,053 - INFO - JSON parsing completed
2025-05-09 14:11:14,458 - INFO - Writing cleaned data to s3a://datastreaming-analytics-1/staging/live_streaming
2025-05-09 14:11:31,834 - INFO - Streaming query started: None, ID: e2b2ed6a-b6bd-4214-85ab-4d3d5ba4141c
2025-05-09 14:12:11,393 - INFO - Starting stream_live_streaming.py
2025-05-09 14:12:11,400 - INFO - AWS configurations loaded
2025-05-09 14:12:17,365 - ERROR - Error in stream_live_streaming.py: [STREAM_FAILED] Query [id = e2b2ed6a-b6bd-4214-85ab-4d3d5ba4141c, runId = 30174106-1653-4323-9e04-a4fc4be22bfc] terminated with exception: Unable to find batch s3a://datastreaming-analytics-1/staging/live_streaming/_spark_metadata/9.compact.
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 162, in <module>
    query.awaitTermination()
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/query.py", line 201, in awaitTermination
    return self._jsq.awaitTermination()
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.StreamingQueryException: [STREAM_FAILED] Query [id = e2b2ed6a-b6bd-4214-85ab-4d3d5ba4141c, runId = 30174106-1653-4323-9e04-a4fc4be22bfc] terminated with exception: Unable to find batch s3a://datastreaming-analytics-1/staging/live_streaming/_spark_metadata/9.compact.
2025-05-09 14:12:17,761 - INFO - Spark session stopped
2025-05-09 14:12:17,780 - INFO - Closing down clientserver connection
2025-05-09 14:12:20,391 - INFO - Spark session initialized
2025-05-09 14:12:20,392 - INFO - Schema defined
2025-05-09 14:12:27,556 - INFO - Kafka stream initialized
2025-05-09 14:12:28,200 - INFO - JSON parsing completed
2025-05-09 14:12:29,496 - INFO - Writing cleaned data to s3a://datastreaming-analytics-1/staging/live_streaming
2025-05-09 14:12:31,145 - INFO - Streaming query started: None, ID: e2b2ed6a-b6bd-4214-85ab-4d3d5ba4141c
2025-05-09 14:12:45,681 - ERROR - Error in stream_live_streaming.py: [STREAM_FAILED] Query [id = e2b2ed6a-b6bd-4214-85ab-4d3d5ba4141c, runId = 0e19caa4-4626-4367-8e93-b1ff0b340c92] terminated with exception: Unable to find batch s3a://datastreaming-analytics-1/staging/live_streaming/_spark_metadata/9.compact.
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 162, in <module>
    query.awaitTermination()
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/query.py", line 201, in awaitTermination
    return self._jsq.awaitTermination()
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.StreamingQueryException: [STREAM_FAILED] Query [id = e2b2ed6a-b6bd-4214-85ab-4d3d5ba4141c, runId = 0e19caa4-4626-4367-8e93-b1ff0b340c92] terminated with exception: Unable to find batch s3a://datastreaming-analytics-1/staging/live_streaming/_spark_metadata/9.compact.
2025-05-09 14:12:46,338 - INFO - Spark session stopped
2025-05-09 14:12:46,359 - INFO - Closing down clientserver connection
2025-05-09 14:50:08,884 - INFO - Starting stream_live_streaming.py
2025-05-09 14:50:08,892 - INFO - AWS configurations loaded
2025-05-09 14:50:12,570 - INFO - Spark session initialized
2025-05-09 14:50:12,571 - ERROR - Error in stream_live_streaming.py: name 'DaytonType' is not defined
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 57, in <module>
    DaytonType(),
NameError: name 'DaytonType' is not defined
2025-05-09 14:50:13,187 - INFO - Spark session stopped
2025-05-09 14:50:13,197 - INFO - Closing down clientserver connection
2025-05-09 14:50:39,856 - INFO - Starting stream_live_streaming.py
2025-05-09 14:50:39,865 - INFO - AWS configurations loaded
2025-05-09 14:50:43,062 - INFO - Spark session initialized
2025-05-09 14:50:43,064 - ERROR - Error in stream_live_streaming.py: name 'DaytonType' is not defined
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 57, in <module>
    DaytonType(),
NameError: name 'DaytonType' is not defined
2025-05-09 14:50:40,948 - INFO - Spark session stopped
2025-05-09 14:50:40,958 - INFO - Closing down clientserver connection
2025-05-09 14:51:35,905 - INFO - Starting stream_live_streaming.py
2025-05-09 14:51:35,914 - INFO - AWS configurations loaded
2025-05-09 14:51:39,240 - INFO - Spark session initialized
2025-05-09 14:51:39,241 - ERROR - Error in stream_live_streaming.py: name 'DaytonType' is not defined
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 57, in <module>
    DaytonType(),
NameError: name 'DaytonType' is not defined
2025-05-09 14:51:39,850 - INFO - Spark session stopped
2025-05-09 14:51:39,858 - INFO - Closing down clientserver connection
2025-05-09 14:54:12,684 - INFO - Starting stream_live_streaming.py
2025-05-09 14:54:12,691 - INFO - AWS configurations loaded
2025-05-09 14:54:16,062 - INFO - Spark session initialized
2025-05-09 14:54:16,063 - ERROR - Error in stream_live_streaming.py: name 'DaytonType' is not defined
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 57, in <module>
    DaytonType(),
NameError: name 'DaytonType' is not defined
2025-05-09 14:54:16,229 - INFO - Spark session stopped
2025-05-09 14:54:16,237 - INFO - Closing down clientserver connection
2025-05-09 15:01:23,787 - INFO - Starting stream_live_streaming.py
2025-05-09 15:01:23,795 - INFO - AWS configurations loaded
2025-05-09 15:01:32,261 - INFO - Spark session initialized
2025-05-09 15:01:32,262 - ERROR - Error in stream_live_streaming.py: name 'DaytonType' is not defined
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 57, in <module>
    DaytonType(),
NameError: name 'DaytonType' is not defined
2025-05-09 15:01:33,161 - INFO - Spark session stopped
2025-05-09 15:01:33,171 - INFO - Closing down clientserver connection
2025-05-09 15:01:43,621 - INFO - Starting stream_live_streaming.py
2025-05-09 15:01:43,627 - INFO - AWS configurations loaded
2025-05-09 15:01:52,448 - INFO - Spark session initialized
2025-05-09 15:01:52,449 - ERROR - Error in stream_live_streaming.py: name 'DaytonType' is not defined
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 57, in <module>
    DaytonType(),
NameError: name 'DaytonType' is not defined
2025-05-09 15:01:53,128 - INFO - Spark session stopped
2025-05-09 15:01:53,159 - INFO - Closing down clientserver connection
2025-05-09 15:02:50,985 - INFO - Starting stream_live_streaming.py
2025-05-09 15:02:50,992 - INFO - AWS configurations loaded
2025-05-09 15:02:54,208 - INFO - Spark session initialized
2025-05-09 15:02:54,209 - INFO - Schema defined
2025-05-09 15:02:59,930 - INFO - Kafka stream initialized
2025-05-09 15:02:57,746 - INFO - JSON parsing completed
2025-05-09 15:02:58,633 - INFO - Writing cleaned data to s3a://datastreaming-analytics-1/staging/live_streaming
2025-05-09 15:03:02,388 - INFO - Streaming query started: None, ID: 298f5372-a6ce-407a-986d-155848b07a34
2025-05-09 15:03:07,773 - ERROR - Error in stream_live_streaming.py: [STREAM_FAILED] Query [id = 298f5372-a6ce-407a-986d-155848b07a34, runId = 0d282ac0-3dab-4fdd-a856-b17564c4d7d5] terminated with exception: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 162, in <module>
    query.awaitTermination()
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/query.py", line 201, in awaitTermination
    return self._jsq.awaitTermination()
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.StreamingQueryException: [STREAM_FAILED] Query [id = 298f5372-a6ce-407a-986d-155848b07a34, runId = 0d282ac0-3dab-4fdd-a856-b17564c4d7d5] terminated with exception: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
2025-05-09 15:03:08,206 - INFO - Spark session stopped
2025-05-09 15:03:08,227 - INFO - Closing down clientserver connection
2025-05-09 15:07:32,775 - INFO - Starting stream_live_streaming.py
2025-05-09 15:07:32,782 - INFO - AWS configurations loaded
2025-05-09 15:07:33,309 - INFO - Spark session initialized
2025-05-09 15:07:33,311 - INFO - Schema defined
2025-05-09 15:07:38,774 - INFO - Kafka stream initialized
2025-05-09 15:07:39,372 - INFO - JSON parsing completed
2025-05-09 15:07:40,282 - INFO - Writing cleaned data to s3a://datastreaming-analytics-1/staging/live_streaming
2025-05-09 15:07:43,972 - INFO - Streaming query started: None, ID: 298f5372-a6ce-407a-986d-155848b07a34
2025-05-09 15:44:41,372 - INFO - Error while receiving.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
RuntimeError: reentrant call inside <_io.BufferedReader name=4>
2025-05-09 15:44:41,376 - INFO - Closing down clientserver connection
2025-05-09 15:44:41,377 - ERROR - Exception while sending command.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
RuntimeError: reentrant call inside <_io.BufferedReader name=4>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
2025-05-09 15:44:41,383 - INFO - Error while receiving.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "/opt/bitnami/python/lib/python3.9/socket.py", line 704, in readinto
    return self._sock.recv_into(b)
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/context.py", line 377, in signal_handler
    self.cancelAllJobs()
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/context.py", line 2255, in cancelAllJobs
    self._jsc.sc().cancelAllJobs()
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 169, in deco
    return f(*a, **kw)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 334, in get_return_value
    raise Py4JError(
py4j.protocol.Py4JError: An error occurred while calling o25.sc
2025-05-09 15:44:41,388 - INFO - Closing down clientserver connection
2025-05-09 15:44:41,389 - ERROR - Exception while sending command.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "/opt/bitnami/python/lib/python3.9/socket.py", line 704, in readinto
    return self._sock.recv_into(b)
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/context.py", line 377, in signal_handler
    self.cancelAllJobs()
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/context.py", line 2255, in cancelAllJobs
    self._jsc.sc().cancelAllJobs()
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 169, in deco
    return f(*a, **kw)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 334, in get_return_value
    raise Py4JError(
py4j.protocol.Py4JError: An error occurred while calling o25.sc

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
2025-05-09 15:44:41,391 - INFO - Closing down clientserver connection
2025-05-09 15:44:41,392 - ERROR - Error in stream_live_streaming.py: An error occurred while calling o177.awaitTermination
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 162, in <module>
    query.awaitTermination()
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/query.py", line 201, in awaitTermination
    return self._jsq.awaitTermination()
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 169, in deco
    return f(*a, **kw)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 334, in get_return_value
    raise Py4JError(
py4j.protocol.Py4JError: An error occurred while calling o177.awaitTermination
2025-05-09 15:44:41,560 - INFO - Spark session stopped
2025-05-09 15:44:41,580 - INFO - Closing down clientserver connection
2025-05-09 15:56:16,612 - INFO - Starting stream_live_streaming.py
2025-05-09 15:56:16,645 - INFO - AWS configurations loaded
2025-05-09 15:56:29,236 - INFO - Spark session initialized
2025-05-09 15:56:29,240 - INFO - Schema defined
2025-05-09 15:56:44,574 - INFO - Kafka stream initialized
2025-05-09 15:56:46,787 - INFO - JSON parsing completed
2025-05-09 15:56:50,992 - INFO - Writing cleaned data to s3a://datastreaming-analytics-1/staging/live_streaming
2025-05-09 15:57:04,173 - INFO - Streaming query started: None, ID: 298f5372-a6ce-407a-986d-155848b07a34
2025-05-09 15:57:13,257 - ERROR - Error in stream_live_streaming.py: [STREAM_FAILED] Query [id = 298f5372-a6ce-407a-986d-155848b07a34, runId = b2c6e604-e1a5-48a2-a989-216c1a3c3738] terminated with exception: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
Traceback (most recent call last):
  File "/app/scripts/stream_live_streaming.py", line 162, in <module>
    query.awaitTermination()
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/query.py", line 201, in awaitTermination
    return self._jsq.awaitTermination()
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.StreamingQueryException: [STREAM_FAILED] Query [id = 298f5372-a6ce-407a-986d-155848b07a34, runId = b2c6e604-e1a5-48a2-a989-216c1a3c3738] terminated with exception: org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition.
2025-05-09 15:57:14,049 - INFO - Spark session stopped
2025-05-09 15:57:14,096 - INFO - Closing down clientserver connection

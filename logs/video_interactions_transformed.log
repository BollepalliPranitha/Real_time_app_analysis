2025-05-07 04:01:43,648 - INFO - Initializing Spark session
2025-05-07 04:01:48,253 - INFO - Snowflake connection options: {'sfURL': 'uxctays-kqc58183.snowflakecomputing.com', 'sfUser': 'Pranitha22', 'sfPassword': 'Pradyumna@9999', 'sfDatabase': 'ANALYTICS', 'sfSchema': 'PUBLIC', 'sfWarehouse': 'COMPUTE_WHI', 'sfRole': 'ACCOUNTADMIN'}
2025-05-07 04:01:48,257 - INFO - Reading staging data from s3a://datastreaming-analytics-1/staging/video_interactions
2025-05-07 04:01:49,958 - WARNING - Hadoop check failed for S3 path s3a://datastreaming-analytics-1/staging/video_interactions: Expected authority at index 6: s3a://
Traceback (most recent call last):
  File "/app/scripts/video_interactions_transformed.py", line 73, in check_s3_path
    spark._jvm.java.net.URI.create(s3_path.split("://")[0] + "://"),
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 175, in deco
    raise converted from None
pyspark.errors.exceptions.captured.IllegalArgumentException: Expected authority at index 6: s3a://

2025-05-07 04:01:51,018 - INFO - AWS SDK check for s3a://datastreaming-analytics-1/staging/video_interactions: file count: 2, files: [('staging/video_interactions/_SUCCESS', 0), ('staging/video_interactions/part-00000-86ce8a08-8a1f-4d6b-8111-b315bed37390-c000.snappy.parquet', 144894)]
2025-05-07 04:01:58,512 - INFO - Checking duplicates in video_df
2025-05-07 04:02:04,706 - ERROR - Unexpected error in video_interactions_transformed.py: An error occurred while calling o73.count.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (8b973f15099c executor driver): org.apache.spark.SparkException: Parquet column cannot be converted in file s3a://datastreaming-analytics-1/staging/video_interactions/part-00000-86ce8a08-8a1f-4d6b-8111-b315bed37390-c000.snappy.parquet. Column: [Engagement], Expected: float, Found: INT64.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:868)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:298)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:125)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:594)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)
	at org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)
	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)
	at org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1535)
	at org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1462)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1526)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1349)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:375)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:326)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:139)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException: column: [Engagement], physicalType: INT64, logicalType: float
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1127)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:189)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:175)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:328)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:219)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:125)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:294)
	... 42 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
Caused by: org.apache.spark.SparkException: Parquet column cannot be converted in file s3a://datastreaming-analytics-1/staging/video_interactions/part-00000-86ce8a08-8a1f-4d6b-8111-b315bed37390-c000.snappy.parquet. Column: [Engagement], Expected: float, Found: INT64.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:868)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:298)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:125)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:594)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)
	at org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)
	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)
	at org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1535)
	at org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1462)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1526)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1349)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:375)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:326)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:139)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException: column: [Engagement], physicalType: INT64, logicalType: float
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1127)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:189)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:175)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:328)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:219)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:125)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:294)
	... 42 more

Traceback (most recent call last):
  File "/app/scripts/video_interactions_transformed.py", line 161, in <module>
    duplicate_count = video_df.groupBy("UserID", "VideoID", "IngestionTimestamp").count().filter(col("count") > 1).count()
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py", line 1193, in count
    return int(self._jdf.count())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 169, in deco
    return f(*a, **kw)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 326, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: An error occurred while calling o73.count.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (8b973f15099c executor driver): org.apache.spark.SparkException: Parquet column cannot be converted in file s3a://datastreaming-analytics-1/staging/video_interactions/part-00000-86ce8a08-8a1f-4d6b-8111-b315bed37390-c000.snappy.parquet. Column: [Engagement], Expected: float, Found: INT64.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:868)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:298)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:125)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:594)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)
	at org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)
	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)
	at org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1535)
	at org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1462)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1526)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1349)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:375)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:326)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:139)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException: column: [Engagement], physicalType: INT64, logicalType: float
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1127)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:189)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:175)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:328)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:219)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:125)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:294)
	... 42 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
Caused by: org.apache.spark.SparkException: Parquet column cannot be converted in file s3a://datastreaming-analytics-1/staging/video_interactions/part-00000-86ce8a08-8a1f-4d6b-8111-b315bed37390-c000.snappy.parquet. Column: [Engagement], Expected: float, Found: INT64.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:868)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:298)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:125)
	at org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:594)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)
	at org.apache.spark.sql.execution.columnar.DefaultCachedBatchSerializer$$anon$1.hasNext(InMemoryRelation.scala:118)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)
	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)
	at org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1535)
	at org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1462)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1526)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1349)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:375)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:326)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:139)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException: column: [Engagement], physicalType: INT64, logicalType: float
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1127)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:189)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:175)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:328)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:219)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:125)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:294)
	... 42 more


2025-05-07 04:02:04,776 - INFO - Closing down clientserver connection
2025-05-07 04:05:12,853 - INFO - Initializing Spark session
2025-05-07 04:05:16,885 - INFO - Snowflake connection options: {'sfURL': 'uxctays-kqc58183.snowflakecomputing.com', 'sfUser': 'Pranitha22', 'sfPassword': 'Pradyumna@9999', 'sfDatabase': 'ANALYTICS', 'sfSchema': 'PUBLIC', 'sfWarehouse': 'COMPUTE_WHI', 'sfRole': 'ACCOUNTADMIN'}
2025-05-07 04:05:16,886 - INFO - Reading staging data from s3a://datastreaming-analytics-1/staging/video_interactions
2025-05-07 04:05:19,311 - WARNING - Hadoop check failed for S3 path s3a://datastreaming-analytics-1/staging/video_interactions: 'JavaObject' object is not iterable
Traceback (most recent call last):
  File "/app/scripts/video_interactions_transformed.py", line 79, in check_s3_path
    file_list = [(f.getPath().getName(), f.getLen()) for f in files]
TypeError: 'JavaObject' object is not iterable

2025-05-07 04:05:20,496 - INFO - AWS SDK check for s3a://datastreaming-analytics-1/staging/video_interactions: file count: 2, files: [('staging/video_interactions/_SUCCESS', 0), ('staging/video_interactions/part-00000-86ce8a08-8a1f-4d6b-8111-b315bed37390-c000.snappy.parquet', 144894)]
2025-05-07 04:05:26,732 - INFO - Actual Parquet schema: StructType([StructField('UserID', LongType(), True), StructField('Age', LongType(), True), StructField('Gender', StringType(), True), StructField('Location', StringType(), True), StructField('Income', LongType(), True), StructField('Debt', BooleanType(), True), StructField('OwnsProperty', BooleanType(), True), StructField('Profession', StringType(), True), StructField('Demographics', StringType(), True), StructField('Platform', StringType(), True), StructField('TotalTimeSpent', LongType(), True), StructField('NumberOfSessions', LongType(), True), StructField('VideoID', LongType(), True), StructField('VideoCategory', StringType(), True), StructField('VideoLength', LongType(), True), StructField('Engagement', LongType(), True), StructField('ImportanceScore', LongType(), True), StructField('TimeSpentOnVideo', LongType(), True), StructField('NumberOfVideosWatched', LongType(), True), StructField('ScrollRate', LongType(), True), StructField('Frequency', StringType(), True), StructField('ProductivityLoss', LongType(), True), StructField('Satisfaction', LongType(), True), StructField('WatchReason', StringType(), True), StructField('DeviceType', StringType(), True), StructField('OS', StringType(), True), StructField('WatchTime', StringType(), True), StructField('SelfControl', LongType(), True), StructField('AddictionLevel', LongType(), True), StructField('CurrentActivity', StringType(), True), StructField('ConnectionType', StringType(), True), StructField('IngestionTimestamp', TimestampType(), True), StructField('AgeGroup', StringType(), True)])
2025-05-07 04:05:27,845 - INFO - Checking duplicates in video_df
2025-05-07 04:05:32,290 - INFO - Found 0 duplicate combinations in video_df
2025-05-07 04:05:33,728 - INFO - Read and deduplicated video_interactions: 2844 rows
2025-05-07 04:05:33,733 - INFO - Input schema: StructType([StructField('VideoID', LongType(), True), StructField('UserID', LongType(), True), StructField('Platform', StringType(), True), StructField('WatchTime', StringType(), True), StructField('IngestionTimestamp', TimestampType(), True), StructField('Age', LongType(), True), StructField('Gender', StringType(), True), StructField('Location', StringType(), True), StructField('Income', LongType(), True), StructField('Debt', BooleanType(), True), StructField('OwnsProperty', BooleanType(), True), StructField('Profession', StringType(), True), StructField('Demographics', StringType(), True), StructField('CurrentActivity', StringType(), True), StructField('AgeGroup', StringType(), True), StructField('VideoCategory', StringType(), True), StructField('VideoLength', LongType(), True), StructField('TimeSpentOnVideo', LongType(), True), StructField('TotalTimeSpent', LongType(), True), StructField('Engagement', LongType(), True), StructField('NumberOfSessions', LongType(), True), StructField('NumberOfVideosWatched', LongType(), True), StructField('ScrollRate', LongType(), True), StructField('ImportanceScore', LongType(), True), StructField('ProductivityLoss', LongType(), True), StructField('Satisfaction', LongType(), True), StructField('SelfControl', LongType(), True), StructField('AddictionLevel', LongType(), True), StructField('DeviceType', StringType(), True), StructField('OS', StringType(), True), StructField('ConnectionType', StringType(), True), StructField('WatchReason', StringType(), True)])
2025-05-07 04:05:35,372 - INFO - Testing access to Snowflake tables
2025-05-07 04:05:37,597 - ERROR - Failed to access Snowflake tables: An error occurred while calling o102.load.
: net.snowflake.client.jdbc.SnowflakeSQLException: SQL compilation error:
Object 'ANALYTICS.PUBLIC.DIM_TIME' does not exist or not authorized.
	at net.snowflake.client.jdbc.SnowflakeUtil.checkErrorAndThrowExceptionSub(SnowflakeUtil.java:144)
	at net.snowflake.client.jdbc.SnowflakeUtil.checkErrorAndThrowException(SnowflakeUtil.java:77)
	at net.snowflake.client.core.StmtUtil.pollForOutput(StmtUtil.java:501)
	at net.snowflake.client.core.StmtUtil.execute(StmtUtil.java:407)
	at net.snowflake.client.core.SFStatement.executeHelper(SFStatement.java:490)
	at net.snowflake.client.core.SFStatement.executeQueryInternal(SFStatement.java:207)
	at net.snowflake.client.core.SFStatement.executeQuery(SFStatement.java:141)
	at net.snowflake.client.core.SFStatement.describe(SFStatement.java:162)
	at net.snowflake.client.jdbc.SnowflakePreparedStatementV1.describeSqlIfNotTried(SnowflakePreparedStatementV1.java:99)
	at net.snowflake.client.jdbc.SnowflakePreparedStatementV1.getMetaData(SnowflakePreparedStatementV1.java:584)
	at net.snowflake.spark.snowflake.DefaultJDBCWrapper$DataBaseOperations.tableMetaDataFromStatement(SnowflakeJDBCWrapper.scala:444)
	at net.snowflake.spark.snowflake.DefaultJDBCWrapper$DataBaseOperations.tableMetaData(SnowflakeJDBCWrapper.scala:437)
	at net.snowflake.spark.snowflake.JDBCWrapper.resolveTable(SnowflakeJDBCWrapper.scala:77)
	at net.snowflake.spark.snowflake.SnowflakeRelation.$anonfun$schema$1(SnowflakeRelation.scala:63)
	at scala.Option.getOrElse(Option.scala:189)
	at net.snowflake.spark.snowflake.SnowflakeRelation.schema$lzycompute(SnowflakeRelation.scala:58)
	at net.snowflake.spark.snowflake.SnowflakeRelation.schema(SnowflakeRelation.scala:57)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:434)
	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)
	at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)

Traceback (most recent call last):
  File "/app/scripts/video_interactions_transformed.py", line 186, in <module>
    test_df = spark.read.format("snowflake").options(**snowflake_options).option("dbtable", f"PUBLIC.{table}").load()
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 307, in load
    return self._df(self._jreader.load())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 169, in deco
    return f(*a, **kw)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 326, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: An error occurred while calling o102.load.
: net.snowflake.client.jdbc.SnowflakeSQLException: SQL compilation error:
Object 'ANALYTICS.PUBLIC.DIM_TIME' does not exist or not authorized.
	at net.snowflake.client.jdbc.SnowflakeUtil.checkErrorAndThrowExceptionSub(SnowflakeUtil.java:144)
	at net.snowflake.client.jdbc.SnowflakeUtil.checkErrorAndThrowException(SnowflakeUtil.java:77)
	at net.snowflake.client.core.StmtUtil.pollForOutput(StmtUtil.java:501)
	at net.snowflake.client.core.StmtUtil.execute(StmtUtil.java:407)
	at net.snowflake.client.core.SFStatement.executeHelper(SFStatement.java:490)
	at net.snowflake.client.core.SFStatement.executeQueryInternal(SFStatement.java:207)
	at net.snowflake.client.core.SFStatement.executeQuery(SFStatement.java:141)
	at net.snowflake.client.core.SFStatement.describe(SFStatement.java:162)
	at net.snowflake.client.jdbc.SnowflakePreparedStatementV1.describeSqlIfNotTried(SnowflakePreparedStatementV1.java:99)
	at net.snowflake.client.jdbc.SnowflakePreparedStatementV1.getMetaData(SnowflakePreparedStatementV1.java:584)
	at net.snowflake.spark.snowflake.DefaultJDBCWrapper$DataBaseOperations.tableMetaDataFromStatement(SnowflakeJDBCWrapper.scala:444)
	at net.snowflake.spark.snowflake.DefaultJDBCWrapper$DataBaseOperations.tableMetaData(SnowflakeJDBCWrapper.scala:437)
	at net.snowflake.spark.snowflake.JDBCWrapper.resolveTable(SnowflakeJDBCWrapper.scala:77)
	at net.snowflake.spark.snowflake.SnowflakeRelation.$anonfun$schema$1(SnowflakeRelation.scala:63)
	at scala.Option.getOrElse(Option.scala:189)
	at net.snowflake.spark.snowflake.SnowflakeRelation.schema$lzycompute(SnowflakeRelation.scala:58)
	at net.snowflake.spark.snowflake.SnowflakeRelation.schema(SnowflakeRelation.scala:57)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:434)
	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)
	at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)


2025-05-07 04:05:37,606 - ERROR - Unexpected error in video_interactions_transformed.py: An error occurred while calling o102.load.
: net.snowflake.client.jdbc.SnowflakeSQLException: SQL compilation error:
Object 'ANALYTICS.PUBLIC.DIM_TIME' does not exist or not authorized.
	at net.snowflake.client.jdbc.SnowflakeUtil.checkErrorAndThrowExceptionSub(SnowflakeUtil.java:144)
	at net.snowflake.client.jdbc.SnowflakeUtil.checkErrorAndThrowException(SnowflakeUtil.java:77)
	at net.snowflake.client.core.StmtUtil.pollForOutput(StmtUtil.java:501)
	at net.snowflake.client.core.StmtUtil.execute(StmtUtil.java:407)
	at net.snowflake.client.core.SFStatement.executeHelper(SFStatement.java:490)
	at net.snowflake.client.core.SFStatement.executeQueryInternal(SFStatement.java:207)
	at net.snowflake.client.core.SFStatement.executeQuery(SFStatement.java:141)
	at net.snowflake.client.core.SFStatement.describe(SFStatement.java:162)
	at net.snowflake.client.jdbc.SnowflakePreparedStatementV1.describeSqlIfNotTried(SnowflakePreparedStatementV1.java:99)
	at net.snowflake.client.jdbc.SnowflakePreparedStatementV1.getMetaData(SnowflakePreparedStatementV1.java:584)
	at net.snowflake.spark.snowflake.DefaultJDBCWrapper$DataBaseOperations.tableMetaDataFromStatement(SnowflakeJDBCWrapper.scala:444)
	at net.snowflake.spark.snowflake.DefaultJDBCWrapper$DataBaseOperations.tableMetaData(SnowflakeJDBCWrapper.scala:437)
	at net.snowflake.spark.snowflake.JDBCWrapper.resolveTable(SnowflakeJDBCWrapper.scala:77)
	at net.snowflake.spark.snowflake.SnowflakeRelation.$anonfun$schema$1(SnowflakeRelation.scala:63)
	at scala.Option.getOrElse(Option.scala:189)
	at net.snowflake.spark.snowflake.SnowflakeRelation.schema$lzycompute(SnowflakeRelation.scala:58)
	at net.snowflake.spark.snowflake.SnowflakeRelation.schema(SnowflakeRelation.scala:57)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:434)
	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)
	at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)

Traceback (most recent call last):
  File "/app/scripts/video_interactions_transformed.py", line 186, in <module>
    test_df = spark.read.format("snowflake").options(**snowflake_options).option("dbtable", f"PUBLIC.{table}").load()
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 307, in load
    return self._df(self._jreader.load())
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 169, in deco
    return f(*a, **kw)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 326, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: An error occurred while calling o102.load.
: net.snowflake.client.jdbc.SnowflakeSQLException: SQL compilation error:
Object 'ANALYTICS.PUBLIC.DIM_TIME' does not exist or not authorized.
	at net.snowflake.client.jdbc.SnowflakeUtil.checkErrorAndThrowExceptionSub(SnowflakeUtil.java:144)
	at net.snowflake.client.jdbc.SnowflakeUtil.checkErrorAndThrowException(SnowflakeUtil.java:77)
	at net.snowflake.client.core.StmtUtil.pollForOutput(StmtUtil.java:501)
	at net.snowflake.client.core.StmtUtil.execute(StmtUtil.java:407)
	at net.snowflake.client.core.SFStatement.executeHelper(SFStatement.java:490)
	at net.snowflake.client.core.SFStatement.executeQueryInternal(SFStatement.java:207)
	at net.snowflake.client.core.SFStatement.executeQuery(SFStatement.java:141)
	at net.snowflake.client.core.SFStatement.describe(SFStatement.java:162)
	at net.snowflake.client.jdbc.SnowflakePreparedStatementV1.describeSqlIfNotTried(SnowflakePreparedStatementV1.java:99)
	at net.snowflake.client.jdbc.SnowflakePreparedStatementV1.getMetaData(SnowflakePreparedStatementV1.java:584)
	at net.snowflake.spark.snowflake.DefaultJDBCWrapper$DataBaseOperations.tableMetaDataFromStatement(SnowflakeJDBCWrapper.scala:444)
	at net.snowflake.spark.snowflake.DefaultJDBCWrapper$DataBaseOperations.tableMetaData(SnowflakeJDBCWrapper.scala:437)
	at net.snowflake.spark.snowflake.JDBCWrapper.resolveTable(SnowflakeJDBCWrapper.scala:77)
	at net.snowflake.spark.snowflake.SnowflakeRelation.$anonfun$schema$1(SnowflakeRelation.scala:63)
	at scala.Option.getOrElse(Option.scala:189)
	at net.snowflake.spark.snowflake.SnowflakeRelation.schema$lzycompute(SnowflakeRelation.scala:58)
	at net.snowflake.spark.snowflake.SnowflakeRelation.schema(SnowflakeRelation.scala:57)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:434)
	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)
	at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:172)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)


2025-05-07 04:05:37,621 - INFO - Closing down clientserver connection
2025-05-07 04:08:50,822 - INFO - Initializing Spark session
2025-05-07 04:08:54,649 - INFO - Snowflake connection options: {'sfURL': 'uxctays-kqc58183.snowflakecomputing.com', 'sfUser': 'Pranitha22', 'sfPassword': 'Pradyumna@9999', 'sfDatabase': 'ANALYTICS', 'sfSchema': 'PUBLIC', 'sfWarehouse': 'COMPUTE_WHI', 'sfRole': 'ACCOUNTADMIN'}
2025-05-07 04:08:54,651 - INFO - Reading staging data from s3a://datastreaming-analytics-1/staging/video_interactions
2025-05-07 04:08:56,963 - WARNING - Hadoop check failed for S3 path s3a://datastreaming-analytics-1/staging/video_interactions: 'JavaObject' object is not iterable
Traceback (most recent call last):
  File "/app/scripts/video_interactions_transformed.py", line 79, in check_s3_path
    file_list = [(f.getPath().getName(), f.getLen()) for f in files]
TypeError: 'JavaObject' object is not iterable

2025-05-07 04:08:58,047 - INFO - AWS SDK check for s3a://datastreaming-analytics-1/staging/video_interactions: file count: 2, files: [('staging/video_interactions/_SUCCESS', 0), ('staging/video_interactions/part-00000-86ce8a08-8a1f-4d6b-8111-b315bed37390-c000.snappy.parquet', 144894)]
2025-05-07 04:09:05,776 - INFO - Actual Parquet schema: StructType([StructField('UserID', LongType(), True), StructField('Age', LongType(), True), StructField('Gender', StringType(), True), StructField('Location', StringType(), True), StructField('Income', LongType(), True), StructField('Debt', BooleanType(), True), StructField('OwnsProperty', BooleanType(), True), StructField('Profession', StringType(), True), StructField('Demographics', StringType(), True), StructField('Platform', StringType(), True), StructField('TotalTimeSpent', LongType(), True), StructField('NumberOfSessions', LongType(), True), StructField('VideoID', LongType(), True), StructField('VideoCategory', StringType(), True), StructField('VideoLength', LongType(), True), StructField('Engagement', LongType(), True), StructField('ImportanceScore', LongType(), True), StructField('TimeSpentOnVideo', LongType(), True), StructField('NumberOfVideosWatched', LongType(), True), StructField('ScrollRate', LongType(), True), StructField('Frequency', StringType(), True), StructField('ProductivityLoss', LongType(), True), StructField('Satisfaction', LongType(), True), StructField('WatchReason', StringType(), True), StructField('DeviceType', StringType(), True), StructField('OS', StringType(), True), StructField('WatchTime', StringType(), True), StructField('SelfControl', LongType(), True), StructField('AddictionLevel', LongType(), True), StructField('CurrentActivity', StringType(), True), StructField('ConnectionType', StringType(), True), StructField('IngestionTimestamp', TimestampType(), True), StructField('AgeGroup', StringType(), True)])
2025-05-07 04:09:04,675 - INFO - Checking duplicates in video_df
2025-05-07 04:09:09,264 - INFO - Found 0 duplicate combinations in video_df
2025-05-07 04:09:10,707 - INFO - Read and deduplicated video_interactions: 2844 rows
2025-05-07 04:09:10,712 - INFO - Input schema: StructType([StructField('VideoID', LongType(), True), StructField('UserID', LongType(), True), StructField('Platform', StringType(), True), StructField('WatchTime', StringType(), True), StructField('IngestionTimestamp', TimestampType(), True), StructField('Age', LongType(), True), StructField('Gender', StringType(), True), StructField('Location', StringType(), True), StructField('Income', LongType(), True), StructField('Debt', BooleanType(), True), StructField('OwnsProperty', BooleanType(), True), StructField('Profession', StringType(), True), StructField('Demographics', StringType(), True), StructField('CurrentActivity', StringType(), True), StructField('AgeGroup', StringType(), True), StructField('VideoCategory', StringType(), True), StructField('VideoLength', LongType(), True), StructField('TimeSpentOnVideo', LongType(), True), StructField('TotalTimeSpent', LongType(), True), StructField('Engagement', LongType(), True), StructField('NumberOfSessions', LongType(), True), StructField('NumberOfVideosWatched', LongType(), True), StructField('ScrollRate', LongType(), True), StructField('ImportanceScore', LongType(), True), StructField('ProductivityLoss', LongType(), True), StructField('Satisfaction', LongType(), True), StructField('SelfControl', LongType(), True), StructField('AddictionLevel', LongType(), True), StructField('DeviceType', StringType(), True), StructField('OS', StringType(), True), StructField('ConnectionType', StringType(), True), StructField('WatchReason', StringType(), True)])
2025-05-07 04:09:12,397 - INFO - Testing access to Snowflake tables
2025-05-07 04:09:16,254 - INFO - Successfully accessed PUBLIC.DIM_TIME: 0 rows
2025-05-07 04:09:18,333 - INFO - Successfully accessed PUBLIC.DIM_USER: 0 rows
2025-05-07 04:09:20,378 - INFO - Successfully accessed PUBLIC.DIM_VIDEO: 0 rows
2025-05-07 04:09:21,782 - INFO - Successfully accessed PUBLIC.DIM_PLATFORM: 0 rows
2025-05-07 04:09:23,695 - INFO - Successfully accessed PUBLIC.DIM_DEVICE_TYPE: 0 rows
2025-05-07 04:09:25,113 - INFO - Successfully accessed PUBLIC.DIM_OS: 0 rows
2025-05-07 04:09:26,418 - INFO - Successfully accessed PUBLIC.DIM_CONNECTION_TYPE: 0 rows
2025-05-07 04:09:27,185 - INFO - Successfully accessed PUBLIC.DIM_WATCH_REASON: 0 rows
2025-05-07 04:09:28,109 - INFO - Creating dim_time
2025-05-07 04:09:29,105 - INFO - Found 1842 new times to add to dim_time
2025-05-07 04:09:29,108 - INFO - missing_time_df schema: StructType([StructField('TimeID', StringType(), False), StructField('WatchTime', StringType(), False), StructField('Hour', LongType(), True)])
2025-05-07 04:09:32,474 - INFO - Successfully appended to dim_time: 1842 rows
2025-05-07 04:09:32,475 - INFO - Creating dim_user
2025-05-07 04:09:33,683 - INFO - Found 2844 new users to add to dim_user
2025-05-07 04:09:33,686 - INFO - missing_user_df schema: StructType([StructField('User_S_ID', StringType(), True), StructField('UserID', StringType(), True), StructField('Age', LongType(), True), StructField('Gender', StringType(), True), StructField('Location', StringType(), True), StructField('Income', LongType(), True), StructField('Debt', BooleanType(), True), StructField('OwnsProperty', BooleanType(), True), StructField('Profession', StringType(), True), StructField('Demographics', StringType(), True), StructField('CurrentActivity', StringType(), True), StructField('AgeGroup', StringType(), True)])
2025-05-07 04:09:38,779 - INFO - Successfully appended to dim_user: 2844 rows
2025-05-07 04:09:38,782 - INFO - Creating dim_video
2025-05-07 04:09:39,738 - INFO - Prepared dim_video: 2842 rows
2025-05-07 04:09:39,741 - INFO - video_dim_df schema: StructType([StructField('Video_S_ID', StringType(), True), StructField('VideoID', LongType(), True), StructField('VideoCategory', StringType(), True), StructField('VideoLength', LongType(), True), StructField('TimeSpentCategory', StringType(), False)])
2025-05-07 04:09:43,657 - INFO - Successfully wrote dim_video: 2842 rows
2025-05-07 04:09:43,658 - INFO - Creating dim_platform
2025-05-07 04:09:44,204 - INFO - Found 4 new platforms to add to dim_platform
2025-05-07 04:09:44,207 - INFO - missing_platform_df schema: StructType([StructField('PlatformID', StringType(), True), StructField('Platform', StringType(), True)])
2025-05-07 04:09:47,052 - INFO - Successfully appended to dim_platform: 4 rows
2025-05-07 04:09:47,053 - INFO - Creating dim_device_type
2025-05-07 04:09:47,572 - INFO - Found 3 new device types to add to dim_device_type
2025-05-07 04:09:47,574 - INFO - missing_device_type_df schema: StructType([StructField('DeviceTypeID', StringType(), True), StructField('DeviceType', StringType(), True)])
2025-05-07 04:09:50,167 - INFO - Successfully appended to dim_device_type: 3 rows
2025-05-07 04:09:50,168 - INFO - Creating dim_os
2025-05-07 04:09:50,567 - INFO - Found 4 new OS to add to dim_os
2025-05-07 04:09:50,571 - INFO - missing_os_df schema: StructType([StructField('OS_ID', StringType(), True), StructField('OS', StringType(), True)])
2025-05-07 04:09:53,764 - INFO - Successfully appended to dim_os: 4 rows
2025-05-07 04:09:53,765 - INFO - Creating dim_connection_type
2025-05-07 04:09:54,156 - INFO - Found 2 new connection types to add to dim_connection_type
2025-05-07 04:09:54,159 - INFO - missing_connection_type_df schema: StructType([StructField('ConnectionTypeID', StringType(), True), StructField('ConnectionType', StringType(), True)])
2025-05-07 04:09:56,649 - INFO - Successfully appended to dim_connection_type: 2 rows
2025-05-07 04:09:56,650 - INFO - Creating dim_watch_reason
2025-05-07 04:09:57,083 - INFO - Prepared dim_watch_reason: 4 rows
2025-05-07 04:09:57,086 - INFO - watch_reason_df schema: StructType([StructField('WatchReasonID', StringType(), True), StructField('WatchReason', StringType(), True)])
2025-05-07 04:10:00,103 - INFO - Successfully wrote dim_watch_reason: 4 rows
2025-05-07 04:10:00,104 - INFO - Creating fact_video_interactions
2025-05-07 04:10:00,252 - INFO - Checking duplicates in fact_df before joins
2025-05-07 04:10:01,371 - INFO - Found 0 duplicate InteractionIDs in fact_df
2025-05-07 04:10:00,926 - INFO - dim_user_df schema: StructType([StructField('User_S_ID', StringType(), True), StructField('UserID', StringType(), True)])
2025-05-07 04:10:02,009 - INFO - After joining with dim_user: 2844 rows
2025-05-07 04:10:03,120 - INFO - dim_video_df schema: StructType([StructField('Video_S_ID', StringType(), True), StructField('VideoID', DecimalType(38,0), True)])
2025-05-07 04:10:04,042 - INFO - After joining with dim_video: 5050 rows
2025-05-07 04:10:04,958 - INFO - dim_platform_df schema: StructType([StructField('PlatformID', StringType(), True), StructField('Platform', StringType(), True)])
2025-05-07 04:10:05,872 - INFO - After joining with dim_platform: 5050 rows
2025-05-07 04:10:06,850 - INFO - dim_device_type_df schema: StructType([StructField('DeviceTypeID', StringType(), True), StructField('DeviceType', StringType(), True)])
2025-05-07 04:10:07,601 - INFO - After joining with dim_device_type: 5050 rows
2025-05-07 04:10:07,982 - INFO - dim_os_df schema: StructType([StructField('OS_ID', StringType(), True), StructField('OS', StringType(), True)])
2025-05-07 04:10:08,879 - INFO - After joining with dim_os: 5050 rows
2025-05-07 04:10:09,848 - INFO - dim_connection_type_df schema: StructType([StructField('ConnectionTypeID', StringType(), True), StructField('ConnectionType', StringType(), True)])
2025-05-07 04:10:10,731 - INFO - After joining with dim_connection_type: 5050 rows
2025-05-07 04:10:11,805 - INFO - dim_time_df schema: StructType([StructField('TimeID', StringType(), True), StructField('WatchTime', StringType(), True)])
2025-05-07 04:10:12,839 - INFO - After joining with dim_time: 5050 rows
2025-05-07 04:10:13,857 - INFO - dim_watch_reason_df schema: StructType([StructField('WatchReasonID', StringType(), True), StructField('WatchReason', StringType(), True)])
2025-05-07 04:10:14,960 - INFO - After joining with dim_watch_reason: 5050 rows
2025-05-07 04:10:14,962 - INFO - Deduplicating fact_df
2025-05-07 04:10:16,293 - INFO - After deduplication: 2844 rows
2025-05-07 04:10:19,177 - INFO - Null counts for fact_video_interactions: {'InteractionID': 0, 'UserID_Surrogate': 0, 'VideoID_Surrogate': 0, 'PlatformID': 0, 'DeviceTypeID': 0, 'OS_ID': 0, 'ConnectionTypeID': 0, 'TimeID': 0, 'WatchReasonID': 0, 'TimeSpentOnVideo': 0, 'TotalTimeSpent': 0, 'Engagement': 0, 'NumberOfSessions': 0, 'NumberOfVideosWatched': 0, 'ScrollRate': 0, 'ImportanceScore': 0, 'ProductivityLoss': 0, 'Satisfaction': 0, 'SelfControl': 0, 'AddictionLevel': 0, 'IngestionTimestamp': 0}
2025-05-07 04:10:22,085 - INFO - Prepared fact_video_interactions: 2844 rows
2025-05-07 04:10:22,088 - INFO - fact_df schema: StructType([StructField('InteractionID', StringType(), False), StructField('UserID_Surrogate', StringType(), True), StructField('VideoID_Surrogate', StringType(), True), StructField('PlatformID', StringType(), True), StructField('DeviceTypeID', StringType(), True), StructField('OS_ID', StringType(), True), StructField('ConnectionTypeID', StringType(), True), StructField('TimeID', StringType(), True), StructField('WatchReasonID', StringType(), True), StructField('TimeSpentOnVideo', LongType(), True), StructField('TotalTimeSpent', LongType(), True), StructField('Engagement', FloatType(), True), StructField('NumberOfSessions', LongType(), True), StructField('NumberOfVideosWatched', LongType(), True), StructField('ScrollRate', FloatType(), True), StructField('ImportanceScore', FloatType(), True), StructField('ProductivityLoss', FloatType(), True), StructField('Satisfaction', FloatType(), True), StructField('SelfControl', FloatType(), True), StructField('AddictionLevel', FloatType(), True), StructField('IngestionTimestamp', TimestampType(), True)])
2025-05-07 04:10:23,545 - INFO - Writing to PUBLIC.FACT_VIDEO_INTERACTIONS with options: {'sfURL': 'uxctays-kqc58183.snowflakecomputing.com', 'sfUser': 'Pranitha22', 'sfPassword': 'Pradyumna@9999', 'sfDatabase': 'ANALYTICS', 'sfSchema': 'PUBLIC', 'sfWarehouse': 'COMPUTE_WHI', 'sfRole': 'ACCOUNTADMIN'}
2025-05-07 04:10:29,206 - INFO - Successfully wrote fact_video_interactions: 2844 rows
2025-05-07 04:10:27,621 - INFO - Spark session stopped
2025-05-07 04:10:27,624 - INFO - Closing down clientserver connection
2025-05-07 04:17:01,954 - INFO - Initializing Spark session
2025-05-07 04:17:06,195 - INFO - Snowflake connection options: {'sfURL': 'uxctays-kqc58183.snowflakecomputing.com', 'sfUser': 'Pranitha22', 'sfPassword': 'Pradyumna@9999', 'sfDatabase': 'ANALYTICS', 'sfSchema': 'PUBLIC', 'sfWarehouse': 'COMPUTE_WHI', 'sfRole': 'ACCOUNTADMIN'}
2025-05-07 04:17:06,197 - INFO - Reading staging data from s3a://datastreaming-analytics-1/staging/video_interactions
2025-05-07 04:17:08,597 - WARNING - Hadoop check failed for S3 path s3a://datastreaming-analytics-1/staging/video_interactions: 'JavaObject' object is not iterable
Traceback (most recent call last):
  File "/app/scripts/video_interactions_transformed.py", line 79, in check_s3_path
    file_list = [(f.getPath().getName(), f.getLen()) for f in files]
TypeError: 'JavaObject' object is not iterable

2025-05-07 04:17:09,753 - INFO - AWS SDK check for s3a://datastreaming-analytics-1/staging/video_interactions: file count: 2, files: [('staging/video_interactions/_SUCCESS', 0), ('staging/video_interactions/part-00000-86ce8a08-8a1f-4d6b-8111-b315bed37390-c000.snappy.parquet', 144894)]
2025-05-07 04:17:17,277 - INFO - Actual Parquet schema: StructType([StructField('UserID', LongType(), True), StructField('Age', LongType(), True), StructField('Gender', StringType(), True), StructField('Location', StringType(), True), StructField('Income', LongType(), True), StructField('Debt', BooleanType(), True), StructField('OwnsProperty', BooleanType(), True), StructField('Profession', StringType(), True), StructField('Demographics', StringType(), True), StructField('Platform', StringType(), True), StructField('TotalTimeSpent', LongType(), True), StructField('NumberOfSessions', LongType(), True), StructField('VideoID', LongType(), True), StructField('VideoCategory', StringType(), True), StructField('VideoLength', LongType(), True), StructField('Engagement', LongType(), True), StructField('ImportanceScore', LongType(), True), StructField('TimeSpentOnVideo', LongType(), True), StructField('NumberOfVideosWatched', LongType(), True), StructField('ScrollRate', LongType(), True), StructField('Frequency', StringType(), True), StructField('ProductivityLoss', LongType(), True), StructField('Satisfaction', LongType(), True), StructField('WatchReason', StringType(), True), StructField('DeviceType', StringType(), True), StructField('OS', StringType(), True), StructField('WatchTime', StringType(), True), StructField('SelfControl', LongType(), True), StructField('AddictionLevel', LongType(), True), StructField('CurrentActivity', StringType(), True), StructField('ConnectionType', StringType(), True), StructField('IngestionTimestamp', TimestampType(), True), StructField('AgeGroup', StringType(), True)])
2025-05-07 04:17:18,486 - INFO - Checking duplicates in video_df
2025-05-07 04:17:23,083 - INFO - Found 0 duplicate combinations in video_df
2025-05-07 04:17:24,533 - INFO - Read and deduplicated video_interactions: 2844 rows
2025-05-07 04:17:24,538 - INFO - Input schema: StructType([StructField('VideoID', LongType(), True), StructField('UserID', LongType(), True), StructField('Platform', StringType(), True), StructField('WatchTime', StringType(), True), StructField('IngestionTimestamp', TimestampType(), True), StructField('Age', LongType(), True), StructField('Gender', StringType(), True), StructField('Location', StringType(), True), StructField('Income', LongType(), True), StructField('Debt', BooleanType(), True), StructField('OwnsProperty', BooleanType(), True), StructField('Profession', StringType(), True), StructField('Demographics', StringType(), True), StructField('CurrentActivity', StringType(), True), StructField('AgeGroup', StringType(), True), StructField('VideoCategory', StringType(), True), StructField('VideoLength', LongType(), True), StructField('TimeSpentOnVideo', LongType(), True), StructField('TotalTimeSpent', LongType(), True), StructField('Engagement', LongType(), True), StructField('NumberOfSessions', LongType(), True), StructField('NumberOfVideosWatched', LongType(), True), StructField('ScrollRate', LongType(), True), StructField('ImportanceScore', LongType(), True), StructField('ProductivityLoss', LongType(), True), StructField('Satisfaction', LongType(), True), StructField('SelfControl', LongType(), True), StructField('AddictionLevel', LongType(), True), StructField('DeviceType', StringType(), True), StructField('OS', StringType(), True), StructField('ConnectionType', StringType(), True), StructField('WatchReason', StringType(), True)])
2025-05-07 04:17:26,213 - INFO - Testing access to Snowflake tables
2025-05-07 04:17:27,818 - INFO - Successfully accessed PUBLIC.DIM_TIME: 0 rows
2025-05-07 04:17:29,237 - INFO - Successfully accessed PUBLIC.DIM_USER: 0 rows
2025-05-07 04:17:30,712 - INFO - Successfully accessed PUBLIC.DIM_VIDEO: 0 rows
2025-05-07 04:17:32,152 - INFO - Successfully accessed PUBLIC.DIM_PLATFORM: 0 rows
2025-05-07 04:17:33,540 - INFO - Successfully accessed PUBLIC.DIM_DEVICE_TYPE: 0 rows
2025-05-07 04:17:34,876 - INFO - Successfully accessed PUBLIC.DIM_OS: 0 rows
2025-05-07 04:17:36,228 - INFO - Successfully accessed PUBLIC.DIM_CONNECTION_TYPE: 0 rows
2025-05-07 04:17:37,070 - INFO - Successfully accessed PUBLIC.DIM_WATCH_REASON: 0 rows
2025-05-07 04:17:38,006 - INFO - Creating dim_time
2025-05-07 04:17:39,020 - INFO - Found 1842 new times to add to dim_time
2025-05-07 04:17:39,023 - INFO - missing_time_df schema: StructType([StructField('TimeID', StringType(), False), StructField('WatchTime', StringType(), False), StructField('Hour', LongType(), True)])
2025-05-07 04:17:44,797 - INFO - Successfully appended to dim_time: 1842 rows
2025-05-07 04:17:44,797 - INFO - Creating dim_user
2025-05-07 04:17:46,044 - INFO - Found 2844 new users to add to dim_user
2025-05-07 04:17:46,047 - INFO - missing_user_df schema: StructType([StructField('User_S_ID', StringType(), True), StructField('UserID', StringType(), True), StructField('Age', LongType(), True), StructField('Gender', StringType(), True), StructField('Location', StringType(), True), StructField('Income', LongType(), True), StructField('Debt', BooleanType(), True), StructField('OwnsProperty', BooleanType(), True), StructField('Profession', StringType(), True), StructField('Demographics', StringType(), True), StructField('CurrentActivity', StringType(), True), StructField('AgeGroup', StringType(), True)])
2025-05-07 04:17:51,041 - INFO - Successfully appended to dim_user: 2844 rows
2025-05-07 04:17:51,041 - INFO - Creating dim_video
2025-05-07 04:17:51,897 - INFO - Prepared dim_video: 2842 rows
2025-05-07 04:17:51,900 - INFO - video_dim_df schema: StructType([StructField('Video_S_ID', StringType(), True), StructField('VideoID', LongType(), True), StructField('VideoCategory', StringType(), True), StructField('VideoLength', LongType(), True), StructField('TimeSpentCategory', StringType(), False)])
2025-05-07 04:17:53,558 - INFO - Successfully wrote dim_video: 2842 rows
2025-05-07 04:17:53,560 - INFO - Creating dim_platform
2025-05-07 04:17:54,255 - INFO - Found 4 new platforms to add to dim_platform
2025-05-07 04:17:54,258 - INFO - missing_platform_df schema: StructType([StructField('PlatformID', StringType(), True), StructField('Platform', StringType(), True)])
2025-05-07 04:17:57,086 - INFO - Successfully appended to dim_platform: 4 rows
2025-05-07 04:17:57,086 - INFO - Creating dim_device_type
2025-05-07 04:17:57,588 - INFO - Found 3 new device types to add to dim_device_type
2025-05-07 04:17:57,590 - INFO - missing_device_type_df schema: StructType([StructField('DeviceTypeID', StringType(), True), StructField('DeviceType', StringType(), True)])
2025-05-07 04:18:00,804 - INFO - Successfully appended to dim_device_type: 3 rows
2025-05-07 04:18:00,804 - INFO - Creating dim_os
2025-05-07 04:18:01,202 - INFO - Found 4 new OS to add to dim_os
2025-05-07 04:18:01,206 - INFO - missing_os_df schema: StructType([StructField('OS_ID', StringType(), True), StructField('OS', StringType(), True)])
2025-05-07 04:18:03,825 - INFO - Successfully appended to dim_os: 4 rows
2025-05-07 04:18:03,826 - INFO - Creating dim_connection_type
2025-05-07 04:18:04,224 - INFO - Found 2 new connection types to add to dim_connection_type
2025-05-07 04:18:04,228 - INFO - missing_connection_type_df schema: StructType([StructField('ConnectionTypeID', StringType(), True), StructField('ConnectionType', StringType(), True)])
2025-05-07 04:18:06,775 - INFO - Successfully appended to dim_connection_type: 2 rows
2025-05-07 04:18:06,776 - INFO - Creating dim_watch_reason
2025-05-07 04:18:07,307 - INFO - Prepared dim_watch_reason: 4 rows
2025-05-07 04:18:07,311 - INFO - watch_reason_df schema: StructType([StructField('WatchReasonID', StringType(), True), StructField('WatchReason', StringType(), True)])
2025-05-07 04:18:10,183 - INFO - Successfully wrote dim_watch_reason: 4 rows
2025-05-07 04:18:10,184 - INFO - Creating fact_video_interactions
2025-05-07 04:18:10,290 - INFO - Checking duplicates in fact_df before joins
2025-05-07 04:18:11,385 - INFO - Found 0 duplicate InteractionIDs in fact_df
2025-05-07 04:18:13,100 - INFO - dim_user_df schema: StructType([StructField('User_S_ID', StringType(), True), StructField('UserID', StringType(), True)])
2025-05-07 04:18:14,196 - INFO - After joining with dim_user: 2844 rows
2025-05-07 04:18:15,158 - INFO - dim_video_df schema: StructType([StructField('Video_S_ID', StringType(), True), StructField('VideoID', DecimalType(38,0), True)])
2025-05-07 04:18:16,160 - INFO - After joining with dim_video: 5050 rows
2025-05-07 04:18:17,065 - INFO - dim_platform_df schema: StructType([StructField('PlatformID', StringType(), True), StructField('Platform', StringType(), True)])
2025-05-07 04:18:17,875 - INFO - After joining with dim_platform: 5050 rows
2025-05-07 04:18:18,840 - INFO - dim_device_type_df schema: StructType([StructField('DeviceTypeID', StringType(), True), StructField('DeviceType', StringType(), True)])
2025-05-07 04:18:19,562 - INFO - After joining with dim_device_type: 5050 rows
2025-05-07 04:18:20,528 - INFO - dim_os_df schema: StructType([StructField('OS_ID', StringType(), True), StructField('OS', StringType(), True)])
2025-05-07 04:18:21,350 - INFO - After joining with dim_os: 5050 rows
2025-05-07 04:18:19,993 - INFO - dim_connection_type_df schema: StructType([StructField('ConnectionTypeID', StringType(), True), StructField('ConnectionType', StringType(), True)])
2025-05-07 04:18:20,847 - INFO - After joining with dim_connection_type: 5050 rows
2025-05-07 04:18:21,881 - INFO - dim_time_df schema: StructType([StructField('TimeID', StringType(), True), StructField('WatchTime', StringType(), True)])
2025-05-07 04:18:22,808 - INFO - After joining with dim_time: 5050 rows
2025-05-07 04:18:23,706 - INFO - dim_watch_reason_df schema: StructType([StructField('WatchReasonID', StringType(), True), StructField('WatchReason', StringType(), True)])
2025-05-07 04:18:24,693 - INFO - After joining with dim_watch_reason: 5050 rows
2025-05-07 04:18:24,693 - INFO - Deduplicating fact_df
2025-05-07 04:18:25,829 - INFO - After deduplication: 2844 rows
2025-05-07 04:18:28,546 - INFO - Null counts for fact_video_interactions: {'InteractionID': 0, 'UserID_Surrogate': 0, 'VideoID_Surrogate': 0, 'PlatformID': 0, 'DeviceTypeID': 0, 'OS_ID': 0, 'ConnectionTypeID': 0, 'TimeID': 0, 'WatchReasonID': 0, 'TimeSpentOnVideo': 0, 'TotalTimeSpent': 0, 'Engagement': 0, 'NumberOfSessions': 0, 'NumberOfVideosWatched': 0, 'ScrollRate': 0, 'ImportanceScore': 0, 'ProductivityLoss': 0, 'Satisfaction': 0, 'SelfControl': 0, 'AddictionLevel': 0, 'IngestionTimestamp': 0}
2025-05-07 04:18:31,161 - INFO - Prepared fact_video_interactions: 2844 rows
2025-05-07 04:18:31,165 - INFO - fact_df schema: StructType([StructField('InteractionID', StringType(), False), StructField('UserID_Surrogate', StringType(), True), StructField('VideoID_Surrogate', StringType(), True), StructField('PlatformID', StringType(), True), StructField('DeviceTypeID', StringType(), True), StructField('OS_ID', StringType(), True), StructField('ConnectionTypeID', StringType(), True), StructField('TimeID', StringType(), True), StructField('WatchReasonID', StringType(), True), StructField('TimeSpentOnVideo', LongType(), True), StructField('TotalTimeSpent', LongType(), True), StructField('Engagement', FloatType(), True), StructField('NumberOfSessions', LongType(), True), StructField('NumberOfVideosWatched', LongType(), True), StructField('ScrollRate', FloatType(), True), StructField('ImportanceScore', FloatType(), True), StructField('ProductivityLoss', FloatType(), True), StructField('Satisfaction', FloatType(), True), StructField('SelfControl', FloatType(), True), StructField('AddictionLevel', FloatType(), True), StructField('IngestionTimestamp', TimestampType(), True)])
2025-05-07 04:18:32,730 - INFO - Writing to PUBLIC.FACT_VIDEO_INTERACTIONS with options: {'sfURL': 'uxctays-kqc58183.snowflakecomputing.com', 'sfUser': 'Pranitha22', 'sfPassword': 'Pradyumna@9999', 'sfDatabase': 'ANALYTICS', 'sfSchema': 'PUBLIC', 'sfWarehouse': 'COMPUTE_WHI', 'sfRole': 'ACCOUNTADMIN'}
2025-05-07 04:18:38,254 - INFO - Successfully wrote fact_video_interactions: 2844 rows
2025-05-07 04:18:38,585 - INFO - Spark session stopped
2025-05-07 04:18:38,587 - INFO - Closing down clientserver connection
2025-05-07 13:48:47,845 - INFO - Initializing Spark session
2025-05-07 13:48:52,742 - INFO - Snowflake connection options: {'sfURL': 'uxctays-kqc58183.snowflakecomputing.com', 'sfUser': 'Pranitha22', 'sfPassword': 'Pradyumna@9999', 'sfDatabase': 'ANALYTICS', 'sfSchema': 'PUBLIC', 'sfWarehouse': 'COMPUTE_WHI', 'sfRole': 'ACCOUNTADMIN'}
2025-05-07 13:48:52,743 - INFO - Reading staging data from s3a://datastreaming-analytics-1/staging/video_interactions
2025-05-07 13:48:55,811 - WARNING - Hadoop check failed for S3 path s3a://datastreaming-analytics-1/staging/video_interactions: 'JavaObject' object is not iterable
Traceback (most recent call last):
  File "/app/scripts/video_interactions_transformed.py", line 79, in check_s3_path
    file_list = [(f.getPath().getName(), f.getLen()) for f in files]
TypeError: 'JavaObject' object is not iterable

2025-05-07 13:48:57,011 - INFO - AWS SDK check for s3a://datastreaming-analytics-1/staging/video_interactions: file count: 2, files: [('staging/video_interactions/_SUCCESS', 0), ('staging/video_interactions/part-00000-86ce8a08-8a1f-4d6b-8111-b315bed37390-c000.snappy.parquet', 144894)]
2025-05-07 13:49:06,258 - INFO - Actual Parquet schema: StructType([StructField('UserID', LongType(), True), StructField('Age', LongType(), True), StructField('Gender', StringType(), True), StructField('Location', StringType(), True), StructField('Income', LongType(), True), StructField('Debt', BooleanType(), True), StructField('OwnsProperty', BooleanType(), True), StructField('Profession', StringType(), True), StructField('Demographics', StringType(), True), StructField('Platform', StringType(), True), StructField('TotalTimeSpent', LongType(), True), StructField('NumberOfSessions', LongType(), True), StructField('VideoID', LongType(), True), StructField('VideoCategory', StringType(), True), StructField('VideoLength', LongType(), True), StructField('Engagement', LongType(), True), StructField('ImportanceScore', LongType(), True), StructField('TimeSpentOnVideo', LongType(), True), StructField('NumberOfVideosWatched', LongType(), True), StructField('ScrollRate', LongType(), True), StructField('Frequency', StringType(), True), StructField('ProductivityLoss', LongType(), True), StructField('Satisfaction', LongType(), True), StructField('WatchReason', StringType(), True), StructField('DeviceType', StringType(), True), StructField('OS', StringType(), True), StructField('WatchTime', StringType(), True), StructField('SelfControl', LongType(), True), StructField('AddictionLevel', LongType(), True), StructField('CurrentActivity', StringType(), True), StructField('ConnectionType', StringType(), True), StructField('IngestionTimestamp', TimestampType(), True), StructField('AgeGroup', StringType(), True)])
2025-05-07 13:49:07,590 - INFO - Checking duplicates in video_df
2025-05-07 13:49:13,987 - INFO - Found 0 duplicate combinations in video_df
2025-05-07 13:49:17,993 - INFO - Read and deduplicated video_interactions: 2844 rows
2025-05-07 13:49:18,001 - INFO - Input schema: StructType([StructField('VideoID', LongType(), True), StructField('UserID', LongType(), True), StructField('Platform', StringType(), True), StructField('WatchTime', StringType(), True), StructField('IngestionTimestamp', TimestampType(), True), StructField('Age', LongType(), True), StructField('Gender', StringType(), True), StructField('Location', StringType(), True), StructField('Income', LongType(), True), StructField('Debt', BooleanType(), True), StructField('OwnsProperty', BooleanType(), True), StructField('Profession', StringType(), True), StructField('Demographics', StringType(), True), StructField('CurrentActivity', StringType(), True), StructField('AgeGroup', StringType(), True), StructField('VideoCategory', StringType(), True), StructField('VideoLength', LongType(), True), StructField('TimeSpentOnVideo', LongType(), True), StructField('TotalTimeSpent', LongType(), True), StructField('Engagement', LongType(), True), StructField('NumberOfSessions', LongType(), True), StructField('NumberOfVideosWatched', LongType(), True), StructField('ScrollRate', LongType(), True), StructField('ImportanceScore', LongType(), True), StructField('ProductivityLoss', LongType(), True), StructField('Satisfaction', LongType(), True), StructField('SelfControl', LongType(), True), StructField('AddictionLevel', LongType(), True), StructField('DeviceType', StringType(), True), StructField('OS', StringType(), True), StructField('ConnectionType', StringType(), True), StructField('WatchReason', StringType(), True)])
2025-05-07 13:49:21,844 - INFO - Testing access to Snowflake tables
2025-05-07 13:49:27,260 - INFO - Successfully accessed PUBLIC.DIM_TIME: 0 rows
2025-05-07 13:49:29,083 - INFO - Successfully accessed PUBLIC.DIM_USER: 0 rows
2025-05-07 13:49:28,189 - INFO - Successfully accessed PUBLIC.DIM_VIDEO: 0 rows
2025-05-07 13:49:29,838 - INFO - Successfully accessed PUBLIC.DIM_PLATFORM: 0 rows
2025-05-07 13:49:31,548 - INFO - Successfully accessed PUBLIC.DIM_DEVICE_TYPE: 0 rows
2025-05-07 13:49:33,643 - INFO - Successfully accessed PUBLIC.DIM_OS: 0 rows
2025-05-07 13:49:35,476 - INFO - Successfully accessed PUBLIC.DIM_CONNECTION_TYPE: 0 rows
2025-05-07 13:49:37,332 - INFO - Successfully accessed PUBLIC.DIM_WATCH_REASON: 0 rows
2025-05-07 13:49:37,784 - INFO - Creating dim_time
2025-05-07 13:49:39,769 - INFO - Found 1842 new times to add to dim_time
2025-05-07 13:49:39,779 - INFO - missing_time_df schema: StructType([StructField('TimeID', StringType(), False), StructField('WatchTime', StringType(), False), StructField('Hour', LongType(), True)])
2025-05-07 13:49:46,353 - INFO - Successfully appended to dim_time: 1842 rows
2025-05-07 13:49:46,356 - INFO - Creating dim_user
2025-05-07 13:49:48,156 - INFO - Found 2844 new users to add to dim_user
2025-05-07 13:49:48,171 - INFO - missing_user_df schema: StructType([StructField('User_S_ID', StringType(), True), StructField('UserID', StringType(), True), StructField('Age', LongType(), True), StructField('Gender', StringType(), True), StructField('Location', StringType(), True), StructField('Income', LongType(), True), StructField('Debt', BooleanType(), True), StructField('OwnsProperty', BooleanType(), True), StructField('Profession', StringType(), True), StructField('Demographics', StringType(), True), StructField('CurrentActivity', StringType(), True), StructField('AgeGroup', StringType(), True)])
2025-05-07 13:49:55,749 - INFO - Successfully appended to dim_user: 2844 rows
2025-05-07 13:49:55,752 - INFO - Creating dim_video
2025-05-07 13:49:55,425 - INFO - Prepared dim_video: 2842 rows
2025-05-07 13:49:55,439 - INFO - video_dim_df schema: StructType([StructField('Video_S_ID', StringType(), True), StructField('VideoID', LongType(), True), StructField('VideoCategory', StringType(), True), StructField('VideoLength', LongType(), True), StructField('TimeSpentCategory', StringType(), False)])
2025-05-07 13:50:00,290 - INFO - Successfully wrote dim_video: 2842 rows
2025-05-07 13:50:00,291 - INFO - Creating dim_platform
2025-05-07 13:50:01,150 - INFO - Found 4 new platforms to add to dim_platform
2025-05-07 13:50:01,153 - INFO - missing_platform_df schema: StructType([StructField('PlatformID', StringType(), True), StructField('Platform', StringType(), True)])
2025-05-07 13:50:04,787 - INFO - Successfully appended to dim_platform: 4 rows
2025-05-07 13:50:04,788 - INFO - Creating dim_device_type
2025-05-07 13:50:05,575 - INFO - Found 3 new device types to add to dim_device_type
2025-05-07 13:50:05,581 - INFO - missing_device_type_df schema: StructType([StructField('DeviceTypeID', StringType(), True), StructField('DeviceType', StringType(), True)])
2025-05-07 13:50:09,343 - INFO - Successfully appended to dim_device_type: 3 rows
2025-05-07 13:50:09,344 - INFO - Creating dim_os
2025-05-07 13:50:10,153 - INFO - Found 4 new OS to add to dim_os
2025-05-07 13:50:10,169 - INFO - missing_os_df schema: StructType([StructField('OS_ID', StringType(), True), StructField('OS', StringType(), True)])
2025-05-07 13:50:14,459 - INFO - Successfully appended to dim_os: 4 rows
2025-05-07 13:50:14,459 - INFO - Creating dim_connection_type
2025-05-07 13:50:15,018 - INFO - Found 2 new connection types to add to dim_connection_type
2025-05-07 13:50:15,022 - INFO - missing_connection_type_df schema: StructType([StructField('ConnectionTypeID', StringType(), True), StructField('ConnectionType', StringType(), True)])
2025-05-07 13:50:18,089 - INFO - Successfully appended to dim_connection_type: 2 rows
2025-05-07 13:50:18,090 - INFO - Creating dim_watch_reason
2025-05-07 13:50:18,578 - INFO - Prepared dim_watch_reason: 4 rows
2025-05-07 13:50:18,584 - INFO - watch_reason_df schema: StructType([StructField('WatchReasonID', StringType(), True), StructField('WatchReason', StringType(), True)])
2025-05-07 13:50:21,646 - INFO - Successfully wrote dim_watch_reason: 4 rows
2025-05-07 13:50:21,647 - INFO - Creating fact_video_interactions
2025-05-07 13:50:21,767 - INFO - Checking duplicates in fact_df before joins
2025-05-07 13:50:23,263 - INFO - Found 0 duplicate InteractionIDs in fact_df
2025-05-07 13:50:22,455 - INFO - dim_user_df schema: StructType([StructField('User_S_ID', StringType(), True), StructField('UserID', StringType(), True)])
2025-05-07 13:50:24,255 - INFO - After joining with dim_user: 2844 rows
2025-05-07 13:50:25,441 - INFO - dim_video_df schema: StructType([StructField('Video_S_ID', StringType(), True), StructField('VideoID', DecimalType(38,0), True)])
2025-05-07 13:50:28,194 - INFO - After joining with dim_video: 5050 rows
2025-05-07 13:50:29,154 - INFO - dim_platform_df schema: StructType([StructField('PlatformID', StringType(), True), StructField('Platform', StringType(), True)])
2025-05-07 13:50:30,977 - INFO - After joining with dim_platform: 5050 rows
2025-05-07 13:50:31,975 - INFO - dim_device_type_df schema: StructType([StructField('DeviceTypeID', StringType(), True), StructField('DeviceType', StringType(), True)])
2025-05-07 13:50:32,983 - INFO - After joining with dim_device_type: 5050 rows
2025-05-07 13:50:33,600 - INFO - dim_os_df schema: StructType([StructField('OS_ID', StringType(), True), StructField('OS', StringType(), True)])
2025-05-07 13:50:34,769 - INFO - After joining with dim_os: 5050 rows
2025-05-07 13:50:35,713 - INFO - dim_connection_type_df schema: StructType([StructField('ConnectionTypeID', StringType(), True), StructField('ConnectionType', StringType(), True)])
2025-05-07 13:50:37,469 - INFO - After joining with dim_connection_type: 5050 rows
2025-05-07 13:50:38,510 - INFO - dim_time_df schema: StructType([StructField('TimeID', StringType(), True), StructField('WatchTime', StringType(), True)])
2025-05-07 13:50:39,899 - INFO - After joining with dim_time: 5050 rows
2025-05-07 13:50:40,299 - INFO - dim_watch_reason_df schema: StructType([StructField('WatchReasonID', StringType(), True), StructField('WatchReason', StringType(), True)])
2025-05-07 13:50:42,604 - INFO - After joining with dim_watch_reason: 5050 rows
2025-05-07 13:50:42,605 - INFO - Deduplicating fact_df
2025-05-07 13:50:45,372 - INFO - After deduplication: 2844 rows
2025-05-07 13:50:50,911 - INFO - Null counts for fact_video_interactions: {'InteractionID': 0, 'UserID_Surrogate': 0, 'VideoID_Surrogate': 0, 'PlatformID': 0, 'DeviceTypeID': 0, 'OS_ID': 0, 'ConnectionTypeID': 0, 'TimeID': 0, 'WatchReasonID': 0, 'TimeSpentOnVideo': 0, 'TotalTimeSpent': 0, 'Engagement': 0, 'NumberOfSessions': 0, 'NumberOfVideosWatched': 0, 'ScrollRate': 0, 'ImportanceScore': 0, 'ProductivityLoss': 0, 'Satisfaction': 0, 'SelfControl': 0, 'AddictionLevel': 0, 'IngestionTimestamp': 0}
2025-05-07 13:50:51,596 - INFO - Prepared fact_video_interactions: 2844 rows
2025-05-07 13:50:51,597 - INFO - fact_df schema: StructType([StructField('InteractionID', StringType(), False), StructField('UserID_Surrogate', StringType(), True), StructField('VideoID_Surrogate', StringType(), True), StructField('PlatformID', StringType(), True), StructField('DeviceTypeID', StringType(), True), StructField('OS_ID', StringType(), True), StructField('ConnectionTypeID', StringType(), True), StructField('TimeID', StringType(), True), StructField('WatchReasonID', StringType(), True), StructField('TimeSpentOnVideo', LongType(), True), StructField('TotalTimeSpent', LongType(), True), StructField('Engagement', FloatType(), True), StructField('NumberOfSessions', LongType(), True), StructField('NumberOfVideosWatched', LongType(), True), StructField('ScrollRate', FloatType(), True), StructField('ImportanceScore', FloatType(), True), StructField('ProductivityLoss', FloatType(), True), StructField('Satisfaction', FloatType(), True), StructField('SelfControl', FloatType(), True), StructField('AddictionLevel', FloatType(), True), StructField('IngestionTimestamp', TimestampType(), True)])
2025-05-07 13:50:53,597 - INFO - Writing to PUBLIC.FACT_VIDEO_INTERACTIONS with options: {'sfURL': 'uxctays-kqc58183.snowflakecomputing.com', 'sfUser': 'Pranitha22', 'sfPassword': 'Pradyumna@9999', 'sfDatabase': 'ANALYTICS', 'sfSchema': 'PUBLIC', 'sfWarehouse': 'COMPUTE_WHI', 'sfRole': 'ACCOUNTADMIN'}
2025-05-07 13:51:02,254 - INFO - Successfully wrote fact_video_interactions: 2844 rows
2025-05-07 13:51:02,682 - INFO - Spark session stopped
2025-05-07 13:51:02,685 - INFO - Closing down clientserver connection
2025-05-07 14:07:13,556 - INFO - Initializing Spark session
2025-05-07 14:07:19,080 - INFO - Snowflake connection options: {'sfURL': 'uxctays-kqc58183.snowflakecomputing.com', 'sfUser': 'Pranitha22', 'sfPassword': 'Pradyumna@9999', 'sfDatabase': 'ANALYTICS', 'sfSchema': 'PUBLIC', 'sfWarehouse': 'COMPUTE_WHI', 'sfRole': 'ACCOUNTADMIN'}
2025-05-07 14:07:19,081 - INFO - Reading staging data from s3a://datastreaming-analytics-1/staging/video_interactions
2025-05-07 14:07:21,897 - WARNING - Hadoop check failed for S3 path s3a://datastreaming-analytics-1/staging/video_interactions: 'JavaObject' object is not iterable
Traceback (most recent call last):
  File "/app/scripts/video_interactions_transformed.py", line 79, in check_s3_path
    file_list = [(f.getPath().getName(), f.getLen()) for f in files]
TypeError: 'JavaObject' object is not iterable

2025-05-07 14:07:23,080 - INFO - AWS SDK check for s3a://datastreaming-analytics-1/staging/video_interactions: file count: 2, files: [('staging/video_interactions/_SUCCESS', 0), ('staging/video_interactions/part-00000-86ce8a08-8a1f-4d6b-8111-b315bed37390-c000.snappy.parquet', 144894)]
2025-05-07 14:07:33,764 - INFO - Actual Parquet schema: StructType([StructField('UserID', LongType(), True), StructField('Age', LongType(), True), StructField('Gender', StringType(), True), StructField('Location', StringType(), True), StructField('Income', LongType(), True), StructField('Debt', BooleanType(), True), StructField('OwnsProperty', BooleanType(), True), StructField('Profession', StringType(), True), StructField('Demographics', StringType(), True), StructField('Platform', StringType(), True), StructField('TotalTimeSpent', LongType(), True), StructField('NumberOfSessions', LongType(), True), StructField('VideoID', LongType(), True), StructField('VideoCategory', StringType(), True), StructField('VideoLength', LongType(), True), StructField('Engagement', LongType(), True), StructField('ImportanceScore', LongType(), True), StructField('TimeSpentOnVideo', LongType(), True), StructField('NumberOfVideosWatched', LongType(), True), StructField('ScrollRate', LongType(), True), StructField('Frequency', StringType(), True), StructField('ProductivityLoss', LongType(), True), StructField('Satisfaction', LongType(), True), StructField('WatchReason', StringType(), True), StructField('DeviceType', StringType(), True), StructField('OS', StringType(), True), StructField('WatchTime', StringType(), True), StructField('SelfControl', LongType(), True), StructField('AddictionLevel', LongType(), True), StructField('CurrentActivity', StringType(), True), StructField('ConnectionType', StringType(), True), StructField('IngestionTimestamp', TimestampType(), True), StructField('AgeGroup', StringType(), True)])
2025-05-07 14:07:34,948 - INFO - Checking duplicates in video_df
2025-05-07 14:07:41,382 - INFO - Found 0 duplicate combinations in video_df
2025-05-07 14:07:44,921 - INFO - Read and deduplicated video_interactions: 2844 rows
2025-05-07 14:07:44,928 - INFO - Input schema: StructType([StructField('VideoID', LongType(), True), StructField('UserID', LongType(), True), StructField('Platform', StringType(), True), StructField('WatchTime', StringType(), True), StructField('IngestionTimestamp', TimestampType(), True), StructField('Age', LongType(), True), StructField('Gender', StringType(), True), StructField('Location', StringType(), True), StructField('Income', LongType(), True), StructField('Debt', BooleanType(), True), StructField('OwnsProperty', BooleanType(), True), StructField('Profession', StringType(), True), StructField('Demographics', StringType(), True), StructField('CurrentActivity', StringType(), True), StructField('AgeGroup', StringType(), True), StructField('VideoCategory', StringType(), True), StructField('VideoLength', LongType(), True), StructField('TimeSpentOnVideo', LongType(), True), StructField('TotalTimeSpent', LongType(), True), StructField('Engagement', LongType(), True), StructField('NumberOfSessions', LongType(), True), StructField('NumberOfVideosWatched', LongType(), True), StructField('ScrollRate', LongType(), True), StructField('ImportanceScore', LongType(), True), StructField('ProductivityLoss', LongType(), True), StructField('Satisfaction', LongType(), True), StructField('SelfControl', LongType(), True), StructField('AddictionLevel', LongType(), True), StructField('DeviceType', StringType(), True), StructField('OS', StringType(), True), StructField('ConnectionType', StringType(), True), StructField('WatchReason', StringType(), True)])
2025-05-07 14:07:45,624 - INFO - Testing access to Snowflake tables
2025-05-07 14:07:50,684 - INFO - Successfully accessed PUBLIC.DIM_TIME: 0 rows
2025-05-07 14:07:51,848 - INFO - Successfully accessed PUBLIC.DIM_USER: 0 rows
2025-05-07 14:07:53,531 - INFO - Successfully accessed PUBLIC.DIM_VIDEO: 0 rows
2025-05-07 14:07:55,205 - INFO - Successfully accessed PUBLIC.DIM_PLATFORM: 0 rows
2025-05-07 14:07:56,714 - INFO - Successfully accessed PUBLIC.DIM_DEVICE_TYPE: 0 rows
2025-05-07 14:07:58,313 - INFO - Successfully accessed PUBLIC.DIM_OS: 0 rows
2025-05-07 14:07:59,941 - INFO - Successfully accessed PUBLIC.DIM_CONNECTION_TYPE: 0 rows
2025-05-07 14:08:01,434 - INFO - Successfully accessed PUBLIC.DIM_WATCH_REASON: 0 rows
2025-05-07 14:08:01,786 - INFO - Creating dim_time
2025-05-07 14:08:04,307 - INFO - Found 1842 new times to add to dim_time
2025-05-07 14:08:04,320 - INFO - missing_time_df schema: StructType([StructField('TimeID', StringType(), False), StructField('WatchTime', StringType(), False), StructField('Hour', LongType(), True)])
2025-05-07 14:08:10,922 - INFO - Successfully appended to dim_time: 0 rows
2025-05-07 14:08:10,944 - INFO - Creating dim_user
2025-05-07 14:08:12,380 - INFO - Found 2844 new users to add to dim_user
2025-05-07 14:08:12,383 - INFO - missing_user_df schema: StructType([StructField('User_S_ID', StringType(), True), StructField('UserID', StringType(), True), StructField('Age', LongType(), True), StructField('Gender', StringType(), True), StructField('Location', StringType(), True), StructField('Income', LongType(), True), StructField('Debt', BooleanType(), True), StructField('OwnsProperty', BooleanType(), True), StructField('Profession', StringType(), True), StructField('Demographics', StringType(), True), StructField('CurrentActivity', StringType(), True), StructField('AgeGroup', StringType(), True)])
2025-05-07 14:08:16,805 - INFO - Successfully appended to dim_user: 2844 rows
2025-05-07 14:08:16,808 - INFO - Creating dim_video
2025-05-07 14:08:17,604 - INFO - Prepared dim_video: 2842 rows
2025-05-07 14:08:17,608 - INFO - video_dim_df schema: StructType([StructField('Video_S_ID', StringType(), True), StructField('VideoID', LongType(), True), StructField('VideoCategory', StringType(), True), StructField('VideoLength', LongType(), True), StructField('TimeSpentCategory', StringType(), False)])
2025-05-07 14:08:21,066 - INFO - Successfully wrote dim_video: 2842 rows
2025-05-07 14:08:21,067 - INFO - Creating dim_platform
2025-05-07 14:08:21,934 - INFO - Found 4 new platforms to add to dim_platform
2025-05-07 14:08:21,947 - INFO - missing_platform_df schema: StructType([StructField('PlatformID', StringType(), True), StructField('Platform', StringType(), True)])
2025-05-07 14:08:25,411 - INFO - Successfully appended to dim_platform: 4 rows
2025-05-07 14:08:25,413 - INFO - Creating dim_device_type
2025-05-07 14:08:26,316 - INFO - Found 3 new device types to add to dim_device_type
2025-05-07 14:08:26,338 - INFO - missing_device_type_df schema: StructType([StructField('DeviceTypeID', StringType(), True), StructField('DeviceType', StringType(), True)])
2025-05-07 14:08:29,483 - INFO - Successfully appended to dim_device_type: 3 rows
2025-05-07 14:08:29,484 - INFO - Creating dim_os
2025-05-07 14:08:30,141 - INFO - Found 4 new OS to add to dim_os
2025-05-07 14:08:30,143 - INFO - missing_os_df schema: StructType([StructField('OS_ID', StringType(), True), StructField('OS', StringType(), True)])
2025-05-07 14:08:32,956 - INFO - Successfully appended to dim_os: 4 rows
2025-05-07 14:08:32,959 - INFO - Creating dim_connection_type
2025-05-07 14:08:33,677 - INFO - Found 2 new connection types to add to dim_connection_type
2025-05-07 14:08:33,680 - INFO - missing_connection_type_df schema: StructType([StructField('ConnectionTypeID', StringType(), True), StructField('ConnectionType', StringType(), True)])
2025-05-07 14:08:36,963 - INFO - Successfully appended to dim_connection_type: 2 rows
2025-05-07 14:08:36,965 - INFO - Creating dim_watch_reason
2025-05-07 14:08:35,259 - INFO - Found 4 new watch reasons to add to dim_watch_reason
2025-05-07 14:08:35,263 - INFO - missing_watch_reason_df schema: StructType([StructField('WatchReasonID', StringType(), True), StructField('WatchReason', StringType(), True)])
2025-05-07 14:08:38,954 - INFO - Successfully appended to dim_watch_reason: 4 rows
2025-05-07 14:08:38,957 - INFO - Creating fact_video_interactions
2025-05-07 14:08:39,048 - INFO - Checking duplicates in fact_df before joins
2025-05-07 14:08:39,944 - INFO - Found 0 duplicate InteractionIDs in fact_df
2025-05-07 14:08:41,783 - INFO - dim_user_df schema: StructType([StructField('User_S_ID', StringType(), True), StructField('UserID', StringType(), True)])
2025-05-07 14:08:42,705 - INFO - After joining with dim_user: 2844 rows
2025-05-07 14:08:43,914 - INFO - dim_video_df schema: StructType([StructField('Video_S_ID', StringType(), True), StructField('VideoID', DecimalType(38,0), True)])
2025-05-07 14:08:47,220 - INFO - After joining with dim_video: 5050 rows
2025-05-07 14:08:47,624 - INFO - dim_platform_df schema: StructType([StructField('PlatformID', StringType(), True), StructField('Platform', StringType(), True)])
2025-05-07 14:08:50,393 - INFO - After joining with dim_platform: 5050 rows
2025-05-07 14:08:51,272 - INFO - dim_device_type_df schema: StructType([StructField('DeviceTypeID', StringType(), True), StructField('DeviceType', StringType(), True)])
2025-05-07 14:08:55,260 - INFO - After joining with dim_device_type: 5050 rows
2025-05-07 14:08:55,728 - INFO - dim_os_df schema: StructType([StructField('OS_ID', StringType(), True), StructField('OS', StringType(), True)])
2025-05-07 14:08:58,704 - INFO - After joining with dim_os: 5050 rows
2025-05-07 14:08:59,091 - INFO - dim_connection_type_df schema: StructType([StructField('ConnectionTypeID', StringType(), True), StructField('ConnectionType', StringType(), True)])
2025-05-07 14:09:02,822 - INFO - After joining with dim_connection_type: 5050 rows
2025-05-07 14:09:03,916 - INFO - dim_time_df schema: StructType([StructField('TimeID', StringType(), True), StructField('WatchTime', StringType(), True)])
2025-05-07 14:09:06,489 - INFO - After joining with dim_time: 5050 rows
2025-05-07 14:09:07,017 - INFO - dim_watch_reason_df schema: StructType([StructField('WatchReasonID', StringType(), True), StructField('WatchReason', StringType(), True)])
2025-05-07 14:09:12,235 - INFO - After joining with dim_watch_reason: 5050 rows
2025-05-07 14:09:12,241 - INFO - Deduplicating fact_df
2025-05-07 14:09:16,891 - INFO - After deduplication: 2844 rows
2025-05-07 14:09:28,236 - INFO - Null counts for fact_video_interactions: {'InteractionID': 0, 'UserID_Surrogate': 0, 'VideoID_Surrogate': 0, 'PlatformID': 0, 'DeviceTypeID': 0, 'OS_ID': 0, 'ConnectionTypeID': 0, 'TimeID': 0, 'WatchReasonID': 0, 'TimeSpentOnVideo': 0, 'TotalTimeSpent': 0, 'Engagement': 0, 'NumberOfSessions': 0, 'NumberOfVideosWatched': 0, 'ScrollRate': 0, 'ImportanceScore': 0, 'ProductivityLoss': 0, 'Satisfaction': 0, 'SelfControl': 0, 'AddictionLevel': 0, 'IngestionTimestamp': 0}
2025-05-07 14:09:35,664 - INFO - Prepared fact_video_interactions: 2844 rows
2025-05-07 14:09:35,671 - INFO - fact_df schema: StructType([StructField('InteractionID', StringType(), False), StructField('UserID_Surrogate', StringType(), True), StructField('VideoID_Surrogate', StringType(), True), StructField('PlatformID', StringType(), True), StructField('DeviceTypeID', StringType(), True), StructField('OS_ID', StringType(), True), StructField('ConnectionTypeID', StringType(), True), StructField('TimeID', StringType(), True), StructField('WatchReasonID', StringType(), True), StructField('TimeSpentOnVideo', LongType(), True), StructField('TotalTimeSpent', LongType(), True), StructField('Engagement', FloatType(), True), StructField('NumberOfSessions', LongType(), True), StructField('NumberOfVideosWatched', LongType(), True), StructField('ScrollRate', FloatType(), True), StructField('ImportanceScore', FloatType(), True), StructField('ProductivityLoss', FloatType(), True), StructField('Satisfaction', FloatType(), True), StructField('SelfControl', FloatType(), True), StructField('AddictionLevel', FloatType(), True), StructField('IngestionTimestamp', TimestampType(), True)])
2025-05-07 14:09:41,530 - INFO - Writing to PUBLIC.FACT_VIDEO_INTERACTIONS with options: {'sfURL': 'uxctays-kqc58183.snowflakecomputing.com', 'sfUser': 'Pranitha22', 'sfPassword': 'Pradyumna@9999', 'sfDatabase': 'ANALYTICS', 'sfSchema': 'PUBLIC', 'sfWarehouse': 'COMPUTE_WHI', 'sfRole': 'ACCOUNTADMIN'}
2025-05-07 14:09:54,282 - INFO - Successfully wrote fact_video_interactions: 2844 rows
2025-05-07 14:09:54,998 - INFO - Spark session stopped
2025-05-07 14:09:55,002 - INFO - Closing down clientserver connection
2025-05-07 14:16:47,027 - INFO - Initializing Spark session
2025-05-07 14:16:52,148 - INFO - Snowflake connection options: {'sfURL': 'uxctays-kqc58183.snowflakecomputing.com', 'sfUser': 'Pranitha22', 'sfPassword': 'Pradyumna@9999', 'sfDatabase': 'ANALYTICS', 'sfSchema': 'PUBLIC', 'sfWarehouse': 'COMPUTE_WHI', 'sfRole': 'ACCOUNTADMIN'}
2025-05-07 14:16:52,149 - INFO - Reading staging data from s3a://datastreaming-analytics-1/staging/video_interactions
2025-05-07 14:16:56,853 - WARNING - Hadoop check failed for S3 path s3a://datastreaming-analytics-1/staging/video_interactions: 'JavaObject' object is not iterable
Traceback (most recent call last):
  File "/app/scripts/video_interactions_transformed.py", line 79, in check_s3_path
    file_list = [(f.getPath().getName(), f.getLen()) for f in files]
TypeError: 'JavaObject' object is not iterable

2025-05-07 14:16:58,363 - INFO - AWS SDK check for s3a://datastreaming-analytics-1/staging/video_interactions: file count: 2, files: [('staging/video_interactions/_SUCCESS', 0), ('staging/video_interactions/part-00000-86ce8a08-8a1f-4d6b-8111-b315bed37390-c000.snappy.parquet', 144894)]
2025-05-07 14:17:11,455 - INFO - Actual Parquet schema: StructType([StructField('UserID', LongType(), True), StructField('Age', LongType(), True), StructField('Gender', StringType(), True), StructField('Location', StringType(), True), StructField('Income', LongType(), True), StructField('Debt', BooleanType(), True), StructField('OwnsProperty', BooleanType(), True), StructField('Profession', StringType(), True), StructField('Demographics', StringType(), True), StructField('Platform', StringType(), True), StructField('TotalTimeSpent', LongType(), True), StructField('NumberOfSessions', LongType(), True), StructField('VideoID', LongType(), True), StructField('VideoCategory', StringType(), True), StructField('VideoLength', LongType(), True), StructField('Engagement', LongType(), True), StructField('ImportanceScore', LongType(), True), StructField('TimeSpentOnVideo', LongType(), True), StructField('NumberOfVideosWatched', LongType(), True), StructField('ScrollRate', LongType(), True), StructField('Frequency', StringType(), True), StructField('ProductivityLoss', LongType(), True), StructField('Satisfaction', LongType(), True), StructField('WatchReason', StringType(), True), StructField('DeviceType', StringType(), True), StructField('OS', StringType(), True), StructField('WatchTime', StringType(), True), StructField('SelfControl', LongType(), True), StructField('AddictionLevel', LongType(), True), StructField('CurrentActivity', StringType(), True), StructField('ConnectionType', StringType(), True), StructField('IngestionTimestamp', TimestampType(), True), StructField('AgeGroup', StringType(), True)])
2025-05-07 14:17:12,876 - INFO - Checking duplicates in video_df
2025-05-07 14:17:17,141 - INFO - Found 0 duplicate combinations in video_df
2025-05-07 14:17:26,863 - INFO - Read and deduplicated video_interactions: 2844 rows
2025-05-07 14:17:26,889 - INFO - Input schema: StructType([StructField('VideoID', LongType(), True), StructField('UserID', LongType(), True), StructField('Platform', StringType(), True), StructField('WatchTime', StringType(), True), StructField('IngestionTimestamp', TimestampType(), True), StructField('Age', LongType(), True), StructField('Gender', StringType(), True), StructField('Location', StringType(), True), StructField('Income', LongType(), True), StructField('Debt', BooleanType(), True), StructField('OwnsProperty', BooleanType(), True), StructField('Profession', StringType(), True), StructField('Demographics', StringType(), True), StructField('CurrentActivity', StringType(), True), StructField('AgeGroup', StringType(), True), StructField('VideoCategory', StringType(), True), StructField('VideoLength', LongType(), True), StructField('TimeSpentOnVideo', LongType(), True), StructField('TotalTimeSpent', LongType(), True), StructField('Engagement', LongType(), True), StructField('NumberOfSessions', LongType(), True), StructField('NumberOfVideosWatched', LongType(), True), StructField('ScrollRate', LongType(), True), StructField('ImportanceScore', LongType(), True), StructField('ProductivityLoss', LongType(), True), StructField('Satisfaction', LongType(), True), StructField('SelfControl', LongType(), True), StructField('AddictionLevel', LongType(), True), StructField('DeviceType', StringType(), True), StructField('OS', StringType(), True), StructField('ConnectionType', StringType(), True), StructField('WatchReason', StringType(), True)])
2025-05-07 14:17:28,768 - INFO - Testing access to Snowflake tables
2025-05-07 14:17:34,496 - INFO - Successfully accessed PUBLIC.DIM_TIME: 0 rows
2025-05-07 14:17:36,203 - INFO - Successfully accessed PUBLIC.DIM_USER: 0 rows
2025-05-07 14:17:38,632 - INFO - Successfully accessed PUBLIC.DIM_VIDEO: 0 rows
2025-05-07 14:17:41,018 - INFO - Successfully accessed PUBLIC.DIM_PLATFORM: 0 rows
2025-05-07 14:17:42,996 - INFO - Successfully accessed PUBLIC.DIM_DEVICE_TYPE: 0 rows
2025-05-07 14:17:42,281 - INFO - Successfully accessed PUBLIC.DIM_OS: 0 rows
2025-05-07 14:17:44,118 - INFO - Successfully accessed PUBLIC.DIM_CONNECTION_TYPE: 0 rows
2025-05-07 14:17:46,204 - INFO - Successfully accessed PUBLIC.DIM_WATCH_REASON: 0 rows
2025-05-07 14:17:47,118 - INFO - Creating dim_time
2025-05-07 14:17:49,963 - INFO - Found 1842 new times to add to dim_time
2025-05-07 14:17:49,974 - INFO - missing_time_df schema: StructType([StructField('TimeID', StringType(), False), StructField('WatchTime', StringType(), False), StructField('Hour', LongType(), True)])
2025-05-07 14:18:00,292 - INFO - Successfully appended to dim_time: 0 rows
2025-05-07 14:18:00,317 - INFO - Creating dim_user
2025-05-07 14:18:03,424 - INFO - Found 2844 new users to add to dim_user
2025-05-07 14:18:03,435 - INFO - missing_user_df schema: StructType([StructField('User_S_ID', StringType(), True), StructField('UserID', StringType(), True), StructField('Age', LongType(), True), StructField('Gender', StringType(), True), StructField('Location', StringType(), True), StructField('Income', LongType(), True), StructField('Debt', BooleanType(), True), StructField('OwnsProperty', BooleanType(), True), StructField('Profession', StringType(), True), StructField('Demographics', StringType(), True), StructField('CurrentActivity', StringType(), True), StructField('AgeGroup', StringType(), True)])
2025-05-07 14:18:08,325 - INFO - Successfully appended to dim_user: 2844 rows
2025-05-07 14:18:08,331 - INFO - Creating dim_video
2025-05-07 14:18:09,247 - INFO - Prepared dim_video: 2842 rows
2025-05-07 14:18:09,254 - INFO - video_dim_df schema: StructType([StructField('Video_S_ID', StringType(), True), StructField('VideoID', LongType(), True), StructField('VideoCategory', StringType(), True), StructField('VideoLength', LongType(), True), StructField('TimeSpentCategory', StringType(), False)])
2025-05-07 14:18:13,218 - INFO - Successfully wrote dim_video: 2842 rows
2025-05-07 14:18:13,219 - INFO - Creating dim_platform
2025-05-07 14:18:15,209 - INFO - Found 4 new platforms to add to dim_platform
2025-05-07 14:18:15,222 - INFO - missing_platform_df schema: StructType([StructField('PlatformID', StringType(), True), StructField('Platform', StringType(), True)])
2025-05-07 14:18:20,610 - INFO - Successfully appended to dim_platform: 4 rows
2025-05-07 14:18:20,613 - INFO - Creating dim_device_type
2025-05-07 14:18:21,732 - INFO - Found 3 new device types to add to dim_device_type
2025-05-07 14:18:21,745 - INFO - missing_device_type_df schema: StructType([StructField('DeviceTypeID', StringType(), True), StructField('DeviceType', StringType(), True)])
2025-05-07 14:18:27,247 - INFO - Successfully appended to dim_device_type: 3 rows
2025-05-07 14:18:27,249 - INFO - Creating dim_os
2025-05-07 14:18:28,126 - INFO - Found 4 new OS to add to dim_os
2025-05-07 14:18:28,129 - INFO - missing_os_df schema: StructType([StructField('OS_ID', StringType(), True), StructField('OS', StringType(), True)])
2025-05-07 14:18:34,886 - INFO - Successfully appended to dim_os: 4 rows
2025-05-07 14:18:34,889 - INFO - Creating dim_connection_type
2025-05-07 14:18:36,310 - INFO - Found 2 new connection types to add to dim_connection_type
2025-05-07 14:18:36,316 - INFO - missing_connection_type_df schema: StructType([StructField('ConnectionTypeID', StringType(), True), StructField('ConnectionType', StringType(), True)])
2025-05-07 14:18:37,936 - INFO - Successfully appended to dim_connection_type: 2 rows
2025-05-07 14:18:37,938 - INFO - Creating dim_watch_reason
2025-05-07 14:18:39,365 - INFO - Found 4 new watch reasons to add to dim_watch_reason
2025-05-07 14:18:39,377 - INFO - missing_watch_reason_df schema: StructType([StructField('WatchReasonID', StringType(), True), StructField('WatchReason', StringType(), True)])
2025-05-07 14:18:43,344 - INFO - Successfully appended to dim_watch_reason: 4 rows
2025-05-07 14:18:43,346 - INFO - Creating fact_video_interactions
2025-05-07 14:18:43,438 - INFO - Checking duplicates in fact_df before joins
2025-05-07 14:18:44,946 - INFO - Found 0 duplicate InteractionIDs in fact_df
2025-05-07 14:18:47,557 - INFO - dim_user_df schema: StructType([StructField('User_S_ID', StringType(), True), StructField('UserID', StringType(), True)])
2025-05-07 14:18:49,186 - INFO - After joining with dim_user: 2844 rows
2025-05-07 14:18:50,299 - INFO - dim_video_df schema: StructType([StructField('Video_S_ID', StringType(), True), StructField('VideoID', DecimalType(38,0), True)])
2025-05-07 14:18:53,803 - INFO - After joining with dim_video: 5050 rows
2025-05-07 14:18:54,270 - INFO - dim_platform_df schema: StructType([StructField('PlatformID', StringType(), True), StructField('Platform', StringType(), True)])
2025-05-07 14:18:58,315 - INFO - After joining with dim_platform: 5050 rows
2025-05-07 14:18:58,700 - INFO - dim_device_type_df schema: StructType([StructField('DeviceTypeID', StringType(), True), StructField('DeviceType', StringType(), True)])
2025-05-07 14:19:02,759 - INFO - After joining with dim_device_type: 5050 rows
2025-05-07 14:19:03,791 - INFO - dim_os_df schema: StructType([StructField('OS_ID', StringType(), True), StructField('OS', StringType(), True)])
2025-05-07 14:19:05,412 - INFO - After joining with dim_os: 5050 rows
2025-05-07 14:19:05,936 - INFO - dim_connection_type_df schema: StructType([StructField('ConnectionTypeID', StringType(), True), StructField('ConnectionType', StringType(), True)])
2025-05-07 14:19:11,917 - INFO - After joining with dim_connection_type: 5050 rows
2025-05-07 14:19:13,144 - INFO - dim_time_df schema: StructType([StructField('TimeID', StringType(), True), StructField('WatchTime', StringType(), True)])
2025-05-07 14:19:18,968 - INFO - After joining with dim_time: 5050 rows
2025-05-07 14:19:19,595 - INFO - dim_watch_reason_df schema: StructType([StructField('WatchReasonID', StringType(), True), StructField('WatchReason', StringType(), True)])
2025-05-07 14:19:25,252 - INFO - After joining with dim_watch_reason: 5050 rows
2025-05-07 14:19:25,258 - INFO - Deduplicating fact_df
2025-05-07 14:19:32,617 - INFO - After deduplication: 2844 rows
2025-05-07 14:19:43,221 - INFO - Null counts for fact_video_interactions: {'InteractionID': 0, 'UserID_Surrogate': 0, 'VideoID_Surrogate': 0, 'PlatformID': 0, 'DeviceTypeID': 0, 'OS_ID': 0, 'ConnectionTypeID': 0, 'TimeID': 0, 'WatchReasonID': 0, 'TimeSpentOnVideo': 0, 'TotalTimeSpent': 0, 'Engagement': 0, 'NumberOfSessions': 0, 'NumberOfVideosWatched': 0, 'ScrollRate': 0, 'ImportanceScore': 0, 'ProductivityLoss': 0, 'Satisfaction': 0, 'SelfControl': 0, 'AddictionLevel': 0, 'IngestionTimestamp': 0}
2025-05-07 14:19:56,620 - INFO - Prepared fact_video_interactions: 2844 rows
2025-05-07 14:19:56,622 - INFO - fact_df schema: StructType([StructField('InteractionID', StringType(), False), StructField('UserID_Surrogate', StringType(), True), StructField('VideoID_Surrogate', StringType(), True), StructField('PlatformID', StringType(), True), StructField('DeviceTypeID', StringType(), True), StructField('OS_ID', StringType(), True), StructField('ConnectionTypeID', StringType(), True), StructField('TimeID', StringType(), True), StructField('WatchReasonID', StringType(), True), StructField('TimeSpentOnVideo', LongType(), True), StructField('TotalTimeSpent', LongType(), True), StructField('Engagement', FloatType(), True), StructField('NumberOfSessions', LongType(), True), StructField('NumberOfVideosWatched', LongType(), True), StructField('ScrollRate', FloatType(), True), StructField('ImportanceScore', FloatType(), True), StructField('ProductivityLoss', FloatType(), True), StructField('Satisfaction', FloatType(), True), StructField('SelfControl', FloatType(), True), StructField('AddictionLevel', FloatType(), True), StructField('IngestionTimestamp', TimestampType(), True)])
2025-05-07 14:20:02,053 - INFO - Writing to PUBLIC.FACT_VIDEO_INTERACTIONS with options: {'sfURL': 'uxctays-kqc58183.snowflakecomputing.com', 'sfUser': 'Pranitha22', 'sfPassword': 'Pradyumna@9999', 'sfDatabase': 'ANALYTICS', 'sfSchema': 'PUBLIC', 'sfWarehouse': 'COMPUTE_WHI', 'sfRole': 'ACCOUNTADMIN'}
2025-05-07 14:20:18,381 - INFO - Successfully wrote fact_video_interactions: 2844 rows
2025-05-07 14:20:19,073 - INFO - Spark session stopped
2025-05-07 14:20:19,078 - INFO - Closing down clientserver connection
2025-05-07 14:21:30,108 - INFO - Initializing Spark session
2025-05-07 14:21:34,751 - INFO - Snowflake connection options: {'sfURL': 'uxctays-kqc58183.snowflakecomputing.com', 'sfUser': 'Pranitha22', 'sfPassword': 'Pradyumna@9999', 'sfDatabase': 'ANALYTICS', 'sfSchema': 'PUBLIC', 'sfWarehouse': 'COMPUTE_WHI', 'sfRole': 'ACCOUNTADMIN'}
2025-05-07 14:21:34,752 - INFO - Reading staging data from s3a://datastreaming-analytics-1/staging/video_interactions
2025-05-07 14:21:37,845 - WARNING - Hadoop check failed for S3 path s3a://datastreaming-analytics-1/staging/video_interactions: 'JavaObject' object is not iterable
Traceback (most recent call last):
  File "/app/scripts/video_interactions_transformed.py", line 79, in check_s3_path
    file_list = [(f.getPath().getName(), f.getLen()) for f in files]
TypeError: 'JavaObject' object is not iterable

2025-05-07 14:21:39,606 - INFO - AWS SDK check for s3a://datastreaming-analytics-1/staging/video_interactions: file count: 2, files: [('staging/video_interactions/_SUCCESS', 0), ('staging/video_interactions/part-00000-86ce8a08-8a1f-4d6b-8111-b315bed37390-c000.snappy.parquet', 144894)]
2025-05-07 14:21:48,396 - INFO - Actual Parquet schema: StructType([StructField('UserID', LongType(), True), StructField('Age', LongType(), True), StructField('Gender', StringType(), True), StructField('Location', StringType(), True), StructField('Income', LongType(), True), StructField('Debt', BooleanType(), True), StructField('OwnsProperty', BooleanType(), True), StructField('Profession', StringType(), True), StructField('Demographics', StringType(), True), StructField('Platform', StringType(), True), StructField('TotalTimeSpent', LongType(), True), StructField('NumberOfSessions', LongType(), True), StructField('VideoID', LongType(), True), StructField('VideoCategory', StringType(), True), StructField('VideoLength', LongType(), True), StructField('Engagement', LongType(), True), StructField('ImportanceScore', LongType(), True), StructField('TimeSpentOnVideo', LongType(), True), StructField('NumberOfVideosWatched', LongType(), True), StructField('ScrollRate', LongType(), True), StructField('Frequency', StringType(), True), StructField('ProductivityLoss', LongType(), True), StructField('Satisfaction', LongType(), True), StructField('WatchReason', StringType(), True), StructField('DeviceType', StringType(), True), StructField('OS', StringType(), True), StructField('WatchTime', StringType(), True), StructField('SelfControl', LongType(), True), StructField('AddictionLevel', LongType(), True), StructField('CurrentActivity', StringType(), True), StructField('ConnectionType', StringType(), True), StructField('IngestionTimestamp', TimestampType(), True), StructField('AgeGroup', StringType(), True)])
2025-05-07 14:21:49,358 - INFO - Checking duplicates in video_df
2025-05-07 14:21:56,102 - INFO - Found 0 duplicate combinations in video_df
2025-05-07 14:22:04,118 - INFO - Read and deduplicated video_interactions: 2844 rows
2025-05-07 14:22:04,124 - INFO - Input schema: StructType([StructField('VideoID', LongType(), True), StructField('UserID', LongType(), True), StructField('Platform', StringType(), True), StructField('WatchTime', StringType(), True), StructField('IngestionTimestamp', TimestampType(), True), StructField('Age', LongType(), True), StructField('Gender', StringType(), True), StructField('Location', StringType(), True), StructField('Income', LongType(), True), StructField('Debt', BooleanType(), True), StructField('OwnsProperty', BooleanType(), True), StructField('Profession', StringType(), True), StructField('Demographics', StringType(), True), StructField('CurrentActivity', StringType(), True), StructField('AgeGroup', StringType(), True), StructField('VideoCategory', StringType(), True), StructField('VideoLength', LongType(), True), StructField('TimeSpentOnVideo', LongType(), True), StructField('TotalTimeSpent', LongType(), True), StructField('Engagement', LongType(), True), StructField('NumberOfSessions', LongType(), True), StructField('NumberOfVideosWatched', LongType(), True), StructField('ScrollRate', LongType(), True), StructField('ImportanceScore', LongType(), True), StructField('ProductivityLoss', LongType(), True), StructField('Satisfaction', LongType(), True), StructField('SelfControl', LongType(), True), StructField('AddictionLevel', LongType(), True), StructField('DeviceType', StringType(), True), StructField('OS', StringType(), True), StructField('ConnectionType', StringType(), True), StructField('WatchReason', StringType(), True)])
2025-05-07 14:22:04,634 - INFO - Testing access to Snowflake tables
2025-05-07 14:22:09,581 - INFO - Successfully accessed PUBLIC.DIM_TIME: 0 rows
2025-05-07 14:22:10,922 - INFO - Successfully accessed PUBLIC.DIM_USER: 0 rows
2025-05-07 14:22:13,037 - INFO - Successfully accessed PUBLIC.DIM_VIDEO: 0 rows
2025-05-07 14:22:15,232 - INFO - Successfully accessed PUBLIC.DIM_PLATFORM: 0 rows
2025-05-07 14:22:14,298 - INFO - Successfully accessed PUBLIC.DIM_DEVICE_TYPE: 0 rows
2025-05-07 14:22:16,446 - INFO - Successfully accessed PUBLIC.DIM_OS: 0 rows
2025-05-07 14:22:17,436 - INFO - Successfully accessed PUBLIC.DIM_CONNECTION_TYPE: 0 rows
2025-05-07 14:22:18,854 - INFO - Successfully accessed PUBLIC.DIM_WATCH_REASON: 0 rows
2025-05-07 14:22:19,888 - INFO - Creating dim_time
2025-05-07 14:22:20,804 - INFO - New WatchTime values: 1842 rows
2025-05-07 14:22:22,995 - INFO - Found 1842 new times to add to dim_time
2025-05-07 14:22:23,001 - INFO - missing_time_df schema: StructType([StructField('TimeID', StringType(), False), StructField('WatchTime', StringType(), False), StructField('Hour', LongType(), True)])
2025-05-07 14:22:23,002 - INFO - Sample missing_time_df rows:
2025-05-07 14:22:32,233 - INFO - Successfully appended to dim_time: 0 rows
2025-05-07 14:22:32,244 - INFO - Creating dim_user
2025-05-07 14:22:33,681 - INFO - Found 2844 new users to add to dim_user
2025-05-07 14:22:33,686 - INFO - missing_user_df schema: StructType([StructField('User_S_ID', StringType(), True), StructField('UserID', StringType(), True), StructField('Age', LongType(), True), StructField('Gender', StringType(), True), StructField('Location', StringType(), True), StructField('Income', LongType(), True), StructField('Debt', BooleanType(), True), StructField('OwnsProperty', BooleanType(), True), StructField('Profession', StringType(), True), StructField('Demographics', StringType(), True), StructField('CurrentActivity', StringType(), True), StructField('AgeGroup', StringType(), True)])
2025-05-07 14:22:33,688 - INFO - Sample missing_user_df rows:
2025-05-07 14:22:39,116 - INFO - Successfully appended to dim_user: 2844 rows
2025-05-07 14:22:39,118 - INFO - Creating dim_video
2025-05-07 14:22:39,985 - INFO - Prepared dim_video: 2842 rows
2025-05-07 14:22:40,000 - INFO - video_dim_df schema: StructType([StructField('Video_S_ID', StringType(), True), StructField('VideoID', LongType(), True), StructField('VideoCategory', StringType(), True), StructField('VideoLength', LongType(), True), StructField('TimeSpentCategory', StringType(), False)])
2025-05-07 14:22:40,000 - INFO - Sample video_dim_df rows:
2025-05-07 14:22:42,067 - INFO - Successfully wrote dim_video: 2842 rows
2025-05-07 14:22:42,068 - INFO - Creating dim_platform
2025-05-07 14:22:43,493 - INFO - Found 4 new platforms to add to dim_platform
2025-05-07 14:22:43,498 - INFO - missing_platform_df schema: StructType([StructField('PlatformID', StringType(), True), StructField('Platform', StringType(), True)])
2025-05-07 14:22:43,499 - INFO - Sample missing_platform_df rows:
2025-05-07 14:22:47,874 - INFO - Successfully appended to dim_platform: 4 rows
2025-05-07 14:22:47,876 - INFO - Creating dim_device_type
2025-05-07 14:22:48,718 - INFO - Found 3 new device types to add to dim_device_type
2025-05-07 14:22:48,721 - INFO - missing_device_type_df schema: StructType([StructField('DeviceTypeID', StringType(), True), StructField('DeviceType', StringType(), True)])
2025-05-07 14:22:48,721 - INFO - Sample missing_device_type_df rows:
2025-05-07 14:22:52,713 - INFO - Successfully appended to dim_device_type: 3 rows
2025-05-07 14:22:52,716 - INFO - Creating dim_os
2025-05-07 14:22:53,762 - INFO - Found 4 new OS to add to dim_os
2025-05-07 14:22:53,771 - INFO - missing_os_df schema: StructType([StructField('OS_ID', StringType(), True), StructField('OS', StringType(), True)])
2025-05-07 14:22:53,785 - INFO - Sample missing_os_df rows:
2025-05-07 14:22:58,081 - INFO - Successfully appended to dim_os: 4 rows
2025-05-07 14:22:58,083 - INFO - Creating dim_connection_type
2025-05-07 14:22:59,338 - INFO - Found 2 new connection types to add to dim_connection_type
2025-05-07 14:22:59,342 - INFO - missing_connection_type_df schema: StructType([StructField('ConnectionTypeID', StringType(), True), StructField('ConnectionType', StringType(), True)])
2025-05-07 14:22:59,343 - INFO - Sample missing_connection_type_df rows:
2025-05-07 14:23:03,493 - INFO - Successfully appended to dim_connection_type: 2 rows
2025-05-07 14:23:03,495 - INFO - Creating dim_watch_reason
2025-05-07 14:23:04,269 - INFO - Found 4 new watch reasons to add to dim_watch_reason
2025-05-07 14:23:04,272 - INFO - missing_watch_reason_df schema: StructType([StructField('WatchReasonID', StringType(), True), StructField('WatchReason', StringType(), True)])
2025-05-07 14:23:04,273 - INFO - Sample missing_watch_reason_df rows:
2025-05-07 14:23:08,333 - INFO - Successfully appended to dim_watch_reason: 4 rows
2025-05-07 14:23:08,335 - INFO - Creating fact_video_interactions
2025-05-07 14:23:08,409 - INFO - Checking duplicates in fact_df before joins
2025-05-07 14:23:09,574 - INFO - Found 0 duplicate InteractionIDs in fact_df
2025-05-07 14:23:09,294 - INFO - dim_user_df schema: StructType([StructField('User_S_ID', StringType(), True), StructField('UserID', StringType(), True)])
2025-05-07 14:23:10,494 - INFO - After joining with dim_user: 2844 rows
2025-05-07 14:23:11,721 - INFO - dim_video_df schema: StructType([StructField('Video_S_ID', StringType(), True), StructField('VideoID', DecimalType(38,0), True)])
2025-05-07 14:23:14,929 - INFO - After joining with dim_video: 5050 rows
2025-05-07 14:23:15,361 - INFO - dim_platform_df schema: StructType([StructField('PlatformID', StringType(), True), StructField('Platform', StringType(), True)])
2025-05-07 14:23:18,551 - INFO - After joining with dim_platform: 5050 rows
2025-05-07 14:23:19,601 - INFO - dim_device_type_df schema: StructType([StructField('DeviceTypeID', StringType(), True), StructField('DeviceType', StringType(), True)])
2025-05-07 14:23:22,163 - INFO - After joining with dim_device_type: 5050 rows
2025-05-07 14:23:22,494 - INFO - dim_os_df schema: StructType([StructField('OS_ID', StringType(), True), StructField('OS', StringType(), True)])
2025-05-07 14:23:26,908 - INFO - After joining with dim_os: 5050 rows
2025-05-07 14:23:27,363 - INFO - dim_connection_type_df schema: StructType([StructField('ConnectionTypeID', StringType(), True), StructField('ConnectionType', StringType(), True)])
2025-05-07 14:23:31,958 - INFO - After joining with dim_connection_type: 5050 rows
2025-05-07 14:23:32,528 - INFO - dim_time_df schema: StructType([StructField('TimeID', StringType(), True), StructField('WatchTime', StringType(), True)])
2025-05-07 14:23:37,707 - INFO - After joining with dim_time: 5050 rows
2025-05-07 14:23:38,114 - INFO - dim_watch_reason_df schema: StructType([StructField('WatchReasonID', StringType(), True), StructField('WatchReason', StringType(), True)])
2025-05-07 14:23:40,214 - INFO - After joining with dim_watch_reason: 5050 rows
2025-05-07 14:23:40,220 - INFO - Deduplicating fact_df
2025-05-07 14:23:46,663 - INFO - After deduplication: 2844 rows
2025-05-07 14:23:57,321 - INFO - Null counts for fact_video_interactions: {'InteractionID': 0, 'UserID_Surrogate': 0, 'VideoID_Surrogate': 0, 'PlatformID': 0, 'DeviceTypeID': 0, 'OS_ID': 0, 'ConnectionTypeID': 0, 'TimeID': 0, 'WatchReasonID': 0, 'TimeSpentOnVideo': 0, 'TotalTimeSpent': 0, 'Engagement': 0, 'NumberOfSessions': 0, 'NumberOfVideosWatched': 0, 'ScrollRate': 0, 'ImportanceScore': 0, 'ProductivityLoss': 0, 'Satisfaction': 0, 'SelfControl': 0, 'AddictionLevel': 0, 'IngestionTimestamp': 0}
2025-05-07 14:24:06,472 - INFO - Prepared fact_video_interactions: 2844 rows
2025-05-07 14:24:06,474 - INFO - fact_df schema: StructType([StructField('InteractionID', StringType(), False), StructField('UserID_Surrogate', StringType(), True), StructField('VideoID_Surrogate', StringType(), True), StructField('PlatformID', StringType(), True), StructField('DeviceTypeID', StringType(), True), StructField('OS_ID', StringType(), True), StructField('ConnectionTypeID', StringType(), True), StructField('TimeID', StringType(), True), StructField('WatchReasonID', StringType(), True), StructField('TimeSpentOnVideo', LongType(), True), StructField('TotalTimeSpent', LongType(), True), StructField('Engagement', FloatType(), True), StructField('NumberOfSessions', LongType(), True), StructField('NumberOfVideosWatched', LongType(), True), StructField('ScrollRate', FloatType(), True), StructField('ImportanceScore', FloatType(), True), StructField('ProductivityLoss', FloatType(), True), StructField('Satisfaction', FloatType(), True), StructField('SelfControl', FloatType(), True), StructField('AddictionLevel', FloatType(), True), StructField('IngestionTimestamp', TimestampType(), True)])
2025-05-07 14:24:12,343 - INFO - Writing to PUBLIC.FACT_VIDEO_INTERACTIONS with options: {'sfURL': 'uxctays-kqc58183.snowflakecomputing.com', 'sfUser': 'Pranitha22', 'sfPassword': 'Pradyumna@9999', 'sfDatabase': 'ANALYTICS', 'sfSchema': 'PUBLIC', 'sfWarehouse': 'COMPUTE_WHI', 'sfRole': 'ACCOUNTADMIN'}
2025-05-07 14:24:27,401 - INFO - Successfully wrote fact_video_interactions: 2844 rows
2025-05-07 14:24:28,072 - INFO - Spark session stopped
2025-05-07 14:24:28,075 - INFO - Closing down clientserver connection
2025-05-08 15:37:46,509 - INFO - Initializing Spark session
2025-05-08 15:37:50,841 - INFO - Snowflake connection options: {'sfURL': 'uxctays-kqc58183.snowflakecomputing.com', 'sfUser': 'Pranitha22', 'sfPassword': 'Pradyumna@9999', 'sfDatabase': 'ANALYTICS', 'sfSchema': 'PUBLIC', 'sfWarehouse': 'COMPUTE_WHI', 'sfRole': 'ACCOUNTADMIN'}
2025-05-08 15:37:50,842 - INFO - Reading staging data from s3a://datastreaming-analytics-1/staging/video_interactions
2025-05-08 15:37:54,178 - WARNING - Hadoop check failed for S3 path s3a://datastreaming-analytics-1/staging/video_interactions: 'JavaObject' object is not iterable
Traceback (most recent call last):
  File "/app/scripts/video_interactions_transformed.py", line 79, in check_s3_path
    file_list = [(f.getPath().getName(), f.getLen()) for f in files]
TypeError: 'JavaObject' object is not iterable

2025-05-08 15:37:54,945 - INFO - AWS SDK check for s3a://datastreaming-analytics-1/staging/video_interactions: file count: 14, files: [('staging/video_interactions/IngestionTimestamp=2025-05-08 14%3A55%3A00.022/part-00000-39756958-d5f9-4f9a-b9ba-7791b677e2bc.c000.snappy.parquet', 8591), ('staging/video_interactions/IngestionTimestamp=2025-05-08 14%3A55%3A36.165/part-00000-bf16fbf8-0725-4308-b407-6f2918dd0d6e.c000.snappy.parquet', 9640), ('staging/video_interactions/IngestionTimestamp=2025-05-08 14%3A55%3A58.56/part-00000-60e9926e-c6e0-4e44-b5a9-28e7da430260.c000.snappy.parquet', 9237), ('staging/video_interactions/IngestionTimestamp=2025-05-08 14%3A56%3A17.576/part-00000-280742af-2d52-4d34-a5ec-e3d3a476037b.c000.snappy.parquet', 8856), ('staging/video_interactions/IngestionTimestamp=2025-05-08 14%3A56%3A36.755/part-00000-445fe1a7-4bf7-47fb-8030-107c778292e2.c000.snappy.parquet', 8997), ('staging/video_interactions/_SUCCESS', 0), ('staging/video_interactions/_spark_metadata/0', 2), ('staging/video_interactions/_spark_metadata/1', 309), ('staging/video_interactions/_spark_metadata/2', 309), ('staging/video_interactions/_spark_metadata/3', 308), ('staging/video_interactions/_spark_metadata/4', 309), ('staging/video_interactions/_spark_metadata/5', 309), ('staging/video_interactions/_spark_metadata/6', 2), ('staging/video_interactions/part-00000-86ce8a08-8a1f-4d6b-8111-b315bed37390-c000.snappy.parquet', 144894)]
2025-05-08 15:38:06,321 - INFO - Actual Parquet schema: StructType([StructField('UserID', LongType(), True), StructField('Age', LongType(), False), StructField('Gender', StringType(), False), StructField('Location', StringType(), False), StructField('Income', LongType(), False), StructField('Debt', BooleanType(), False), StructField('OwnsProperty', BooleanType(), False), StructField('Profession', StringType(), False), StructField('Demographics', StringType(), False), StructField('Platform', StringType(), False), StructField('TotalTimeSpent', LongType(), True), StructField('NumberOfSessions', LongType(), False), StructField('VideoID', LongType(), True), StructField('VideoCategory', StringType(), False), StructField('VideoLength', LongType(), False), StructField('Engagement', LongType(), False), StructField('ImportanceScore', LongType(), False), StructField('TimeSpentOnVideo', LongType(), False), StructField('NumberOfVideosWatched', LongType(), False), StructField('ScrollRate', LongType(), False), StructField('Frequency', StringType(), False), StructField('ProductivityLoss', LongType(), False), StructField('Satisfaction', LongType(), False), StructField('WatchReason', StringType(), False), StructField('DeviceType', StringType(), False), StructField('OS', StringType(), False), StructField('WatchTime', StringType(), False), StructField('SelfControl', LongType(), False), StructField('AddictionLevel', LongType(), False), StructField('CurrentActivity', StringType(), False), StructField('ConnectionType', StringType(), False), StructField('AgeGroup', StringType(), False), StructField('IngestionTimestamp', StringType(), True)])
2025-05-08 15:38:08,520 - INFO - Checking duplicates in video_df
2025-05-08 15:38:15,300 - INFO - Found 0 duplicate combinations in video_df
2025-05-08 15:38:22,316 - INFO - Read and deduplicated video_interactions: 13 rows
2025-05-08 15:38:22,321 - INFO - Input schema: StructType([StructField('VideoID', LongType(), True), StructField('UserID', LongType(), True), StructField('Platform', StringType(), True), StructField('WatchTime', StringType(), True), StructField('IngestionTimestamp', TimestampType(), True), StructField('Age', LongType(), True), StructField('Gender', StringType(), True), StructField('Location', StringType(), True), StructField('Income', LongType(), True), StructField('Debt', BooleanType(), True), StructField('OwnsProperty', BooleanType(), True), StructField('Profession', StringType(), True), StructField('Demographics', StringType(), True), StructField('CurrentActivity', StringType(), True), StructField('AgeGroup', StringType(), True), StructField('VideoCategory', StringType(), True), StructField('VideoLength', LongType(), True), StructField('TimeSpentOnVideo', LongType(), True), StructField('TotalTimeSpent', LongType(), True), StructField('Engagement', LongType(), True), StructField('NumberOfSessions', LongType(), True), StructField('NumberOfVideosWatched', LongType(), True), StructField('ScrollRate', LongType(), True), StructField('ImportanceScore', LongType(), True), StructField('ProductivityLoss', LongType(), True), StructField('Satisfaction', LongType(), True), StructField('SelfControl', LongType(), True), StructField('AddictionLevel', LongType(), True), StructField('DeviceType', StringType(), True), StructField('OS', StringType(), True), StructField('ConnectionType', StringType(), True), StructField('WatchReason', StringType(), True)])
2025-05-08 15:38:23,471 - INFO - Testing access to Snowflake tables
2025-05-08 15:38:26,597 - INFO - Successfully accessed PUBLIC.DIM_TIME: 2102 rows
2025-05-08 15:38:27,676 - INFO - Successfully accessed PUBLIC.DIM_USER: 3315 rows
2025-05-08 15:38:29,452 - INFO - Successfully accessed PUBLIC.DIM_VIDEO: 2842 rows
2025-05-08 15:38:30,960 - INFO - Successfully accessed PUBLIC.DIM_PLATFORM: 4 rows
2025-05-08 15:38:32,658 - INFO - Successfully accessed PUBLIC.DIM_DEVICE_TYPE: 3 rows
2025-05-08 15:38:34,971 - INFO - Successfully accessed PUBLIC.DIM_OS: 4 rows
2025-05-08 15:38:37,470 - INFO - Successfully accessed PUBLIC.DIM_CONNECTION_TYPE: 2 rows
2025-05-08 15:38:38,881 - INFO - Successfully accessed PUBLIC.DIM_WATCH_REASON: 4 rows
2025-05-08 15:38:39,890 - INFO - Creating dim_time
2025-05-08 15:38:40,855 - INFO - New WatchTime values: 4 rows
2025-05-08 15:38:43,657 - INFO - Found 4 new times to add to dim_time
2025-05-08 15:38:43,661 - INFO - missing_time_df schema: StructType([StructField('TimeID', StringType(), False), StructField('WatchTime', StringType(), False), StructField('Hour', LongType(), True)])
2025-05-08 15:38:43,662 - INFO - Sample missing_time_df rows:
2025-05-08 15:38:51,071 - INFO - Successfully appended to dim_time: 0 rows
2025-05-08 15:38:51,081 - INFO - Creating dim_user
2025-05-08 15:38:52,788 - INFO - Found 11 new users to add to dim_user
2025-05-08 15:38:52,791 - INFO - missing_user_df schema: StructType([StructField('User_S_ID', StringType(), True), StructField('UserID', StringType(), True), StructField('Age', LongType(), True), StructField('Gender', StringType(), True), StructField('Location', StringType(), True), StructField('Income', LongType(), True), StructField('Debt', BooleanType(), True), StructField('OwnsProperty', BooleanType(), True), StructField('Profession', StringType(), True), StructField('Demographics', StringType(), True), StructField('CurrentActivity', StringType(), True), StructField('AgeGroup', StringType(), True)])
2025-05-08 15:38:52,791 - INFO - Sample missing_user_df rows:
2025-05-08 15:38:57,257 - INFO - Successfully appended to dim_user: 11 rows
2025-05-08 15:38:57,260 - INFO - Creating dim_video
2025-05-08 15:38:58,066 - INFO - Prepared dim_video: 13 rows
2025-05-08 15:38:58,073 - INFO - video_dim_df schema: StructType([StructField('Video_S_ID', StringType(), True), StructField('VideoID', LongType(), True), StructField('VideoCategory', StringType(), True), StructField('VideoLength', LongType(), True), StructField('TimeSpentCategory', StringType(), False)])
2025-05-08 15:38:58,075 - INFO - Sample video_dim_df rows:
2025-05-08 15:39:01,616 - INFO - Successfully wrote dim_video: 13 rows
2025-05-08 15:39:01,616 - INFO - Creating dim_platform
2025-05-08 15:39:02,979 - INFO - Found 1 new platforms to add to dim_platform
2025-05-08 15:39:02,981 - INFO - missing_platform_df schema: StructType([StructField('PlatformID', StringType(), True), StructField('Platform', StringType(), True)])
2025-05-08 15:39:02,982 - INFO - Sample missing_platform_df rows:
2025-05-08 15:39:07,451 - INFO - Successfully appended to dim_platform: 1 rows
2025-05-08 15:39:07,453 - INFO - Creating dim_device_type
2025-05-08 15:39:08,468 - INFO - Found 0 new device types to add to dim_device_type
2025-05-08 15:39:08,471 - INFO - missing_device_type_df schema: StructType([StructField('DeviceTypeID', StringType(), True), StructField('DeviceType', StringType(), True)])
2025-05-08 15:39:08,471 - INFO - Sample missing_device_type_df rows:
2025-05-08 15:39:08,931 - INFO - No new device types to append to dim_device_type
2025-05-08 15:39:08,935 - INFO - Creating dim_os
2025-05-08 15:39:10,551 - INFO - Found 0 new OS to add to dim_os
2025-05-08 15:39:10,553 - INFO - missing_os_df schema: StructType([StructField('OS_ID', StringType(), True), StructField('OS', StringType(), True)])
2025-05-08 15:39:10,554 - INFO - Sample missing_os_df rows:
2025-05-08 15:39:10,934 - INFO - No new OS to append to dim_os
2025-05-08 15:39:10,940 - INFO - Creating dim_connection_type
2025-05-08 15:39:12,124 - INFO - Found 1 new connection types to add to dim_connection_type
2025-05-08 15:39:12,127 - INFO - missing_connection_type_df schema: StructType([StructField('ConnectionTypeID', StringType(), True), StructField('ConnectionType', StringType(), True)])
2025-05-08 15:39:12,128 - INFO - Sample missing_connection_type_df rows:
2025-05-08 15:39:16,289 - INFO - Successfully appended to dim_connection_type: 1 rows
2025-05-08 15:39:16,291 - INFO - Creating dim_watch_reason
2025-05-08 15:39:17,329 - INFO - Found 2 new watch reasons to add to dim_watch_reason
2025-05-08 15:39:17,332 - INFO - missing_watch_reason_df schema: StructType([StructField('WatchReasonID', StringType(), True), StructField('WatchReason', StringType(), True)])
2025-05-08 15:39:17,332 - INFO - Sample missing_watch_reason_df rows:
2025-05-08 15:39:20,384 - INFO - Successfully appended to dim_watch_reason: 2 rows
2025-05-08 15:39:20,386 - INFO - Creating fact_video_interactions
2025-05-08 15:39:20,449 - INFO - Checking duplicates in fact_df before joins
2025-05-08 15:39:21,040 - INFO - Found 0 duplicate InteractionIDs in fact_df
2025-05-08 15:39:22,902 - INFO - dim_user_df schema: StructType([StructField('User_S_ID', StringType(), True), StructField('UserID', StringType(), True)])
2025-05-08 15:39:23,415 - INFO - After joining with dim_user: 13 rows
2025-05-08 15:39:24,347 - INFO - dim_video_df schema: StructType([StructField('Video_S_ID', StringType(), True), StructField('VideoID', DecimalType(38,0), True)])
2025-05-08 15:39:27,037 - INFO - After joining with dim_video: 13 rows
2025-05-08 15:39:27,416 - INFO - dim_platform_df schema: StructType([StructField('PlatformID', StringType(), True), StructField('Platform', StringType(), True)])
2025-05-08 15:39:30,205 - INFO - After joining with dim_platform: 13 rows
2025-05-08 15:39:30,652 - INFO - dim_device_type_df schema: StructType([StructField('DeviceTypeID', StringType(), True), StructField('DeviceType', StringType(), True)])
2025-05-08 15:39:34,148 - INFO - After joining with dim_device_type: 13 rows
2025-05-08 15:39:34,599 - INFO - dim_os_df schema: StructType([StructField('OS_ID', StringType(), True), StructField('OS', StringType(), True)])
2025-05-08 15:39:38,826 - INFO - After joining with dim_os: 13 rows
2025-05-08 15:39:39,762 - INFO - dim_connection_type_df schema: StructType([StructField('ConnectionTypeID', StringType(), True), StructField('ConnectionType', StringType(), True)])
2025-05-08 15:39:43,715 - INFO - After joining with dim_connection_type: 13 rows
2025-05-08 15:39:45,041 - INFO - dim_time_df schema: StructType([StructField('TimeID', StringType(), True), StructField('WatchTime', StringType(), True)])
2025-05-08 15:39:49,564 - INFO - After joining with dim_time: 13 rows
2025-05-08 15:39:49,879 - INFO - dim_watch_reason_df schema: StructType([StructField('WatchReasonID', StringType(), True), StructField('WatchReason', StringType(), True)])
2025-05-08 15:39:53,917 - INFO - After joining with dim_watch_reason: 13 rows
2025-05-08 15:39:53,920 - INFO - Deduplicating fact_df
2025-05-08 15:39:59,141 - INFO - After deduplication: 13 rows
2025-05-08 15:40:08,125 - INFO - Null counts for fact_video_interactions: {'InteractionID': 0, 'UserID_Surrogate': 0, 'VideoID_Surrogate': 0, 'PlatformID': 0, 'DeviceTypeID': 0, 'OS_ID': 0, 'ConnectionTypeID': 0, 'TimeID': 0, 'WatchReasonID': 0, 'TimeSpentOnVideo': 0, 'TotalTimeSpent': 0, 'Engagement': 0, 'NumberOfSessions': 0, 'NumberOfVideosWatched': 0, 'ScrollRate': 0, 'ImportanceScore': 0, 'ProductivityLoss': 0, 'Satisfaction': 0, 'SelfControl': 0, 'AddictionLevel': 0, 'IngestionTimestamp': 0}
2025-05-08 15:40:17,352 - INFO - Prepared fact_video_interactions: 13 rows
2025-05-08 15:40:17,353 - INFO - fact_df schema: StructType([StructField('InteractionID', StringType(), False), StructField('UserID_Surrogate', StringType(), True), StructField('VideoID_Surrogate', StringType(), True), StructField('PlatformID', StringType(), True), StructField('DeviceTypeID', StringType(), True), StructField('OS_ID', StringType(), True), StructField('ConnectionTypeID', StringType(), True), StructField('TimeID', StringType(), True), StructField('WatchReasonID', StringType(), True), StructField('TimeSpentOnVideo', LongType(), True), StructField('TotalTimeSpent', LongType(), True), StructField('Engagement', FloatType(), True), StructField('NumberOfSessions', LongType(), True), StructField('NumberOfVideosWatched', LongType(), True), StructField('ScrollRate', FloatType(), True), StructField('ImportanceScore', FloatType(), True), StructField('ProductivityLoss', FloatType(), True), StructField('Satisfaction', FloatType(), True), StructField('SelfControl', FloatType(), True), StructField('AddictionLevel', FloatType(), True), StructField('IngestionTimestamp', TimestampType(), True)])
2025-05-08 15:40:22,018 - INFO - Writing to PUBLIC.FACT_VIDEO_INTERACTIONS with options: {'sfURL': 'uxctays-kqc58183.snowflakecomputing.com', 'sfUser': 'Pranitha22', 'sfPassword': 'Pradyumna@9999', 'sfDatabase': 'ANALYTICS', 'sfSchema': 'PUBLIC', 'sfWarehouse': 'COMPUTE_WHI', 'sfRole': 'ACCOUNTADMIN'}
2025-05-08 15:40:31,799 - INFO - Successfully wrote fact_video_interactions: 13 rows
2025-05-08 15:40:32,680 - INFO - Spark session stopped
2025-05-08 15:40:32,685 - INFO - Closing down clientserver connection
2025-05-08 19:23:25,967 - INFO - Initializing Spark session
2025-05-08 19:23:29,348 - INFO - Snowflake connection options: {'sfURL': 'uxctays-kqc58183.snowflakecomputing.com', 'sfUser': 'Pranitha22', 'sfPassword': 'Pradyumna@9999', 'sfDatabase': 'ANALYTICS', 'sfSchema': 'PUBLIC', 'sfWarehouse': 'COMPUTE_WHI', 'sfRole': 'ACCOUNTADMIN'}
2025-05-08 19:23:29,349 - INFO - Reading staging data from s3a://datastreaming-analytics-1/staging/video_interactions
2025-05-08 19:23:31,981 - WARNING - Hadoop check failed for S3 path s3a://datastreaming-analytics-1/staging/video_interactions: 'JavaObject' object is not iterable
Traceback (most recent call last):
  File "/app/scripts/video_interactions_transformed.py", line 79, in check_s3_path
    file_list = [(f.getPath().getName(), f.getLen()) for f in files]
TypeError: 'JavaObject' object is not iterable

2025-05-08 19:23:32,330 - INFO - AWS SDK check for s3a://datastreaming-analytics-1/staging/video_interactions: file count: 2, files: [('staging/video_interactions/_SUCCESS', 0), ('staging/video_interactions/part-00000-b3d28f16-32dc-4327-a135-1050e9ddf09a-c000.snappy.parquet', 144894)]
2025-05-08 19:23:40,307 - INFO - Actual Parquet schema: StructType([StructField('UserID', LongType(), True), StructField('Age', LongType(), True), StructField('Gender', StringType(), True), StructField('Location', StringType(), True), StructField('Income', LongType(), True), StructField('Debt', BooleanType(), True), StructField('OwnsProperty', BooleanType(), True), StructField('Profession', StringType(), True), StructField('Demographics', StringType(), True), StructField('Platform', StringType(), True), StructField('TotalTimeSpent', LongType(), True), StructField('NumberOfSessions', LongType(), True), StructField('VideoID', LongType(), True), StructField('VideoCategory', StringType(), True), StructField('VideoLength', LongType(), True), StructField('Engagement', LongType(), True), StructField('ImportanceScore', LongType(), True), StructField('TimeSpentOnVideo', LongType(), True), StructField('NumberOfVideosWatched', LongType(), True), StructField('ScrollRate', LongType(), True), StructField('Frequency', StringType(), True), StructField('ProductivityLoss', LongType(), True), StructField('Satisfaction', LongType(), True), StructField('WatchReason', StringType(), True), StructField('DeviceType', StringType(), True), StructField('OS', StringType(), True), StructField('WatchTime', StringType(), True), StructField('SelfControl', LongType(), True), StructField('AddictionLevel', LongType(), True), StructField('CurrentActivity', StringType(), True), StructField('ConnectionType', StringType(), True), StructField('IngestionTimestamp', TimestampType(), True), StructField('AgeGroup', StringType(), True)])
2025-05-08 19:23:41,315 - INFO - Checking duplicates in video_df
2025-05-08 19:23:47,458 - INFO - Found 0 duplicate combinations in video_df
2025-05-08 19:23:55,333 - INFO - Read and deduplicated video_interactions: 2844 rows
2025-05-08 19:23:55,352 - INFO - Input schema: StructType([StructField('VideoID', LongType(), True), StructField('UserID', LongType(), True), StructField('Platform', StringType(), True), StructField('WatchTime', StringType(), True), StructField('IngestionTimestamp', TimestampType(), True), StructField('Age', LongType(), True), StructField('Gender', StringType(), True), StructField('Location', StringType(), True), StructField('Income', LongType(), True), StructField('Debt', BooleanType(), True), StructField('OwnsProperty', BooleanType(), True), StructField('Profession', StringType(), True), StructField('Demographics', StringType(), True), StructField('CurrentActivity', StringType(), True), StructField('AgeGroup', StringType(), True), StructField('VideoCategory', StringType(), True), StructField('VideoLength', LongType(), True), StructField('TimeSpentOnVideo', LongType(), True), StructField('TotalTimeSpent', LongType(), True), StructField('Engagement', LongType(), True), StructField('NumberOfSessions', LongType(), True), StructField('NumberOfVideosWatched', LongType(), True), StructField('ScrollRate', LongType(), True), StructField('ImportanceScore', LongType(), True), StructField('ProductivityLoss', LongType(), True), StructField('Satisfaction', LongType(), True), StructField('SelfControl', LongType(), True), StructField('AddictionLevel', LongType(), True), StructField('DeviceType', StringType(), True), StructField('OS', StringType(), True), StructField('ConnectionType', StringType(), True), StructField('WatchReason', StringType(), True)])
2025-05-08 19:23:56,388 - INFO - Testing access to Snowflake tables
2025-05-08 19:24:00,490 - INFO - Successfully accessed PUBLIC.DIM_TIME: 0 rows
2025-05-08 19:24:01,667 - INFO - Successfully accessed PUBLIC.DIM_USER: 0 rows
2025-05-08 19:24:03,867 - INFO - Successfully accessed PUBLIC.DIM_VIDEO: 0 rows
2025-05-08 19:24:05,587 - INFO - Successfully accessed PUBLIC.DIM_PLATFORM: 0 rows
2025-05-08 19:24:07,354 - INFO - Successfully accessed PUBLIC.DIM_DEVICE_TYPE: 0 rows
2025-05-08 19:24:08,109 - INFO - Successfully accessed PUBLIC.DIM_OS: 0 rows
2025-05-08 19:24:09,539 - INFO - Successfully accessed PUBLIC.DIM_CONNECTION_TYPE: 0 rows
2025-05-08 19:24:10,548 - INFO - Successfully accessed PUBLIC.DIM_WATCH_REASON: 0 rows
2025-05-08 19:24:11,513 - INFO - Creating dim_time
2025-05-08 19:24:12,621 - INFO - New WatchTime values: 1842 rows
2025-05-08 19:24:14,756 - INFO - Found 1842 new times to add to dim_time
2025-05-08 19:24:14,758 - INFO - missing_time_df schema: StructType([StructField('TimeID', StringType(), False), StructField('WatchTime', StringType(), False), StructField('Hour', LongType(), True)])
2025-05-08 19:24:14,759 - INFO - Sample missing_time_df rows:
2025-05-08 19:24:23,400 - INFO - Successfully appended to dim_time: 0 rows
2025-05-08 19:24:23,413 - INFO - Creating dim_user
2025-05-08 19:24:25,707 - INFO - Found 2844 new users to add to dim_user
2025-05-08 19:24:25,714 - INFO - missing_user_df schema: StructType([StructField('User_S_ID', StringType(), True), StructField('UserID', StringType(), True), StructField('Age', LongType(), True), StructField('Gender', StringType(), True), StructField('Location', StringType(), True), StructField('Income', LongType(), True), StructField('Debt', BooleanType(), True), StructField('OwnsProperty', BooleanType(), True), StructField('Profession', StringType(), True), StructField('Demographics', StringType(), True), StructField('CurrentActivity', StringType(), True), StructField('AgeGroup', StringType(), True)])
2025-05-08 19:24:25,715 - INFO - Sample missing_user_df rows:
2025-05-08 19:24:31,709 - INFO - Successfully appended to dim_user: 2844 rows
2025-05-08 19:24:31,712 - INFO - Creating dim_video
2025-05-08 19:24:32,825 - INFO - Prepared dim_video: 2842 rows
2025-05-08 19:24:32,827 - INFO - video_dim_df schema: StructType([StructField('Video_S_ID', StringType(), True), StructField('VideoID', LongType(), True), StructField('VideoCategory', StringType(), True), StructField('VideoLength', LongType(), True), StructField('TimeSpentCategory', StringType(), False)])
2025-05-08 19:24:32,828 - INFO - Sample video_dim_df rows:
2025-05-08 19:24:36,158 - INFO - Successfully wrote dim_video: 2842 rows
2025-05-08 19:24:36,159 - INFO - Creating dim_platform
2025-05-08 19:24:37,230 - INFO - Found 4 new platforms to add to dim_platform
2025-05-08 19:24:37,233 - INFO - missing_platform_df schema: StructType([StructField('PlatformID', StringType(), True), StructField('Platform', StringType(), True)])
2025-05-08 19:24:37,234 - INFO - Sample missing_platform_df rows:
2025-05-08 19:24:41,347 - INFO - Successfully appended to dim_platform: 4 rows
2025-05-08 19:24:41,349 - INFO - Creating dim_device_type
2025-05-08 19:24:42,386 - INFO - Found 3 new device types to add to dim_device_type
2025-05-08 19:24:42,390 - INFO - missing_device_type_df schema: StructType([StructField('DeviceTypeID', StringType(), True), StructField('DeviceType', StringType(), True)])
2025-05-08 19:24:42,391 - INFO - Sample missing_device_type_df rows:
2025-05-08 19:24:46,915 - INFO - Successfully appended to dim_device_type: 3 rows
2025-05-08 19:24:46,917 - INFO - Creating dim_os
2025-05-08 19:24:47,890 - INFO - Found 4 new OS to add to dim_os
2025-05-08 19:24:47,893 - INFO - missing_os_df schema: StructType([StructField('OS_ID', StringType(), True), StructField('OS', StringType(), True)])
2025-05-08 19:24:47,894 - INFO - Sample missing_os_df rows:
2025-05-08 19:24:52,590 - INFO - Successfully appended to dim_os: 4 rows
2025-05-08 19:24:52,592 - INFO - Creating dim_connection_type
2025-05-08 19:24:53,482 - INFO - Found 2 new connection types to add to dim_connection_type
2025-05-08 19:24:53,485 - INFO - missing_connection_type_df schema: StructType([StructField('ConnectionTypeID', StringType(), True), StructField('ConnectionType', StringType(), True)])
2025-05-08 19:24:53,486 - INFO - Sample missing_connection_type_df rows:
2025-05-08 19:24:57,596 - INFO - Successfully appended to dim_connection_type: 2 rows
2025-05-08 19:24:57,598 - INFO - Creating dim_watch_reason
2025-05-08 19:24:58,426 - INFO - Found 4 new watch reasons to add to dim_watch_reason
2025-05-08 19:24:58,428 - INFO - missing_watch_reason_df schema: StructType([StructField('WatchReasonID', StringType(), True), StructField('WatchReason', StringType(), True)])
2025-05-08 19:24:58,429 - INFO - Sample missing_watch_reason_df rows:
2025-05-08 19:25:02,698 - INFO - Successfully appended to dim_watch_reason: 4 rows
2025-05-08 19:25:02,700 - INFO - Creating fact_video_interactions
2025-05-08 19:25:02,763 - INFO - Checking duplicates in fact_df before joins
2025-05-08 19:25:03,892 - INFO - Found 0 duplicate InteractionIDs in fact_df
2025-05-08 19:25:05,160 - INFO - dim_user_df schema: StructType([StructField('User_S_ID', StringType(), True), StructField('UserID', StringType(), True)])
2025-05-08 19:25:06,268 - INFO - After joining with dim_user: 2844 rows
2025-05-08 19:25:07,426 - INFO - dim_video_df schema: StructType([StructField('Video_S_ID', StringType(), True), StructField('VideoID', DecimalType(38,0), True)])
2025-05-08 19:25:10,436 - INFO - After joining with dim_video: 5050 rows
2025-05-08 19:25:11,101 - INFO - dim_platform_df schema: StructType([StructField('PlatformID', StringType(), True), StructField('Platform', StringType(), True)])
2025-05-08 19:25:14,174 - INFO - After joining with dim_platform: 5050 rows
2025-05-08 19:25:15,074 - INFO - dim_device_type_df schema: StructType([StructField('DeviceTypeID', StringType(), True), StructField('DeviceType', StringType(), True)])
2025-05-08 19:25:18,564 - INFO - After joining with dim_device_type: 5050 rows
2025-05-08 19:25:18,952 - INFO - dim_os_df schema: StructType([StructField('OS_ID', StringType(), True), StructField('OS', StringType(), True)])
2025-05-08 19:25:22,890 - INFO - After joining with dim_os: 5050 rows
2025-05-08 19:25:23,940 - INFO - dim_connection_type_df schema: StructType([StructField('ConnectionTypeID', StringType(), True), StructField('ConnectionType', StringType(), True)])
2025-05-08 19:25:28,250 - INFO - After joining with dim_connection_type: 5050 rows
2025-05-08 19:25:29,507 - INFO - dim_time_df schema: StructType([StructField('TimeID', StringType(), True), StructField('WatchTime', StringType(), True)])
2025-05-08 19:25:34,467 - INFO - After joining with dim_time: 5050 rows
2025-05-08 19:25:35,015 - INFO - dim_watch_reason_df schema: StructType([StructField('WatchReasonID', StringType(), True), StructField('WatchReason', StringType(), True)])
2025-05-08 19:25:40,446 - INFO - After joining with dim_watch_reason: 5050 rows
2025-05-08 19:25:40,494 - INFO - Deduplicating fact_df
2025-05-08 19:25:45,538 - INFO - After deduplication: 2844 rows
2025-05-08 19:25:55,942 - INFO - Null counts for fact_video_interactions: {'InteractionID': 0, 'UserID_Surrogate': 0, 'VideoID_Surrogate': 0, 'PlatformID': 0, 'DeviceTypeID': 0, 'OS_ID': 0, 'ConnectionTypeID': 0, 'TimeID': 0, 'WatchReasonID': 0, 'TimeSpentOnVideo': 0, 'TotalTimeSpent': 0, 'Engagement': 0, 'NumberOfSessions': 0, 'NumberOfVideosWatched': 0, 'ScrollRate': 0, 'ImportanceScore': 0, 'ProductivityLoss': 0, 'Satisfaction': 0, 'SelfControl': 0, 'AddictionLevel': 0, 'IngestionTimestamp': 0}
2025-05-08 19:26:04,999 - INFO - Prepared fact_video_interactions: 2844 rows
2025-05-08 19:26:05,001 - INFO - fact_df schema: StructType([StructField('InteractionID', StringType(), False), StructField('UserID_Surrogate', StringType(), True), StructField('VideoID_Surrogate', StringType(), True), StructField('PlatformID', StringType(), True), StructField('DeviceTypeID', StringType(), True), StructField('OS_ID', StringType(), True), StructField('ConnectionTypeID', StringType(), True), StructField('TimeID', StringType(), True), StructField('WatchReasonID', StringType(), True), StructField('TimeSpentOnVideo', LongType(), True), StructField('TotalTimeSpent', LongType(), True), StructField('Engagement', FloatType(), True), StructField('NumberOfSessions', LongType(), True), StructField('NumberOfVideosWatched', LongType(), True), StructField('ScrollRate', FloatType(), True), StructField('ImportanceScore', FloatType(), True), StructField('ProductivityLoss', FloatType(), True), StructField('Satisfaction', FloatType(), True), StructField('SelfControl', FloatType(), True), StructField('AddictionLevel', FloatType(), True), StructField('IngestionTimestamp', TimestampType(), True)])
2025-05-08 19:26:10,911 - INFO - Writing to PUBLIC.FACT_VIDEO_INTERACTIONS with options: {'sfURL': 'uxctays-kqc58183.snowflakecomputing.com', 'sfUser': 'Pranitha22', 'sfPassword': 'Pradyumna@9999', 'sfDatabase': 'ANALYTICS', 'sfSchema': 'PUBLIC', 'sfWarehouse': 'COMPUTE_WHI', 'sfRole': 'ACCOUNTADMIN'}
2025-05-08 19:26:23,745 - INFO - Successfully wrote fact_video_interactions: 2844 rows
2025-05-08 19:26:24,457 - INFO - Spark session stopped
2025-05-08 19:26:24,460 - INFO - Closing down clientserver connection
2025-05-08 20:43:08,228 - INFO - Initializing Spark session
2025-05-08 20:43:11,649 - INFO - Snowflake connection options: {'sfURL': 'uxctays-kqc58183.snowflakecomputing.com', 'sfUser': 'Pranitha22', 'sfPassword': 'Pradyumna@9999', 'sfDatabase': 'ANALYTICS', 'sfSchema': 'PUBLIC', 'sfWarehouse': 'COMPUTE_WHI', 'sfRole': 'ACCOUNTADMIN'}
2025-05-08 20:43:11,650 - INFO - Reading staging data from s3a://datastreaming-analytics-1/staging/video_interactions
2025-05-08 20:43:15,055 - WARNING - Hadoop check failed for S3 path s3a://datastreaming-analytics-1/staging/video_interactions: 'JavaObject' object is not iterable
Traceback (most recent call last):
  File "/app/scripts/video_interactions_transformed.py", line 79, in check_s3_path
    file_list = [(f.getPath().getName(), f.getLen()) for f in files]
TypeError: 'JavaObject' object is not iterable

2025-05-08 20:43:16,257 - INFO - AWS SDK check for s3a://datastreaming-analytics-1/staging/video_interactions: file count: 5, files: [('staging/video_interactions/IngestionTimestamp=2025-05-08 20%3A05%3A03.933/part-00000-d4f7e0f2-ceb4-4282-9a77-20b0e1314027.c000.snappy.parquet', 8586), ('staging/video_interactions/_SUCCESS', 0), ('staging/video_interactions/_spark_metadata/7', 2), ('staging/video_interactions/_spark_metadata/8', 309), ('staging/video_interactions/part-00000-b3d28f16-32dc-4327-a135-1050e9ddf09a-c000.snappy.parquet', 144894)]
2025-05-08 20:43:24,895 - ERROR - Failed to read staging data: An error occurred while calling o48.parquet.
: java.lang.IllegalStateException: s3a://datastreaming-analytics-1/staging/video_interactions/_spark_metadata/0 doesn't exist (latestId: 8, compactInterval: 10)
	at org.apache.spark.sql.execution.streaming.CompactibleFileStreamLog.$anonfun$allFiles$4(CompactibleFileStreamLog.scala:267)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.execution.streaming.CompactibleFileStreamLog.$anonfun$allFiles$2(CompactibleFileStreamLog.scala:265)
	at org.apache.spark.sql.execution.streaming.CompactibleFileStreamLog.$anonfun$allFiles$2$adapted(CompactibleFileStreamLog.scala:263)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:293)
	at scala.collection.immutable.NumericRange.foreach(NumericRange.scala:75)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:293)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:290)
	at scala.collection.AbstractTraversable.flatMap(Traversable.scala:108)
	at org.apache.spark.sql.execution.streaming.CompactibleFileStreamLog.allFiles(CompactibleFileStreamLog.scala:263)
	at org.apache.spark.sql.execution.streaming.MetadataLogFileIndex.<init>(MetadataLogFileIndex.scala:53)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:369)
	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)
	at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)
	at org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:563)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)

Traceback (most recent call last):
  File "/app/scripts/video_interactions_transformed.py", line 159, in <module>
    temp_df = spark.read.parquet(staging_path)
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 531, in parquet
    return self._df(self._jreader.parquet(_to_seq(self._spark._sc, paths)))
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 169, in deco
    return f(*a, **kw)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 326, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: An error occurred while calling o48.parquet.
: java.lang.IllegalStateException: s3a://datastreaming-analytics-1/staging/video_interactions/_spark_metadata/0 doesn't exist (latestId: 8, compactInterval: 10)
	at org.apache.spark.sql.execution.streaming.CompactibleFileStreamLog.$anonfun$allFiles$4(CompactibleFileStreamLog.scala:267)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.execution.streaming.CompactibleFileStreamLog.$anonfun$allFiles$2(CompactibleFileStreamLog.scala:265)
	at org.apache.spark.sql.execution.streaming.CompactibleFileStreamLog.$anonfun$allFiles$2$adapted(CompactibleFileStreamLog.scala:263)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:293)
	at scala.collection.immutable.NumericRange.foreach(NumericRange.scala:75)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:293)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:290)
	at scala.collection.AbstractTraversable.flatMap(Traversable.scala:108)
	at org.apache.spark.sql.execution.streaming.CompactibleFileStreamLog.allFiles(CompactibleFileStreamLog.scala:263)
	at org.apache.spark.sql.execution.streaming.MetadataLogFileIndex.<init>(MetadataLogFileIndex.scala:53)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:369)
	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)
	at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)
	at org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:563)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)


2025-05-08 20:43:24,901 - ERROR - Unexpected error in video_interactions_transformed.py: An error occurred while calling o48.parquet.
: java.lang.IllegalStateException: s3a://datastreaming-analytics-1/staging/video_interactions/_spark_metadata/0 doesn't exist (latestId: 8, compactInterval: 10)
	at org.apache.spark.sql.execution.streaming.CompactibleFileStreamLog.$anonfun$allFiles$4(CompactibleFileStreamLog.scala:267)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.execution.streaming.CompactibleFileStreamLog.$anonfun$allFiles$2(CompactibleFileStreamLog.scala:265)
	at org.apache.spark.sql.execution.streaming.CompactibleFileStreamLog.$anonfun$allFiles$2$adapted(CompactibleFileStreamLog.scala:263)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:293)
	at scala.collection.immutable.NumericRange.foreach(NumericRange.scala:75)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:293)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:290)
	at scala.collection.AbstractTraversable.flatMap(Traversable.scala:108)
	at org.apache.spark.sql.execution.streaming.CompactibleFileStreamLog.allFiles(CompactibleFileStreamLog.scala:263)
	at org.apache.spark.sql.execution.streaming.MetadataLogFileIndex.<init>(MetadataLogFileIndex.scala:53)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:369)
	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)
	at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)
	at org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:563)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)

Traceback (most recent call last):
  File "/app/scripts/video_interactions_transformed.py", line 159, in <module>
    temp_df = spark.read.parquet(staging_path)
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 531, in parquet
    return self._df(self._jreader.parquet(_to_seq(self._spark._sc, paths)))
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 169, in deco
    return f(*a, **kw)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 326, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: An error occurred while calling o48.parquet.
: java.lang.IllegalStateException: s3a://datastreaming-analytics-1/staging/video_interactions/_spark_metadata/0 doesn't exist (latestId: 8, compactInterval: 10)
	at org.apache.spark.sql.execution.streaming.CompactibleFileStreamLog.$anonfun$allFiles$4(CompactibleFileStreamLog.scala:267)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.execution.streaming.CompactibleFileStreamLog.$anonfun$allFiles$2(CompactibleFileStreamLog.scala:265)
	at org.apache.spark.sql.execution.streaming.CompactibleFileStreamLog.$anonfun$allFiles$2$adapted(CompactibleFileStreamLog.scala:263)
	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:293)
	at scala.collection.immutable.NumericRange.foreach(NumericRange.scala:75)
	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:293)
	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:290)
	at scala.collection.AbstractTraversable.flatMap(Traversable.scala:108)
	at org.apache.spark.sql.execution.streaming.CompactibleFileStreamLog.allFiles(CompactibleFileStreamLog.scala:263)
	at org.apache.spark.sql.execution.streaming.MetadataLogFileIndex.<init>(MetadataLogFileIndex.scala:53)
	at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:369)
	at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)
	at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)
	at org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:563)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)


2025-05-08 20:43:24,941 - INFO - Closing down clientserver connection
2025-05-09 00:43:55,387 - INFO - Initializing Spark session
2025-05-09 00:43:58,770 - INFO - Snowflake connection options: {'sfURL': 'uxctays-kqc58183.snowflakecomputing.com', 'sfUser': 'Pranitha22', 'sfPassword': 'Pradyumna@9999', 'sfDatabase': 'ANALYTICS', 'sfSchema': 'PUBLIC', 'sfWarehouse': 'COMPUTE_WHI', 'sfRole': 'ACCOUNTADMIN'}
2025-05-09 00:43:58,771 - INFO - Reading staging data from s3a://datastreaming-analytics-1/staging/video_interactions/IngestionTimestamp=*
2025-05-09 00:44:01,473 - INFO - S3 Hadoop check for s3a://datastreaming-analytics-1/staging/video_interactions/IngestionTimestamp=* - exists: False, file count: 0, files: []
2025-05-09 00:44:01,832 - INFO - S3 AWS SDK check for s3a://datastreaming-analytics-1/staging/video_interactions/IngestionTimestamp=* - file count: 1, files: [('s3a://datastreaming-analytics-1/staging/video_interactions/IngestionTimestamp=2025-05-08 20%3A05%3A03.933/part-00000-d4f7e0f2-ceb4-4282-9a77-20b0e1314027.c000.snappy.parquet', 8586)]
2025-05-09 00:44:07,900 - INFO - Actual Parquet schema: StructType([StructField('VideoID', LongType(), True), StructField('UserID', LongType(), True), StructField('VideoCategory', StringType(), True), StructField('Platform', StringType(), True), StructField('DeviceType', StringType(), True), StructField('WatchTime', StringType(), True), StructField('EngagementScore', LongType(), True), StructField('IngestionTimestamp', TimestampType(), True)])
2025-05-09 00:44:12,612 - INFO - Read 1 rows from Parquet files
2025-05-09 00:44:12,613 - INFO - Sample data:
2025-05-09 00:44:14,689 - INFO - Distinct IngestionTimestamp values:
2025-05-09 00:44:16,892 - INFO - Checking duplicates in video_df
2025-05-09 00:44:18,763 - INFO - Found 0 duplicate combinations in video_df
2025-05-09 00:44:22,884 - INFO - After deduplication: 1 rows
2025-05-09 00:44:23,383 - INFO - Testing access to Snowflake tables
2025-05-09 00:44:27,912 - INFO - Successfully accessed PUBLIC.DIM_USER: 3322 rows
2025-05-09 00:44:30,258 - INFO - Successfully accessed PUBLIC.DIM_VIDEO: 2842 rows
2025-05-09 00:44:32,282 - INFO - Successfully accessed PUBLIC.DIM_PLATFORM: 5 rows
2025-05-09 00:44:34,274 - INFO - Successfully accessed PUBLIC.DIM_DEVICE_TYPE: 4 rows
2025-05-09 00:44:36,190 - INFO - Successfully accessed PUBLIC.DIM_TIME: 2106 rows
2025-05-09 00:44:37,193 - INFO - Creating dim_time
2025-05-09 00:44:38,104 - INFO - New WatchTime values: 1 rows
2025-05-09 00:44:39,993 - INFO - Found 0 new times to add to dim_time
2025-05-09 00:44:39,997 - INFO - missing_time_df schema: StructType([StructField('TimeID', StringType(), False), StructField('WatchTime', StringType(), False), StructField('Hour', LongType(), True)])
2025-05-09 00:44:39,998 - INFO - Sample missing_time_df rows:
2025-05-09 00:44:40,845 - INFO - No new times to append to dim_time
2025-05-09 00:44:40,869 - INFO - Creating dim_user
2025-05-09 00:44:42,579 - INFO - Found 1 new users to add to dim_user
2025-05-09 00:44:42,581 - INFO - missing_user_df schema: StructType([StructField('User_S_ID', StringType(), True), StructField('UserID', StringType(), True), StructField('Age', LongType(), True), StructField('Gender', StringType(), True), StructField('Location', StringType(), True), StructField('Income', LongType(), True), StructField('Debt', BooleanType(), True), StructField('OwnsProperty', BooleanType(), True), StructField('Profession', StringType(), True), StructField('Demographics', StringType(), True), StructField('CurrentActivity', StringType(), True), StructField('AgeGroup', StringType(), True)])
2025-05-09 00:44:42,582 - INFO - Sample missing_user_df rows:
2025-05-09 00:44:49,699 - INFO - Successfully appended to dim_user: 1 rows
2025-05-09 00:44:49,702 - INFO - Creating dim_video
2025-05-09 00:44:50,471 - INFO - Prepared dim_video: 1 rows
2025-05-09 00:44:50,476 - INFO - video_dim_df schema: StructType([StructField('Video_S_ID', StringType(), True), StructField('VideoID', LongType(), True), StructField('VideoCategory', StringType(), True)])
2025-05-09 00:44:50,476 - INFO - Sample video_dim_df rows:
2025-05-09 00:44:53,759 - INFO - Successfully wrote dim_video: 1 rows
2025-05-09 00:44:53,768 - INFO - Creating dim_platform
2025-05-09 00:44:55,055 - INFO - Found 0 new platforms to add to dim_platform
2025-05-09 00:44:55,058 - INFO - missing_platform_df schema: StructType([StructField('PlatformID', StringType(), True), StructField('Platform', StringType(), True)])
2025-05-09 00:44:55,059 - INFO - Sample missing_platform_df rows:
2025-05-09 00:44:55,611 - INFO - No new platforms to append to dim_platform
2025-05-09 00:44:55,648 - INFO - Creating dim_device_type
2025-05-09 00:44:56,971 - INFO - Found 0 new device types to add to dim_device_type
2025-05-09 00:44:56,983 - INFO - missing_device_type_df schema: StructType([StructField('DeviceTypeID', StringType(), True), StructField('DeviceType', StringType(), True)])
2025-05-09 00:44:56,984 - INFO - Sample missing_device_type_df rows:
2025-05-09 00:44:57,479 - INFO - No new device types to append to dim_device_type
2025-05-09 00:44:57,484 - INFO - Creating fact_video_interactions
2025-05-09 00:44:57,604 - INFO - Checking duplicates in fact_df before joins
2025-05-09 00:44:58,422 - INFO - Found 0 duplicate InteractionIDs in fact_df
2025-05-09 00:45:00,411 - INFO - dim_user_df schema: StructType([StructField('User_S_ID', StringType(), True), StructField('UserID', StringType(), True)])
2025-05-09 00:45:01,491 - INFO - After joining with dim_user: 1 rows
2025-05-09 00:45:02,365 - INFO - dim_video_df schema: StructType([StructField('Video_S_ID', StringType(), True), StructField('VideoID', DecimalType(38,0), True)])
2025-05-09 00:45:04,943 - INFO - After joining with dim_video: 1 rows
2025-05-09 00:45:05,451 - INFO - dim_platform_df schema: StructType([StructField('PlatformID', StringType(), True), StructField('Platform', StringType(), True)])
2025-05-09 00:45:06,880 - INFO - After joining with dim_platform: 1 rows
2025-05-09 00:45:07,342 - INFO - dim_device_type_df schema: StructType([StructField('DeviceTypeID', StringType(), True), StructField('DeviceType', StringType(), True)])
2025-05-09 00:45:10,351 - INFO - After joining with dim_device_type: 1 rows
2025-05-09 00:45:11,182 - INFO - dim_time_df schema: StructType([StructField('TimeID', StringType(), True), StructField('WatchTime', StringType(), True)])
2025-05-09 00:45:14,538 - INFO - After joining with dim_time: 1 rows
2025-05-09 00:45:14,545 - INFO - Deduplicating fact_df
2025-05-09 00:45:18,762 - INFO - After in-batch deduplication: 1 rows
2025-05-09 00:45:23,435 - INFO - After removing existing InteractionIDs: 1 rows
2025-05-09 00:45:30,251 - INFO - Null counts for fact_video_interactions: {'InteractionID': 0, 'UserID_Surrogate': 0, 'VideoID_Surrogate': 0, 'PlatformID': 0, 'DeviceTypeID': 0, 'TimeID': 0, 'EngagementScore': 1, 'IngestionTimestamp': 1}
2025-05-09 00:45:35,155 - WARNING - Found 1 rows with nulls in critical columns for fact_video_interactions
2025-05-09 00:45:38,473 - ERROR - Error validating nulls for fact_video_interactions: VideoID
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/types.py", line 2164, in __getitem__
    idx = self.__fields__.index(item)
ValueError: 'VideoID' is not in list

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/app/scripts/stream_video_streamed_transformed.py", line 118, in validate_nulls
    logger.info(f"Rejected row (Nulls): VideoID={row['VideoID']}, UserID={row['UserID']}, "
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/types.py", line 2169, in __getitem__
    raise ValueError(item)
ValueError: VideoID

2025-05-09 00:45:41,968 - INFO - Prepared fact_video_interactions: 1 rows
2025-05-09 00:45:41,968 - INFO - fact_df schema: StructType([StructField('InteractionID', StringType(), False), StructField('UserID_Surrogate', StringType(), True), StructField('VideoID_Surrogate', StringType(), True), StructField('PlatformID', StringType(), True), StructField('DeviceTypeID', StringType(), True), StructField('TimeID', StringType(), True), StructField('EngagementScore', FloatType(), True), StructField('IngestionTimestamp', TimestampType(), True)])
2025-05-09 00:45:46,465 - INFO - Writing to PUBLIC.FACT_VIDEO_INTERACTIONS with options: {'sfURL': 'uxctays-kqc58183.snowflakecomputing.com', 'sfUser': 'Pranitha22', 'sfPassword': 'Pradyumna@9999', 'sfDatabase': 'ANALYTICS', 'sfSchema': 'PUBLIC', 'sfWarehouse': 'COMPUTE_WHI', 'sfRole': 'ACCOUNTADMIN'}
2025-05-09 00:45:52,894 - ERROR - Failed to write to fact_video_interactions: An error occurred while calling o671.save.
: java.sql.SQLException: Status of query associated with resultSet is FAILED_WITH_ERROR. Number of columns in file (8) does not match that of the corresponding table (21), use file format option error_on_column_count_mismatch=false to ignore this error
  File 'HQXEvzKJLr/0.CSV.gz', line 1, character 212
  Row 1, column "FACT_VIDEO_INTERACTIONS"["TIMEID":8]
  If you would like to continue loading when an error is encountered, use other values such as 'SKIP_FILE' or 'CONTINUE' for the ON_ERROR option. For more information on loading options, please run 'info loading_data' in a SQL client. Results not generated.
	at net.snowflake.client.jdbc.SFAsyncResultSet.getRealResults(SFAsyncResultSet.java:159)
	at net.snowflake.client.jdbc.SFAsyncResultSet.getMetaData(SFAsyncResultSet.java:298)
	at net.snowflake.spark.snowflake.io.StageWriter$.executeCopyIntoTable(StageWriter.scala:568)
	at net.snowflake.spark.snowflake.io.StageWriter$.writeToTableWithStagingTable(StageWriter.scala:448)
	at net.snowflake.spark.snowflake.io.StageWriter$.writeToTable(StageWriter.scala:288)
	at net.snowflake.spark.snowflake.io.StageWriter$.writeToStage(StageWriter.scala:233)
	at net.snowflake.spark.snowflake.io.package$.writeRDD(package.scala:51)
	at net.snowflake.spark.snowflake.SnowflakeWriter.save(SnowflakeWriter.scala:73)
	at net.snowflake.spark.snowflake.DefaultSource.createRelation(DefaultSource.scala:141)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)

Traceback (most recent call last):
  File "/app/scripts/stream_video_streamed_transformed.py", line 512, in <module>
    fact_df.write \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 1396, in save
    self._jwrite.save()
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 169, in deco
    return f(*a, **kw)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 326, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: An error occurred while calling o671.save.
: java.sql.SQLException: Status of query associated with resultSet is FAILED_WITH_ERROR. Number of columns in file (8) does not match that of the corresponding table (21), use file format option error_on_column_count_mismatch=false to ignore this error
  File 'HQXEvzKJLr/0.CSV.gz', line 1, character 212
  Row 1, column "FACT_VIDEO_INTERACTIONS"["TIMEID":8]
  If you would like to continue loading when an error is encountered, use other values such as 'SKIP_FILE' or 'CONTINUE' for the ON_ERROR option. For more information on loading options, please run 'info loading_data' in a SQL client. Results not generated.
	at net.snowflake.client.jdbc.SFAsyncResultSet.getRealResults(SFAsyncResultSet.java:159)
	at net.snowflake.client.jdbc.SFAsyncResultSet.getMetaData(SFAsyncResultSet.java:298)
	at net.snowflake.spark.snowflake.io.StageWriter$.executeCopyIntoTable(StageWriter.scala:568)
	at net.snowflake.spark.snowflake.io.StageWriter$.writeToTableWithStagingTable(StageWriter.scala:448)
	at net.snowflake.spark.snowflake.io.StageWriter$.writeToTable(StageWriter.scala:288)
	at net.snowflake.spark.snowflake.io.StageWriter$.writeToStage(StageWriter.scala:233)
	at net.snowflake.spark.snowflake.io.package$.writeRDD(package.scala:51)
	at net.snowflake.spark.snowflake.SnowflakeWriter.save(SnowflakeWriter.scala:73)
	at net.snowflake.spark.snowflake.DefaultSource.createRelation(DefaultSource.scala:141)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)


2025-05-09 00:45:52,900 - ERROR - Error creating fact_video_interactions: An error occurred while calling o671.save.
: java.sql.SQLException: Status of query associated with resultSet is FAILED_WITH_ERROR. Number of columns in file (8) does not match that of the corresponding table (21), use file format option error_on_column_count_mismatch=false to ignore this error
  File 'HQXEvzKJLr/0.CSV.gz', line 1, character 212
  Row 1, column "FACT_VIDEO_INTERACTIONS"["TIMEID":8]
  If you would like to continue loading when an error is encountered, use other values such as 'SKIP_FILE' or 'CONTINUE' for the ON_ERROR option. For more information on loading options, please run 'info loading_data' in a SQL client. Results not generated.
	at net.snowflake.client.jdbc.SFAsyncResultSet.getRealResults(SFAsyncResultSet.java:159)
	at net.snowflake.client.jdbc.SFAsyncResultSet.getMetaData(SFAsyncResultSet.java:298)
	at net.snowflake.spark.snowflake.io.StageWriter$.executeCopyIntoTable(StageWriter.scala:568)
	at net.snowflake.spark.snowflake.io.StageWriter$.writeToTableWithStagingTable(StageWriter.scala:448)
	at net.snowflake.spark.snowflake.io.StageWriter$.writeToTable(StageWriter.scala:288)
	at net.snowflake.spark.snowflake.io.StageWriter$.writeToStage(StageWriter.scala:233)
	at net.snowflake.spark.snowflake.io.package$.writeRDD(package.scala:51)
	at net.snowflake.spark.snowflake.SnowflakeWriter.save(SnowflakeWriter.scala:73)
	at net.snowflake.spark.snowflake.DefaultSource.createRelation(DefaultSource.scala:141)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)

Traceback (most recent call last):
  File "/app/scripts/stream_video_streamed_transformed.py", line 512, in <module>
    fact_df.write \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 1396, in save
    self._jwrite.save()
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 169, in deco
    return f(*a, **kw)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 326, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: An error occurred while calling o671.save.
: java.sql.SQLException: Status of query associated with resultSet is FAILED_WITH_ERROR. Number of columns in file (8) does not match that of the corresponding table (21), use file format option error_on_column_count_mismatch=false to ignore this error
  File 'HQXEvzKJLr/0.CSV.gz', line 1, character 212
  Row 1, column "FACT_VIDEO_INTERACTIONS"["TIMEID":8]
  If you would like to continue loading when an error is encountered, use other values such as 'SKIP_FILE' or 'CONTINUE' for the ON_ERROR option. For more information on loading options, please run 'info loading_data' in a SQL client. Results not generated.
	at net.snowflake.client.jdbc.SFAsyncResultSet.getRealResults(SFAsyncResultSet.java:159)
	at net.snowflake.client.jdbc.SFAsyncResultSet.getMetaData(SFAsyncResultSet.java:298)
	at net.snowflake.spark.snowflake.io.StageWriter$.executeCopyIntoTable(StageWriter.scala:568)
	at net.snowflake.spark.snowflake.io.StageWriter$.writeToTableWithStagingTable(StageWriter.scala:448)
	at net.snowflake.spark.snowflake.io.StageWriter$.writeToTable(StageWriter.scala:288)
	at net.snowflake.spark.snowflake.io.StageWriter$.writeToStage(StageWriter.scala:233)
	at net.snowflake.spark.snowflake.io.package$.writeRDD(package.scala:51)
	at net.snowflake.spark.snowflake.SnowflakeWriter.save(SnowflakeWriter.scala:73)
	at net.snowflake.spark.snowflake.DefaultSource.createRelation(DefaultSource.scala:141)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)


2025-05-09 00:45:52,905 - ERROR - Unexpected error in video_interactions_transformed.py: An error occurred while calling o671.save.
: java.sql.SQLException: Status of query associated with resultSet is FAILED_WITH_ERROR. Number of columns in file (8) does not match that of the corresponding table (21), use file format option error_on_column_count_mismatch=false to ignore this error
  File 'HQXEvzKJLr/0.CSV.gz', line 1, character 212
  Row 1, column "FACT_VIDEO_INTERACTIONS"["TIMEID":8]
  If you would like to continue loading when an error is encountered, use other values such as 'SKIP_FILE' or 'CONTINUE' for the ON_ERROR option. For more information on loading options, please run 'info loading_data' in a SQL client. Results not generated.
	at net.snowflake.client.jdbc.SFAsyncResultSet.getRealResults(SFAsyncResultSet.java:159)
	at net.snowflake.client.jdbc.SFAsyncResultSet.getMetaData(SFAsyncResultSet.java:298)
	at net.snowflake.spark.snowflake.io.StageWriter$.executeCopyIntoTable(StageWriter.scala:568)
	at net.snowflake.spark.snowflake.io.StageWriter$.writeToTableWithStagingTable(StageWriter.scala:448)
	at net.snowflake.spark.snowflake.io.StageWriter$.writeToTable(StageWriter.scala:288)
	at net.snowflake.spark.snowflake.io.StageWriter$.writeToStage(StageWriter.scala:233)
	at net.snowflake.spark.snowflake.io.package$.writeRDD(package.scala:51)
	at net.snowflake.spark.snowflake.SnowflakeWriter.save(SnowflakeWriter.scala:73)
	at net.snowflake.spark.snowflake.DefaultSource.createRelation(DefaultSource.scala:141)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)

Traceback (most recent call last):
  File "/app/scripts/stream_video_streamed_transformed.py", line 512, in <module>
    fact_df.write \
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 1396, in save
    self._jwrite.save()
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 169, in deco
    return f(*a, **kw)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 326, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: An error occurred while calling o671.save.
: java.sql.SQLException: Status of query associated with resultSet is FAILED_WITH_ERROR. Number of columns in file (8) does not match that of the corresponding table (21), use file format option error_on_column_count_mismatch=false to ignore this error
  File 'HQXEvzKJLr/0.CSV.gz', line 1, character 212
  Row 1, column "FACT_VIDEO_INTERACTIONS"["TIMEID":8]
  If you would like to continue loading when an error is encountered, use other values such as 'SKIP_FILE' or 'CONTINUE' for the ON_ERROR option. For more information on loading options, please run 'info loading_data' in a SQL client. Results not generated.
	at net.snowflake.client.jdbc.SFAsyncResultSet.getRealResults(SFAsyncResultSet.java:159)
	at net.snowflake.client.jdbc.SFAsyncResultSet.getMetaData(SFAsyncResultSet.java:298)
	at net.snowflake.spark.snowflake.io.StageWriter$.executeCopyIntoTable(StageWriter.scala:568)
	at net.snowflake.spark.snowflake.io.StageWriter$.writeToTableWithStagingTable(StageWriter.scala:448)
	at net.snowflake.spark.snowflake.io.StageWriter$.writeToTable(StageWriter.scala:288)
	at net.snowflake.spark.snowflake.io.StageWriter$.writeToStage(StageWriter.scala:233)
	at net.snowflake.spark.snowflake.io.package$.writeRDD(package.scala:51)
	at net.snowflake.spark.snowflake.SnowflakeWriter.save(SnowflakeWriter.scala:73)
	at net.snowflake.spark.snowflake.DefaultSource.createRelation(DefaultSource.scala:141)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)


2025-05-09 00:45:52,922 - INFO - Closing down clientserver connection
2025-05-09 00:49:09,546 - INFO - Initializing Spark session
2025-05-09 00:49:12,590 - INFO - Snowflake connection options: {'sfURL': 'uxctays-kqc58183.snowflakecomputing.com', 'sfUser': 'Pranitha22', 'sfPassword': 'Pradyumna@9999', 'sfDatabase': 'ANALYTICS', 'sfSchema': 'PUBLIC', 'sfWarehouse': 'COMPUTE_WHI', 'sfRole': 'ACCOUNTADMIN'}
2025-05-09 00:49:12,591 - INFO - Reading staging data from s3a://datastreaming-analytics-1/staging/video_interactions/IngestionTimestamp=*
2025-05-09 00:49:15,182 - INFO - S3 Hadoop check for s3a://datastreaming-analytics-1/staging/video_interactions/IngestionTimestamp=* - exists: False, file count: 0, files: []
2025-05-09 00:49:15,544 - INFO - S3 AWS SDK check for s3a://datastreaming-analytics-1/staging/video_interactions/IngestionTimestamp=* - file count: 1, files: [('s3a://datastreaming-analytics-1/staging/video_interactions/IngestionTimestamp=2025-05-08 20%3A05%3A03.933/part-00000-d4f7e0f2-ceb4-4282-9a77-20b0e1314027.c000.snappy.parquet', 8586)]
2025-05-09 00:49:21,467 - INFO - Actual Parquet schema: StructType([StructField('VideoID', LongType(), True), StructField('UserID', LongType(), True), StructField('VideoCategory', StringType(), True), StructField('Platform', StringType(), True), StructField('DeviceType', StringType(), True), StructField('WatchTime', StringType(), True), StructField('EngagementScore', LongType(), True), StructField('IngestionTimestamp', TimestampType(), True)])
2025-05-09 00:49:26,170 - INFO - Read 1 rows from Parquet files
2025-05-09 00:49:26,171 - INFO - Sample data:
2025-05-09 00:49:27,870 - INFO - Distinct IngestionTimestamp values:
2025-05-09 00:49:29,776 - INFO - Checking duplicates in video_df
2025-05-09 00:49:31,935 - INFO - Found 0 duplicate combinations in video_df
2025-05-09 00:49:35,923 - INFO - After deduplication: 1 rows
2025-05-09 00:49:36,423 - INFO - Testing access to Snowflake tables
2025-05-09 00:49:40,355 - INFO - Successfully accessed PUBLIC.DIM_USER: 3323 rows
2025-05-09 00:49:42,838 - INFO - Successfully accessed PUBLIC.DIM_VIDEO: 1 rows
2025-05-09 00:49:43,839 - INFO - Successfully accessed PUBLIC.DIM_PLATFORM: 5 rows
2025-05-09 00:49:44,932 - INFO - Successfully accessed PUBLIC.DIM_DEVICE_TYPE: 4 rows
2025-05-09 00:49:45,941 - INFO - Successfully accessed PUBLIC.DIM_TIME: 2106 rows
2025-05-09 00:49:46,420 - INFO - Creating dim_time
2025-05-09 00:49:47,358 - INFO - New WatchTime values: 1 rows
2025-05-09 00:49:50,341 - INFO - Found 0 new times to add to dim_time
2025-05-09 00:49:50,344 - INFO - missing_time_df schema: StructType([StructField('TimeID', StringType(), False), StructField('WatchTime', StringType(), False), StructField('Hour', LongType(), True)])
2025-05-09 00:49:50,345 - INFO - Sample missing_time_df rows:
2025-05-09 00:49:51,326 - INFO - No new times to append to dim_time
2025-05-09 00:49:51,353 - INFO - Creating dim_user
2025-05-09 00:49:53,154 - INFO - Found 0 new users to add to dim_user
2025-05-09 00:49:53,157 - INFO - missing_user_df schema: StructType([StructField('User_S_ID', StringType(), True), StructField('UserID', StringType(), True), StructField('Age', LongType(), True), StructField('Gender', StringType(), True), StructField('Location', StringType(), True), StructField('Income', LongType(), True), StructField('Debt', BooleanType(), True), StructField('OwnsProperty', BooleanType(), True), StructField('Profession', StringType(), True), StructField('Demographics', StringType(), True), StructField('CurrentActivity', StringType(), True), StructField('AgeGroup', StringType(), True)])
2025-05-09 00:49:53,157 - INFO - Sample missing_user_df rows:
2025-05-09 00:49:53,942 - INFO - No new users to append to dim_user
2025-05-09 00:49:53,948 - INFO - Creating dim_video
2025-05-09 00:49:54,873 - INFO - Prepared dim_video: 1 rows
2025-05-09 00:49:54,909 - INFO - video_dim_df schema: StructType([StructField('Video_S_ID', StringType(), True), StructField('VideoID', LongType(), True), StructField('VideoCategory', StringType(), True)])
2025-05-09 00:49:54,910 - INFO - Sample video_dim_df rows:
2025-05-09 00:49:58,712 - INFO - Successfully wrote dim_video: 1 rows
2025-05-09 00:49:58,713 - INFO - Creating dim_platform
2025-05-09 00:49:59,829 - INFO - Found 0 new platforms to add to dim_platform
2025-05-09 00:49:59,832 - INFO - missing_platform_df schema: StructType([StructField('PlatformID', StringType(), True), StructField('Platform', StringType(), True)])
2025-05-09 00:49:59,833 - INFO - Sample missing_platform_df rows:
2025-05-09 00:50:00,431 - INFO - No new platforms to append to dim_platform
2025-05-09 00:50:00,437 - INFO - Creating dim_device_type
2025-05-09 00:50:01,711 - INFO - Found 0 new device types to add to dim_device_type
2025-05-09 00:50:01,715 - INFO - missing_device_type_df schema: StructType([StructField('DeviceTypeID', StringType(), True), StructField('DeviceType', StringType(), True)])
2025-05-09 00:50:01,716 - INFO - Sample missing_device_type_df rows:
2025-05-09 00:50:02,226 - INFO - No new device types to append to dim_device_type
2025-05-09 00:50:02,231 - INFO - Creating fact_video_interactions
2025-05-09 00:50:02,435 - INFO - Checking duplicates in fact_df before joins
2025-05-09 00:50:03,242 - INFO - Found 0 duplicate InteractionIDs in fact_df
2025-05-09 00:50:04,870 - INFO - dim_user_df schema: StructType([StructField('User_S_ID', StringType(), True), StructField('UserID', StringType(), True)])
2025-05-09 00:50:06,104 - INFO - After joining with dim_user: 1 rows
2025-05-09 00:50:07,097 - INFO - dim_video_df schema: StructType([StructField('Video_S_ID', StringType(), True), StructField('VideoID', DecimalType(38,0), True)])
2025-05-09 00:50:09,191 - INFO - After joining with dim_video: 1 rows
2025-05-09 00:50:09,650 - INFO - dim_platform_df schema: StructType([StructField('PlatformID', StringType(), True), StructField('Platform', StringType(), True)])
2025-05-09 00:50:12,783 - INFO - After joining with dim_platform: 1 rows
2025-05-09 00:50:13,214 - INFO - dim_device_type_df schema: StructType([StructField('DeviceTypeID', StringType(), True), StructField('DeviceType', StringType(), True)])
2025-05-09 00:50:15,998 - INFO - After joining with dim_device_type: 1 rows
2025-05-09 00:50:16,742 - INFO - dim_time_df schema: StructType([StructField('TimeID', StringType(), True), StructField('WatchTime', StringType(), True)])
2025-05-09 00:50:20,233 - INFO - After joining with dim_time: 1 rows
2025-05-09 00:50:20,283 - INFO - Deduplicating fact_df
2025-05-09 00:50:23,823 - INFO - After in-batch deduplication: 1 rows
2025-05-09 00:50:26,851 - INFO - After removing existing InteractionIDs: 1 rows
2025-05-09 00:50:27,199 - INFO - fact_df schema before write: StructType([StructField('INTERACTIONID', StringType(), False), StructField('USERID_SURROGATE', StringType(), True), StructField('VIDEOID_SURROGATE', StringType(), True), StructField('PLATFORMID', StringType(), True), StructField('DEVICETYPEID', StringType(), True), StructField('TIMEID', StringType(), True), StructField('ENGAGEMENTSCORE', FloatType(), True), StructField('INGESTIONTIMESTAMP', TimestampType(), True), StructField('AGE', StringType(), True), StructField('GENDER', StringType(), True), StructField('LOCATION', StringType(), True), StructField('INCOME', FloatType(), True), StructField('DEBT', BooleanType(), True), StructField('OWNSPROPERTY', BooleanType(), True), StructField('PROFESSION', StringType(), True), StructField('DEMOGRAPHICS', StringType(), True), StructField('CURRENT_ACTIVITY', StringType(), True), StructField('AGEGROUP', StringType(), True), StructField('WATCH_DURATION', FloatType(), True), StructField('VIDEO_CATEGORY', StringType(), True), StructField('LOAD_TIMESTAMP', TimestampType(), False)])
2025-05-09 00:50:27,200 - INFO - fact_df column count: 21
2025-05-09 00:50:33,324 - INFO - Null counts for fact_video_interactions: {'INTERACTIONID': 0, 'USERID_SURROGATE': 0, 'VIDEOID_SURROGATE': 0, 'PLATFORMID': 0, 'DEVICETYPEID': 0, 'TIMEID': 0, 'ENGAGEMENTSCORE': 1, 'INGESTIONTIMESTAMP': 1, 'AGE': 1, 'GENDER': 1, 'LOCATION': 1, 'INCOME': 1, 'DEBT': 1, 'OWNSPROPERTY': 1, 'PROFESSION': 1, 'DEMOGRAPHICS': 1, 'CURRENT_ACTIVITY': 1, 'AGEGROUP': 1, 'WATCH_DURATION': 1, 'VIDEO_CATEGORY': 1, 'LOAD_TIMESTAMP': 0}
2025-05-09 00:50:38,070 - WARNING - Found 1 rows with nulls in critical columns for fact_video_interactions
2025-05-09 00:50:43,257 - ERROR - Error validating nulls for fact_video_interactions: VideoID
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/types.py", line 2164, in __getitem__
    idx = self.__fields__.index(item)
ValueError: 'VideoID' is not in list

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/app/scripts/stream_video_streamed_transformed.py", line 118, in validate_nulls
    logger.info(f"Rejected row (Nulls): VideoID={row['VideoID']}, UserID={row['UserID']}, "
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/types.py", line 2169, in __getitem__
    raise ValueError(item)
ValueError: VideoID

2025-05-09 00:50:46,807 - INFO - Prepared fact_video_interactions: 1 rows
2025-05-09 00:50:46,808 - INFO - fact_df schema: StructType([StructField('INTERACTIONID', StringType(), False), StructField('USERID_SURROGATE', StringType(), True), StructField('VIDEOID_SURROGATE', StringType(), True), StructField('PLATFORMID', StringType(), True), StructField('DEVICETYPEID', StringType(), True), StructField('TIMEID', StringType(), True), StructField('ENGAGEMENTSCORE', FloatType(), True), StructField('INGESTIONTIMESTAMP', TimestampType(), True), StructField('AGE', StringType(), True), StructField('GENDER', StringType(), True), StructField('LOCATION', StringType(), True), StructField('INCOME', FloatType(), True), StructField('DEBT', BooleanType(), True), StructField('OWNSPROPERTY', BooleanType(), True), StructField('PROFESSION', StringType(), True), StructField('DEMOGRAPHICS', StringType(), True), StructField('CURRENT_ACTIVITY', StringType(), True), StructField('AGEGROUP', StringType(), True), StructField('WATCH_DURATION', FloatType(), True), StructField('VIDEO_CATEGORY', StringType(), True), StructField('LOAD_TIMESTAMP', TimestampType(), False)])
2025-05-09 00:50:51,225 - INFO - Writing to PUBLIC.FACT_VIDEO_INTERACTIONS with options: {'sfURL': 'uxctays-kqc58183.snowflakecomputing.com', 'sfUser': 'Pranitha22', 'sfPassword': 'Pradyumna@9999', 'sfDatabase': 'ANALYTICS', 'sfSchema': 'PUBLIC', 'sfWarehouse': 'COMPUTE_WHI', 'sfRole': 'ACCOUNTADMIN'}
2025-05-09 00:50:57,004 - INFO - Successfully appended fact_video_interactions: 1 rows
2025-05-09 00:50:57,385 - INFO - Spark session stopped
2025-05-09 00:50:57,388 - INFO - Closing down clientserver connection
